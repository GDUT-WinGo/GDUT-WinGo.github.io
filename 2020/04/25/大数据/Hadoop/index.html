<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Hadoop | Wingo&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Hadoop">
    <meta name="description" content="大数据技术 Hadoop 的整体入门介绍。">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop">
<meta property="og:url" content="http://yoursite.com/2020/04/25/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/index.html">
<meta property="og:site_name" content="Wingo&#39;s Blog">
<meta property="og:description" content="大数据技术 Hadoop 的整体入门介绍。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop01.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop02.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop03.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop04.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop05.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop06.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop07.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop08.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop09.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop10.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop11.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop12.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop13.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop22.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop19.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop17.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop18.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop14.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop15.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop16.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop20.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop21.png">
<meta property="article:published_time" content="2020-04-25T02:09:27.000Z">
<meta property="article:modified_time" content="2020-07-10T03:39:05.616Z">
<meta property="article:author" content="Wingo">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop01.png">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Wingo</h5>
          <a href="mailto:1318263468@qq.com" title="1318263468@qq.com" class="mail">1318263468@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Hadoop</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Hadoop</h1>
        <h5 class="subtitle">
            
                <time datetime="2020-04-25T02:09:27.000Z" itemprop="datePublished" class="page-time">
  2020-04-25
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基本概念"><span class="post-toc-number">1.</span> <span class="post-toc-text">基本概念</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-x-VS-2-x"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">1.x VS 2.x</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#HDFS"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">HDFS</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#YARN"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">YARN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#MapReduce"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">MapReduce</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#生态体系"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">生态体系</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#环境搭建"><span class="post-toc-number">2.</span> <span class="post-toc-text">环境搭建</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#软件安装"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">软件安装</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Hadoop-目录结构"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Hadoop 目录结构</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#配置文件说明"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">配置文件说明</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#运行模式"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">运行模式</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#本地模式"><span class="post-toc-number">2.4.1.</span> <span class="post-toc-text">本地模式</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#伪分布式"><span class="post-toc-number">2.4.2.</span> <span class="post-toc-text">伪分布式</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#启动-YARN"><span class="post-toc-number">2.4.2.1.</span> <span class="post-toc-text">启动 YARN</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#历史服务器"><span class="post-toc-number">2.4.2.2.</span> <span class="post-toc-text">历史服务器</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#日志聚集"><span class="post-toc-number">2.4.2.3.</span> <span class="post-toc-text">日志聚集</span></a></li></ol></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#完全分布式"><span class="post-toc-number">2.4.3.</span> <span class="post-toc-text">完全分布式</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#集群分发"><span class="post-toc-number">2.4.3.1.</span> <span class="post-toc-text">集群分发</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#集群配置"><span class="post-toc-number">2.4.3.2.</span> <span class="post-toc-text">集群配置</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#单点启动"><span class="post-toc-number">2.4.3.3.</span> <span class="post-toc-text">单点启动</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#集群启动"><span class="post-toc-number">2.4.3.4.</span> <span class="post-toc-text">集群启动</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SSH-配置"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">SSH 配置</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#时间同步"><span class="post-toc-number">2.6.</span> <span class="post-toc-text">时间同步</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#操作集群"><span class="post-toc-number">2.7.</span> <span class="post-toc-text">操作集群</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#启动停止操作"><span class="post-toc-number">2.8.</span> <span class="post-toc-text">启动停止操作</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#HDFS-1"><span class="post-toc-number">3.</span> <span class="post-toc-text">HDFS</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#架构"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">架构</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#文件快大小"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">文件快大小</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Shell-操作"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Shell 操作</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#写流程"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">写流程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#网络拓扑"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">网络拓扑</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#机架感知"><span class="post-toc-number">3.4.2.</span> <span class="post-toc-text">机架感知</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#读流程"><span class="post-toc-number">3.5.</span> <span class="post-toc-text">读流程</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2NN"><span class="post-toc-number">3.6.</span> <span class="post-toc-text">2NN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Fsimage-和-Edits"><span class="post-toc-number">3.6.1.</span> <span class="post-toc-text">Fsimage 和 Edits</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#CheckPoint"><span class="post-toc-number">3.6.2.</span> <span class="post-toc-text">CheckPoint</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#故障处理"><span class="post-toc-number">3.6.3.</span> <span class="post-toc-text">故障处理</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#安全模式"><span class="post-toc-number">3.7.</span> <span class="post-toc-text">安全模式</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#多目录"><span class="post-toc-number">3.8.</span> <span class="post-toc-text">多目录</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#DataNode"><span class="post-toc-number">3.9.</span> <span class="post-toc-text">DataNode</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#数据完整性"><span class="post-toc-number">3.10.</span> <span class="post-toc-text">数据完整性</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#黑白名单"><span class="post-toc-number">3.11.</span> <span class="post-toc-text">黑白名单</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#回收站"><span class="post-toc-number">3.12.</span> <span class="post-toc-text">回收站</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#快照管理"><span class="post-toc-number">3.13.</span> <span class="post-toc-text">快照管理</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#MapReduce-1"><span class="post-toc-number">4.</span> <span class="post-toc-text">MapReduce</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#实例进程"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">实例进程</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#序列化类型"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">序列化类型</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#编程规范"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">编程规范</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Mapper-阶段"><span class="post-toc-number">4.3.1.</span> <span class="post-toc-text">Mapper 阶段</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Reduce-阶段"><span class="post-toc-number">4.3.2.</span> <span class="post-toc-text">Reduce 阶段</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Driver-阶段"><span class="post-toc-number">4.3.3.</span> <span class="post-toc-text">Driver 阶段</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Shuffle"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">Shuffle</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Partition"><span class="post-toc-number">4.4.1.</span> <span class="post-toc-text">Partition</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#自定义"><span class="post-toc-number">4.4.2.</span> <span class="post-toc-text">自定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#排序"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">排序</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#自定义-1"><span class="post-toc-number">4.5.1.</span> <span class="post-toc-text">自定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#合并"><span class="post-toc-number">4.6.</span> <span class="post-toc-text">合并</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#分组"><span class="post-toc-number">4.7.</span> <span class="post-toc-text">分组</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#MapTask"><span class="post-toc-number">4.8.</span> <span class="post-toc-text">MapTask</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ReduceTask"><span class="post-toc-number">4.9.</span> <span class="post-toc-text">ReduceTask</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#工作流程"><span class="post-toc-number">4.10.</span> <span class="post-toc-text">工作流程</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Hadoop-序列化"><span class="post-toc-number">5.</span> <span class="post-toc-text">Hadoop 序列化</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#InputFormat"><span class="post-toc-number">6.</span> <span class="post-toc-text">InputFormat</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#FileInputFormat"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">FileInputFormat</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CombineText"><span class="post-toc-number">6.2.</span> <span class="post-toc-text">CombineText</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#KeyValueText"><span class="post-toc-number">6.3.</span> <span class="post-toc-text">KeyValueText</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#NLine"><span class="post-toc-number">6.4.</span> <span class="post-toc-text">NLine</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#自定义-2"><span class="post-toc-number">6.5.</span> <span class="post-toc-text">自定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#OutputFormat"><span class="post-toc-number">7.</span> <span class="post-toc-text">OutputFormat</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#自定义-3"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">自定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Reduce-Join"><span class="post-toc-number">8.</span> <span class="post-toc-text">Reduce Join</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bean"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">Bean</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Mapper"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">Mapper</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Reduce"><span class="post-toc-number">8.3.</span> <span class="post-toc-text">Reduce</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Driver"><span class="post-toc-number">8.4.</span> <span class="post-toc-text">Driver</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Map-Join"><span class="post-toc-number">9.</span> <span class="post-toc-text">Map Join</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Driver-1"><span class="post-toc-number">9.1.</span> <span class="post-toc-text">Driver</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Map"><span class="post-toc-number">9.2.</span> <span class="post-toc-text">Map</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#数据清洗-ETL"><span class="post-toc-number">10.</span> <span class="post-toc-text">数据清洗 ETL</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Map-1"><span class="post-toc-number">10.1.</span> <span class="post-toc-text">Map</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Driver-2"><span class="post-toc-number">10.2.</span> <span class="post-toc-text">Driver</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#数据压缩"><span class="post-toc-number">11.</span> <span class="post-toc-text">数据压缩</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Driver-定义压缩"><span class="post-toc-number">11.1.</span> <span class="post-toc-text">Driver 定义压缩</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#YARN-1"><span class="post-toc-number">12.</span> <span class="post-toc-text">YARN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#资源调度器"><span class="post-toc-number">12.1.</span> <span class="post-toc-text">资源调度器</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#推测执行"><span class="post-toc-number">12.2.</span> <span class="post-toc-text">推测执行</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-大数据/Hadoop"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Hadoop</h1>
        <div class="post-meta">
            <time class="post-time" title="2020-04-25 10:09:27" datetime="2020-04-25T02:09:27.000Z"  itemprop="datePublished">2020-04-25</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li></ul>



            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>大数据技术 Hadoop 的整体入门介绍。</p>
<a id="more"></a>

<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><blockquote>
<p>主要解决：海量数据的存储和海量数据的分析计算问题。</p>
</blockquote>
<p>一般形式的大数据部门组成结构。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop01.png" alt=""></p>
<p>Hadoop 三大发行版本：</p>
<blockquote>
<p>Apache 版本最原始（最基础）的版本，对于入门学习最好；<br>Cloudera 在大型互联网企业中用的较多；<br>Hortonworks 文档较好。</p>
</blockquote>
<p>Hadoop 优势：</p>
<blockquote>
<p>高可靠性：Hadoop 底层维护多个数据副本，所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失；<br>高扩展性：在集群间分配仼务数据，可方便的扩展数以干计的节点；<br>高效性：在 MapReduce 的思想下,，Hadoop 是并行工作的，以加快任务处理速度；<br>高容错性：能够自动将失败的任务重新分配。</p>
</blockquote>
<h4 id="1-x-VS-2-x"><a href="#1-x-VS-2-x" class="headerlink" title="1.x VS 2.x"></a>1.x VS 2.x</h4><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop02.png" style="zoom:67%;" />

<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>NameNode（nn）：存储文件的元数据，如文件名、文件目录结构、文件属性（生成时间、副本数<br>文件权限），以及每个文件的块列表和块所在的 DataNode 等；</p>
<p>DataNode（dn）：在本地文件系统存储文件块数据,以及块数据的校验和。</p>
<p>Secondary NameNode（2nn）：用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。</p>
<h4 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h4><p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop03.png" alt=""></p>
<p>ResourceManager 由两个关键组件 Scheduler 和 ApplicationsManager 组成。</p>
<blockquote>
<p>Scheduler 在容量和队列限制范围内负责为运行的容器分配资源。Scheduler 是一个纯调度器（pure scheduler），只负责调度，它不会监视或跟踪应用程序的状态，也不负责重启失败任务，这些全都交给 ApplicationMaster 完成。Scheduler 根据各个应用程序的资源需求进行资源分配。</p>
<p>ApplicationManager 负责接收作业的提交，然后启动一个 ApplicationMaster 容器负责该应用。它也会在ApplicationMaster 容器失败时，重新启动 ApplicationMaster 容器。</p>
</blockquote>
<p>Hadoop 2.X 集群中的每个 DataNode 都会运行一个NodeManager 来执行 Yarn 的功能。每个节点上的 NodeManager 执行以下功能：</p>
<ul>
<li>定时向 ResourceManager 汇报本节点资源使用情况和各个 Container 的运行状况</li>
<li>监督应用程序容器的生命周期</li>
<li>（监控资源）监控、管理和提供容器消耗的有关资源（CPU / 内存）的信息</li>
<li>（监控资源使用情况）</li>
<li>（监控容器）监控容器的资源使用情况，杀死失去控制的程序</li>
<li>（启动/停止容器）接受并处理来自 ApplicationMaster 的 Container 启动/停止等各种请求。</li>
</ul>
<p>提交到 YARN 上的每一个应用都有一个专用的 ApplicationMaster（注意，ApplicationMaster 需要和 ApplicationManager 区分）。ApplicationMaster 运行在应用程序启动时的第一个容器内。ApplicationMaster 会与 ResourceManager 协商获取容器来执行应用中的 mappers 和 reducers，之后会将 ResourceManager 分配的容器资源呈现给运行在每个 DataNode 上的 NodeManager。ApplicationMaster 请求的资源是具体的。包括：</p>
<ul>
<li>处理作业需要的文件块</li>
<li>为应用程序创建的以容器为单位的资源</li>
<li>容器大小（例如，1GB 内存和一个虚拟核心）</li>
<li>资源在何处分配，这个依据从 NameNode 获取的块存储位置信息（如机器1的节点10上分配4个容器，机器2的节点20上分配8个容器）</li>
<li>资源请求的优先级</li>
</ul>
<blockquote>
<p>ApplicationMaster 是一个特定的框架。例如，MapReduce 程序是 MRAppMaster，spark 是 SparkAppMaster。</p>
</blockquote>
<p>Container 是对于资源的抽象, 它封装了某个节点上的多维度资源，如内存、CPU 等。</p>
<h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><blockquote>
<p>Map 阶段并行处理输入数据；</p>
<p>Reduce 阶段对 Map 结果进行汇总。</p>
</blockquote>
<h4 id="生态体系"><a href="#生态体系" class="headerlink" title="生态体系"></a>生态体系</h4><p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop04.png" alt=""></p>
<p>Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive 与传统的数据库（MySql）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。</p>
<p>Kafka 是一种高吞吐量的分布式发布订阅消息系统。</p>
<p>Storm 用于连续计算，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</p>
<p>Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。</p>
<p>Oozie 是一个管理 Hdoop 作业（job）的工作流程调度管理系统。</p>
<p>HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
<p>Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。 其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。</p>
<p>R 语言是用于统计分析、绘图的语言和操作环境。R 是属于 GNU 系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。</p>
<p>Apache Mahout 是个可扩展的机器学习和数据挖掘库。</p>
<p>Zookeeper 是 Google 的 Chubby 一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
<h3 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h3><h4 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 /opt 目录下创建 module、software 文件夹</span></span><br><span class="line">mkdir module</span><br><span class="line">mkdir software</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 module、software 文件夹的所有者</span></span><br><span class="line">chown [group]:[user] module/ software/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 JDK</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查是否安装 Java 软件</span></span><br><span class="line">rpm -qa | grep java</span><br><span class="line">rpm -e [software]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看 JDK 安装路径</span></span><br><span class="line">which java</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压 JDK 到 /opt/module 目录下</span></span><br><span class="line">tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置 JDK 环境变量</span></span><br><span class="line">pwd /opt/module/jdk1.8.0_144</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 文件末尾添加 JDK 路径</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置生效</span></span><br><span class="line">source /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试环境</span></span><br><span class="line">java -version</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 Hadoop</span></span><br><span class="line">tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加 Hadoop 环境变量</span></span><br><span class="line">pwd /opt/module/hadoop-2.7.2</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span></span><br><span class="line">source /etc/profile</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure>

<h4 id="Hadoop-目录结构"><a href="#Hadoop-目录结构" class="headerlink" title="Hadoop 目录结构"></a>Hadoop 目录结构</h4><p>bin 目录：存放对 Hadoop 相关服务（HDFS，YARN）进行操作的脚本；</p>
<p>etc 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件；</p>
<p>lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）；</p>
<p>sbin 目录：存放启动或停止 Hadoop 相关服务的脚本；</p>
<p>share 目录：存放 Hadoop 的依赖 jar 包、文档、和官方案例。</p>
<h4 id="配置文件说明"><a href="#配置文件说明" class="headerlink" title="配置文件说明"></a>配置文件说明</h4><blockquote>
<p>Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
</blockquote>
<table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在 Hadoop 的 jar 包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td>[core-default.xml]</td>
<td>hadoop-common-2.7.2.jar/ core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-2.7.2.jar/ hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-2.7.2.jar/ yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml</td>
</tr>
</tbody></table>
<p>自定义配置文件：</p>
<p>​    core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个配置文件存放在 $HADOOP_HOME/etc/hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<h4 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h4><h5 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop-2.7.2 目录下</span></span><br><span class="line">mkdir wcinput</span><br><span class="line">cd wcinput</span><br><span class="line">touch wc.input</span><br><span class="line"><span class="meta">#</span><span class="bash"> 编写分析文本</span></span><br><span class="line">vi wc.input</span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop-2.7.2 目录下执行</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看结果</span></span><br><span class="line">cat wcoutput/part-r-00000</span><br></pre></td></tr></table></figure>

<h5 id="伪分布式"><a href="#伪分布式" class="headerlink" title="伪分布式"></a>伪分布式</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置 hadoop-env.sh，修改 JAVA_HOME 路径</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 core-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 Hadoop 运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 hdfs-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 HDFS 副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 格式化 NameNode（第一次启动时需要格式化）</span></span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 NameNode</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 DataNode</span></span><br><span class="line">sbin/hadoop-daemon.sh start datanode</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看集群是否启动成功</span></span><br><span class="line">jps</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看产生的 Log 日志</span></span><br><span class="line">cd /opt/module/hadoop-2.7.2/logs</span><br></pre></td></tr></table></figure>

<blockquote>
<p>格式化 NameNode 后会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，导致集群找不到之前数据。所以，格式 NameNode 前一定要先删除 data 数据和 log 日志，然后才能格式化 NameNode。</p>
</blockquote>
<h6 id="启动-YARN"><a href="#启动-YARN" class="headerlink" title="启动 YARN"></a>启动 YARN</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置 yarn-env.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置yarn-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Reducer 获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置 mapred-env.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置 mapred-site.xml</span></span><br><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定 MR 运行在 YARN 上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动集群，启动前必须保证 NameNode 和 DataNode 已经启动</span></span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line">sbin/yarn-daemon.sh start nodemanager</span><br><span class="line"><span class="meta">#</span><span class="bash"> 8088 端口查看 web 页面</span></span><br></pre></td></tr></table></figure>

<h6 id="历史服务器"><a href="#历史服务器" class="headerlink" title="历史服务器"></a>历史服务器</h6><blockquote>
<p>用于查看程序的历史运行情况。</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 mapred-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器 web 端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动历史服务器</span></span><br><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"><span class="meta">#</span><span class="bash"> 19888 端口查看 web 页面</span></span><br></pre></td></tr></table></figure>

<h6 id="日志聚集"><a href="#日志聚集" class="headerlink" title="日志聚集"></a>日志聚集</h6><blockquote>
<p>应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上，方便的查看到程序运行详情和开发调试。</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 重启后日志聚集才生效</span></span><br><span class="line">sbin/yarn-daemon.sh stop resourcemanager</span><br><span class="line">sbin/yarn-daemon.sh stop nodemanager</span><br><span class="line">sbin/mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line">sbin/yarn-daemon.sh start nodemanager</span><br><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<h5 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h5><blockquote>
<p>三台虚拟机之间的相互通讯，协调工作。</p>
</blockquote>
<h6 id="集群分发"><a href="#集群分发" class="headerlink" title="集群分发"></a>集群分发</h6><blockquote>
<p>scp（secure copy）可以实现服务器与服务器之间的数据拷贝。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 hadoop101 上将 hadoop101 中 /opt/module 目录下的软件拷贝到 hadoop102 上</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -r 表示递归，即全部都拷贝过去</span></span><br><span class="line">scp -r /opt/module root@hadoop102:/opt/module</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将 hadoop101 中 /etc/profile 文件拷贝到 hadoop102 的 /etc/profile 上</span></span><br><span class="line">scp /etc/profile root@hadoop102:/etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置生效</span></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<blockquote>
<p>rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。</p>
</blockquote>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
<tr>
<td>-l</td>
<td>拷贝符号连接</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 把 hadoop101 机器上的 /opt/software 目录同步到 hadoop102 服务器的 root 用户下的 /opt/ 目录</span></span><br><span class="line">rsync -rvl /opt/software/ root@hadoop102:/opt/software</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 编写集群分发脚本</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /usr/<span class="built_in">local</span>/bin 目录下的脚本全局可用</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /home/wingo/bin 这个目录下存放的脚本，wingo 用户可以在系统任何地方直接执行</span></span><br><span class="line">touch xsync</span><br><span class="line">vi xsync</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取文件名称，basename 获取路劲最后那个名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 循环，这里根据需求修改循环</span></span><br><span class="line">for((host=103; host&lt;105; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h6 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h6><blockquote>
<p>先在 hadoop102 中进行配置，然后将配置分发到其余的节点。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">hadoop102</th>
<th align="center">hadoop103</th>
<th align="center">hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td align="center">HDFS</td>
<td align="center">NameNode、DataNode</td>
<td align="center">DataNode</td>
<td align="center">SecondaryNameNode、DataNode</td>
</tr>
<tr>
<td align="center">YARN</td>
<td align="center">NodeManager</td>
<td align="center">ResourceManager、NodeManager</td>
<td align="center">NodeManager</td>
</tr>
</tbody></table>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 core-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 Hadoop 运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置 hadoop-env.sh，修改 JAVA_HOME 路径</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置 hdfs-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 分片数必须要小于节点数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 Hadoop 辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置 yarn-env.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置yarn-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Reducer 获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置 mapred-env.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置 mapred-site.xml</span></span><br><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定 MR 运行在 YARN 上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在集群上分发配置好的Hadoop配置文件</span></span><br><span class="line">xsync /opt/module/hadoop-2.7.2/</span><br></pre></td></tr></table></figure>

<h6 id="单点启动"><a href="#单点启动" class="headerlink" title="单点启动"></a>单点启动</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> NameNode 第一次启动初始化</span></span><br><span class="line">hadoop namenode -format</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line"><span class="meta">#</span><span class="bash"> 分别在三个节点上启动 DataNode</span></span><br><span class="line">hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<h6 id="集群启动"><a href="#集群启动" class="headerlink" title="集群启动"></a>集群启动</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vi /opt/module/hadoop-2.7.2/etc/hadoop/slaves</span><br><span class="line">    # hadoop102</span><br><span class="line">    # hadoop103</span><br><span class="line">    # hadoop104</span><br><span class="line"><span class="meta">#</span><span class="bash"> 同步配置</span></span><br><span class="line">xsync slaves</span><br><span class="line"><span class="meta">#</span><span class="bash"> NameNode 节点</span></span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> ResourceManage 节点</span></span><br><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="SSH-配置"><a href="#SSH-配置" class="headerlink" title="SSH 配置"></a>SSH 配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ~/.ssh 生成密钥</span></span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个节点都需要进行此操作</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将公钥拷贝到要免密登录的目标机器上</span></span><br><span class="line">ssh-copy-id hadoop102</span><br><span class="line">ssh-copy-id hadoop103</span><br><span class="line">ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>文件名</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>known_hosts</td>
<td>记录ssh访问过计算机的公钥(public key)</td>
</tr>
<tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过得无密登录服务器公钥</td>
</tr>
</tbody></table>
<h4 id="时间同步"><a href="#时间同步" class="headerlink" title="时间同步"></a>时间同步</h4><blockquote>
<p>NPT（Network Time Protocol）</p>
<p>时间同步的方式：找一个节点作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 检查安装</span></span><br><span class="line">rpm -qa | grep ntp</span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line">yum install -y ntp</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改配置</span></span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line">    # 授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间</span><br><span class="line">    # restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">    # 集群在局域网中，不使用其他互联网上的时间</span><br><span class="line">    # #server 0.centos.pool.ntp.org iburst</span><br><span class="line">    # #server 1.centos.pool.ntp.org iburst</span><br><span class="line">    # #server 2.centos.pool.ntp.org iburst</span><br><span class="line">    # #server 3.centos.pool.ntp.org iburst</span><br><span class="line">    # 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步</span><br><span class="line">    # server 127.127.1.0</span><br><span class="line">    # fudge 127.127.1.0 stratum 10</span><br><span class="line">vim /etc/sysconfig/ntpd</span><br><span class="line"><span class="meta">	#</span><span class="bash"> SYNC_HWCLOCK=yes</span></span><br><span class="line">service ntpd restart</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机启动此服务</span></span><br><span class="line">chkconfig ntpd on</span><br><span class="line"><span class="meta">#</span><span class="bash"> 其它节点配置</span></span><br><span class="line">crontab -e</span><br><span class="line"><span class="meta">	#</span><span class="bash"> */10 * * * * /usr/sbin/ntpdate hadoop102</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改时间</span></span><br><span class="line">date -s "2017-9-11 11:11:11"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 一段时间后查看是否同步</span></span><br><span class="line">date</span><br></pre></td></tr></table></figure>



<h4 id="操作集群"><a href="#操作集群" class="headerlink" title="操作集群"></a>操作集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 HDFS 文件系统上创建一个 input 文件夹</span></span><br><span class="line">bin/hdfs dfs -mkdir -p /user/wingo/input</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将测试文件内容上传到文件系统上</span></span><br><span class="line">bin/hdfs dfs -put wcinput/wc.input /user/wingo/input/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看上传的文件</span></span><br><span class="line">bin/hdfs dfs -ls  /user/wingo/input/</span><br><span class="line">bin/hdfs dfs -cat  /user/wingo/input/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行 MapReduce 程序</span></span><br><span class="line">bin/hadoop jar</span><br><span class="line">	share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount </span><br><span class="line">	/user/wingo/input/ </span><br><span class="line">	/user/wingo/output</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看输出结果，可在 50070 端口 Web 查看</span></span><br><span class="line">bin/hdfs dfs -cat /user/wingo/output/*</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载文件到本地</span></span><br><span class="line">bin/hdfs dfs -get /user/wingo/output/part-r-00000 ./wcoutput/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除输出结果</span></span><br><span class="line">bin/hdfs dfs -rm -r /user/wingo/output</span><br></pre></td></tr></table></figure>

<h4 id="启动停止操作"><a href="#启动停止操作" class="headerlink" title="启动停止操作"></a>启动停止操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 分别启动 / 停止 HDFS 组件</span></span><br><span class="line">hadoop-daemon.sh  start / stop  namenode / datanode / secondarynamenode</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 / 停止 YARN</span></span><br><span class="line">yarn-daemon.sh  start / stop  resourcemanager / nodemanager</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置了 ssh 后可一次性启动集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 整体启动 / 停止 HDFS</span></span><br><span class="line">start-dfs.sh / stop-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 整体启动/停止 YARN</span></span><br><span class="line">start-yarn.sh  /  stop-yarn.sh</span><br></pre></td></tr></table></figure>

<h3 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h3><p>HDFS( Hadoop Distributed File System）它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p>
<blockquote>
<p>具有高容错性，处理大数据，可构建在廉价机器上等优点；<br>具有延迟较高，无法高效存储大量小数据，不支持并发写入及文件随机修改等缺陷。</p>
</blockquote>
<h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop05.png" style="zoom:67%;" />

<p>NameNode 就是 Master，它是一个主管、管理者。</p>
<blockquote>
<p>管理HDFS的名称空间；<br>配置副本策略;<br>管理数据块( Block）映射信息;<br>处理客户端读写请求。</p>
</blockquote>
<p>DataNode：就是 Slave。 NameNode 下达命令, DataNode 执行实际的操作。</p>
<blockquote>
<p>存储实际的数据块;<br>执行数据块的读写操作。</p>
</blockquote>
<p>Client：客户端。</p>
<blockquote>
<p>文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block 然后进行上传；<br>与 NameNode 交互，获取文件的位置信息；<br>与 DataNode 交互，读取或者写入数据；<br>Client 提供一些命令来管理 HDFS,比如 NameNode 格式化；<br>Client 可以通过一些命令来访问 HDFS，比如对 HDFS 增删查改操作。</p>
</blockquote>
<p>Secondary NameNode：并非 NameNode 的热备。</p>
<blockquote>
<p>辅助 NameNode，分担其工作量，比如定期合并 Silage 和 Edits，并推送给 NameNode；<br>在紧急情况下,可辅助恢复 NameNode。</p>
</blockquote>
<h4 id="文件快大小"><a href="#文件快大小" class="headerlink" title="文件快大小"></a>文件快大小</h4><p>HDFS 中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定,默认大小在 Hadoop2x 版本中是 128M，老版本中是 64M。</p>
<blockquote>
<p>HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置；<br>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。<br>总结：HDFS 块的大小设置主要取决于磁盘传输速率。（寻址时间为传输时间的 1% 时最合适）</p>
</blockquote>
<h4 id="Shell-操作"><a href="#Shell-操作" class="headerlink" title="Shell 操作"></a>Shell 操作</h4><blockquote>
<p>hfs 是 fs 的具体实现类。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令大全</span></span><br><span class="line">bin/hadoop fs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 输出这个命令参数</span></span><br><span class="line">hadoop fs -help rm</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示目录信息</span></span><br><span class="line">hadoop fs -ls /</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在 HDFS 上创建目录</span></span><br><span class="line">hadoop fs -mkdir -p /win/go</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从本地剪切粘贴到 HDFS</span></span><br><span class="line">hadoop fs  -moveFromLocal  ./win.txt  /win/go</span><br><span class="line"><span class="meta">#</span><span class="bash"> 追加一个文件到已经存在的文件末尾</span></span><br><span class="line">hadoop fs -appendToFile go.txt /win/go/win.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示文件内容</span></span><br><span class="line">hadoop fs -cat /win/go/win.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件所属权限，HDFS 操作的用户及用户组必须是系统中存在的</span></span><br><span class="line">hadoop fs  -chmod  666  /win/go/win.txt</span><br><span class="line">hadoop fs  -chown  win:go   /win/go/win.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从 HDFS 拷贝到本地</span></span><br><span class="line">hadoop fs -copyToLocal /win/go/win.txt ./</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从 HDFS 的一个路径拷贝到 HDFS 的另一个路径</span></span><br><span class="line">hadoop fs -cp /win/go/win.txt /other.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在 HDFS 目录中移动文件</span></span><br><span class="line">hadoop fs -mv /other.txt /win/go/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从 HDFS 下载文件到本地</span></span><br><span class="line">hadoop fs -get /win/go/win.txt ./</span><br><span class="line"><span class="meta">#</span><span class="bash"> 合并下载多个 HDFS 文件</span></span><br><span class="line">hadoop fs -getmerge /user/wingo/test/* ./together.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 等同于 copyFromLocal</span></span><br><span class="line">hadoop fs -put ./local.txt /user/wingo/test/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示一个文件的末尾</span></span><br><span class="line">hadoop fs -tail /win/go/win.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除文件或文件夹</span></span><br><span class="line">hadoop fs -rm /user/wingo/test/delete.txt</span><br><span class="line">hadoop fs -rmdir /test</span><br><span class="line"><span class="meta">#</span><span class="bash"> 统计文件夹的大小信息</span></span><br><span class="line">hadoop fs -du -h /user/wingo/test</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 HDFS 中文件的副本数量：若条件允许的话才会达到</span></span><br><span class="line">hadoop fs -setrep 10 /win/go/win.txt</span><br></pre></td></tr></table></figure>

<h4 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h4><blockquote>
<p>向 HDFS 写数据时，客户端与 NameNode、DataNode 通信过程。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop06.png" alt=""></p>
<h5 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h5><blockquote>
<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop07.png" alt=""></p>
<h5 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h5><blockquote>
<p>副本节点选择。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop08.png" alt=""></p>
<h4 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h4><blockquote>
<p>在 HDFS 读数据时，客户端与 NameNode、DataNode 通信过程。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop09.png" alt=""></p>
<h4 id="2NN"><a href="#2NN" class="headerlink" title="2NN"></a>2NN</h4><p>如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的 FsImage。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由 NameNode 节点完成，又会效率过低。因此，引入一个新的节点 SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop10.png" alt=""></p>
<h5 id="Fsimage-和-Edits"><a href="#Fsimage-和-Edits" class="headerlink" title="Fsimage 和 Edits"></a>Fsimage 和 Edits</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oiv 查看 Fsimage 文件</span></span><br><span class="line">pwd</span><br><span class="line"><span class="meta">	#</span><span class="bash"> /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current</span></span><br><span class="line">hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br><span class="line">cat /opt/module/hadoop-2.7.2/fsimage.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 记录了 NameNode 的信息，DataNode 信息定时同步到 NameNode --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512722284477<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>wingo:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oev 查看 Edits 文件</span></span><br><span class="line">hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</span><br><span class="line">cat /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 记录了操作信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>129<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">EDITS</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 默认配置 hdfs-default.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 一小时执行一次 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作次数达一百万次<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>一分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="故障处理"><a href="#故障处理" class="headerlink" title="故障处理"></a>故障处理</h5><blockquote>
<p>NameNode 故障后，可以采用如下两种方法恢复数据。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 方法一 将 SecondaryNameNode 中数据拷贝到 NameNode 存储数据的目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 杀掉 NameNode 相关进程</span></span><br><span class="line">kill -9 [pid]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除 NameNode 存储的数据</span></span><br><span class="line">rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br><span class="line"><span class="meta">#</span><span class="bash"> 拷贝 SecondaryNameNode 中数据到原 NameNode 存储数据目录</span></span><br><span class="line">scp -r root@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重新启动 NameNode</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 方法二</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用 -importCheckpoint 选项启动 NameNode 守护进程，从而将 Secondary NameNode 中数据拷贝到 NameNode 目录中。</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 修改hdfs-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 杀掉 NameNode 相关进程</span></span><br><span class="line">kill -9 [pid]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除 NameNode 存储的数据</span></span><br><span class="line">rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果 SecondaryNameNode 不和 NameNode 在一个主机节点上，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要将 SecondaryNameNode 存储数据的目录拷贝到 NameNode 存储数据的平级目录，并删除 in_use.lock 文件</span></span><br><span class="line">scp -r root@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line">rm -rf in_use.lock</span><br><span class="line"><span class="meta">#</span><span class="bash"> 导入检查点数据（等待一会 ctrl+c 结束掉）</span></span><br><span class="line">bin/hdfs namenode -importCheckpoint</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 NameNode</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>NameNode 启动时，首先将镜像文件(（Fsimage）载入內存，并执行编辑日志(（Edits）中的各项操作，一旦在内存中成功建立文件系统元数据的映像，则创建一个新的 Silage文件和一个空的编辑日志。此时，NameNode 开始监听 DataNode 请求。这个过程期间，Namenode 一直运行在安全模式，即 NameNode 的文件系统对于客户端来说是只读的。</p>
<p>系统中的数据块的位置并不是由 NameNode 维护的，而是以块列表的形式存储在 DataNode 中。在系统的正常操作期间, NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个 Datanode 会向 NameNode 发送最新的块列表信息,，NameNode 了解到足够多的块位置信息之后，即可高效运行文件系统。</p>
<p>如果满足“最小副本条件”,，NameNode 会在 30 秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中 99.9% 的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的 HDFS 集群时，因为系统中还没有任何块，所以 NameNode 不会进入安全模式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfsadmin -safemode get	# 查看安全模式状态</span><br><span class="line">bin/hdfs dfsadmin -safemode enter # 功能描述：进入安全模式状态</span><br><span class="line">bin/hdfs dfsadmin -safemode leave # 离开安全模式状态</span><br><span class="line">bin/hdfs dfsadmin -safemode wait # 等待安全模式结束状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以编写脚本，在安全模式下堵塞等待安全模式结束运行接下来的命令</span></span><br></pre></td></tr></table></figure>

<h4 id="多目录"><a href="#多目录" class="headerlink" title="多目录"></a>多目录</h4><blockquote>
<p>NameNode 的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml 配置 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>DataNode 也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml 配置 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop11.png" alt=""></p>
<p>DataNode 掉线时限参数设置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Timeout = 2*dfs.namenode.heartbeat.recheck-interval + 10*dfs.heartbeat.interval --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- hdfs-default.xml 默认配置 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 单位 ms --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 单位 s --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h4><blockquote>
<p>检测 DataNode 节点上的数据是否完整？</p>
</blockquote>
<p>当 DataNode 读取 Block 的时候，它会计算 CheckSum，如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。</p>
<p>Client 读取其他 DataNode 上的 Block，DataNode 在其文件创建后周期验证 CheckSum。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop12.png" alt=""></p>
<h4 id="黑白名单"><a href="#黑白名单" class="headerlink" title="黑白名单"></a>黑白名单</h4><blockquote>
<p>添加到白名单的主机节点，都允许访问 NameNode，不在白名单的主机节点，都会被退出。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 白名单</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在 NameNode 的 /opt/module/hadoop-2.7.2/etc/hadoop 目录下创建 dfs.hosts 文件</span></span><br><span class="line">vi dfs.hosts</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 添加允许的主机名</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 分发配置文件</span></span><br><span class="line">xsync hdfs-site.xml</span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新节点信息</span></span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果数据不均衡，可以用命令实现集群的再平衡</span></span><br><span class="line">./start-balancer.sh</span><br></pre></td></tr></table></figure>

<blockquote>
<p>在黑名单上面的主机都会被强制退出。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 黑名单</span></span><br><span class="line">vi dfs.hosts.exclude</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 添加不允许的主机名</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更新节点信息</span></span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果数据不均衡，可以用命令实现集群的再平衡</span></span><br><span class="line">./start-balancer.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查 Web 浏览器，退役节点的状态为 decommission <span class="keyword">in</span> progress（退役中），说明数据节点正在复制块到其他节点</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 等待退役节点状态为 decommissioned（所有块已经复制完成），停止该节点及节点资源管理器</span></span><br></pre></td></tr></table></figure>

<h4 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h4><blockquote>
<p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 0 表禁用，其他值表示设置文件的存活时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> fs.trash.interval=0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查回收站的间隔时间。如果该值为 0，则该值设置和 fs.trash.interval 的参数值相等。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> fs.trash.checkpoint.interval=0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- min --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>wingo<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 进入回收站的名称 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 回收站在集群中的路径</span></span><br><span class="line">/user/wingo/.Trash/...</span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过程序删除的文件不会经过回收站，需要调用 moveToTrash() 才进入回收站</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Trash trash = New Trash(conf);</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> trash.moveToTrash(path);</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 回复回收站数据</span></span><br><span class="line">hadoop fs -mv /user/wingo/.Trash/Current/user/wingo/input /user/wingo/input</span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空回收站</span></span><br><span class="line">hadoop fs -expunge</span><br></pre></td></tr></table></figure>

<h4 id="快照管理"><a href="#快照管理" class="headerlink" title="快照管理"></a>快照管理</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -allowSnapshot [路径] # 开启指定目录的快照功能</span><br><span class="line">hdfs dfsadmin -disallowSnapshot [路径] # 禁用指定目录的快照功能，默认是禁用</span><br><span class="line">hdfs dts -createSnapshot [路径] # 对目录创建快照</span><br><span class="line">hdfs dts -createSnapshot [路径] [名称] # 指定名称创建快照</span><br><span class="line">hdts dfs -renameSnapshot [路径] [旧名称] [新名称] # 重命名快照</span><br><span class="line">hdfs lsSnapshottableDir # 列出当前用户所有可快照目录</span><br><span class="line">hdfs snapshotDiff [路径] [路径] # 比较两个快照目录的不同之处</span><br><span class="line">hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotname&gt; # 删除快照</span><br></pre></td></tr></table></figure>

<h3 id="MapReduce-1"><a href="#MapReduce-1" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce 是一个分布式运算程序的编程框架,是用户开发“基于 Hadoop 的数据分析应用”的核心框架。</p>
<p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>
<blockquote>
<p>具有易于编程、扩展性良好、高容错性、PB 级别海量数据处理能力等优点；<br>具有不擅长实时计算、不擅长流式计算、不擅长 DAG （有向图）计算，即需要迭代的计算等缺陷。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop13.png" alt=""></p>
<h4 id="实例进程"><a href="#实例进程" class="headerlink" title="实例进程"></a>实例进程</h4><blockquote>
<p>MrAppMaster：负责整个程序的过程调度及状态协调；<br>MapTask：负责 Map 阶段的整个数据处理流程；<br>ReduceTask：负责 Reduce 阶段的整个数据处理流程。</p>
</blockquote>
<h4 id="序列化类型"><a href="#序列化类型" class="headerlink" title="序列化类型"></a>序列化类型</h4><table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td>boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
<h4 id="编程规范"><a href="#编程规范" class="headerlink" title="编程规范"></a>编程规范</h4><blockquote>
<p>用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。</p>
</blockquote>
<h5 id="Mapper-阶段"><a href="#Mapper-阶段" class="headerlink" title="Mapper 阶段"></a>Mapper 阶段</h5><blockquote>
<p>用户自定义的 Mapper 要继承指定的父类；<br>Mapper 的输入数据是 K V 对的开式（K V 的类型可自定义）；<br>Mapper 中的业务逻辑写在 map() 方法中；<br>Mapper 的输出数据是 K Ⅴ 对的形式（K V 的类型可自定义）；<br>map() 方法( MapTask 进程）对每一个 K V&gt;调用一次。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key 偏移量 输入数据 Value 类型 输出数据类型的 K V 👉 对应 Reduce 的输入</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 此方法每次只获取一行</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            k.set(word);</span><br><span class="line">            <span class="comment">// (word, 1)</span></span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Reduce-阶段"><a href="#Reduce-阶段" class="headerlink" title="Reduce 阶段"></a>Reduce 阶段</h5><blockquote>
<p>用户自定义的 Reducer 要继承自己的父类；<br>Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 K V 对的形式<br>Reducer 的业务逻辑写在 reduce() 方法中<br>ReduceTask 进程对每一组相同 K 的 K V 组调用一次 reduce() 方法</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> sum;</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line">	<span class="comment">// 这里直接穿入一个迭代器了，直接循环取值即可</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 累加求和</span></span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">            sum += count.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key,v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Driver-阶段"><a href="#Driver-阶段" class="headerlink" title="Driver 阶段"></a>Driver 阶段</h5><blockquote>
<p>相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群,提交的是封装了 MapReduce 程序相关运行参数的 job 对象。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 获取配置信息以及封装任务</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 设置 jar 加载路径</span></span><br><span class="line">		job.setJarByClass(WordcountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 设置 map 和 reduce 类</span></span><br><span class="line">		job.setMapperClass(WordcountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">		job.setReducerClass(WordcountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 设置 map 输出</span></span><br><span class="line">		job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">		job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 设置最终输出 K V 类型</span></span><br><span class="line">		job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">		job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置输入和输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 提交</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h4><blockquote>
<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle，包含了两端的 Combiner 和 Partition。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop22.png" alt=""></p>
<h5 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h5><blockquote>
<p>要求将统计结果按照条件输出到不同文件中（分区）。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认 Partition 分区机制，用户无法控制 Key 的存储分区</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartironer</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V vlaue, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numberReduceTasks;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需求：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取电话号码的前三位</span></span><br><span class="line">        String preNum = key.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是哪个省</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"136"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(preNum)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Driver</span></span><br><span class="line"><span class="comment">// 指定自定义数据分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 同时指定相应数量的 reduce task</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line"><span class="comment">// job.setNumReduceTasks(1) 会正常运行,只不过会产生一个输出文件</span></span><br><span class="line"><span class="comment">// job.setNumReduceTasks(2) 会报错</span></span><br><span class="line"><span class="comment">// job.setNumReduceTasks(6) 大于 5 程序正常运行，会产生空文件</span></span><br></pre></td></tr></table></figure>

<h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p>MapTask 和 ReduceTask 均会对数据按照 key 进行排序，该操作属 Hadoop 的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上否需要。</p>
<blockquote>
<p>默认排序是按照字曲顺序排序，且实现该排序的方法是快速排序。</p>
</blockquote>
<p>对于 MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。</p>
<p>对于 ReduceTask，它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后, ReduceTask 统一对内存和磁盘上的所有数据进行一次归并排序。</p>
<h5 id="自定义-1"><a href="#自定义-1" class="headerlink" title="自定义"></a>自定义</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bean 对象做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法，就可以实现排序</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">	<span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = -<span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = <span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">		result = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><blockquote>
<p>Combiner 组件的父类就是 Reducer。<br>Combiner 是在每一个 MapTask 所在的节点运行，进行局部汇总，减少网络传输量；Reducer 是接收全局所有 Mapper 的输出结果。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Combiner K V 对应 Reducer K V</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span> , <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 汇总操作</span></span><br><span class="line">		<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span>(IntWritable v : values)&#123;</span><br><span class="line">			count += v.get();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">		context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver</span></span><br><span class="line">job.setCombinerClass(WordcountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h4 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h4><blockquote>
<p>对 Reduce 阶段的数据根据某一个或几个字段进行分组。</p>
<p>自定义类继承 WritableComparator；<br>重写 compare() 方法。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// OrderBean</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> order_id; <span class="comment">// 订单id号</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> price; <span class="comment">// 价格</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">(<span class="keyword">int</span> order_id, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">        <span class="keyword">this</span>.price = price;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeInt(order_id);</span><br><span class="line">        out.writeDouble(price);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        order_id = in.readInt();</span><br><span class="line">        price = in.readDouble();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> order_id + <span class="string">"\t"</span> + price;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Getter / Setter</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 二次排序，即 map 结束之前的一次排序，排序后输出到 reduce</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> result;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (order_id &gt; o.getOrder_id()) &#123;</span><br><span class="line">            result = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order_id &lt; o.getOrder_id()) &#123;</span><br><span class="line">            result = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 价格倒序排序</span></span><br><span class="line">            result = price &gt; o.getPrice() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 编写 OrderSortMapper 类</span></span><br><span class="line"><span class="comment">// 0000001	Pdt_01	222.8</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	OrderBean k = <span class="keyword">new</span> OrderBean();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 封装对象</span></span><br><span class="line">		k.setOrder_id(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">		k.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 写出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 编写 OrderSortGroupingComparator 类</span></span><br><span class="line"><span class="comment">// 在 reduce 前通过此排序进行分组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个构造将比较对象的类传给父类</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        OrderBean aBean = (OrderBean) a;</span><br><span class="line">        OrderBean bBean = (OrderBean) b;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> result;</span><br><span class="line">        <span class="keyword">if</span> (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123;</span><br><span class="line">            result = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123;</span><br><span class="line">            result = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            result = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 编写 OrderSortReducer 类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="comment">// 只能通过 NullWritable.get() 获取空值</span></span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver</span></span><br><span class="line"><span class="comment">// 设置 reduce 端的分组</span></span><br><span class="line">job.setGroupingComparatorClass(OrderGroupingComparator<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h4 id="MapTask"><a href="#MapTask" class="headerlink" title="MapTask"></a>MapTask</h4><p>Read 阶段：MapTask 通过用户编写的 RecordReader，从输入 InputSplit 中解析出一个个 key / value；</p>
<p>Map 阶段：该阶段主要是将解析出的 key / value 交给用户编写 map() 函数处理，并产生一系列新的 key / value；</p>
<p>Collect 收集阶段：在用户编写 map() 函数中，当数据处理完成后，一般会调用 OutputCollector.collect() 输出结果。在该函数内部，它会将生成的 key / value 分区（调用 Partitioner），并写入一个环形内存缓冲区中；</p>
<p>Spill 阶段：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<blockquote>
<p>溢写阶段详情：</p>
<p>利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</p>
<p>按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output/spillN.out（N表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件output/spillN.out.index 中。</p>
</blockquote>
<p>Combine阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output/file.out 中，同时生成相应的索引文件 output/file.out.index。</p>
<blockquote>
<p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并 io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
</blockquote>
<h4 id="ReduceTask"><a href="#ReduceTask" class="headerlink" title="ReduceTask"></a>ReduceTask</h4><p>Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>Merge 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>Sort 阶段：按照 MapReduce 语义，用户编写 reduce() 函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p>
<p>Reduce 阶段：reduce() 函数将计算结果写到 HDFS上。</p>
<blockquote>
<p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop19.png" alt=""></p>
<h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h4><p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop17.png" alt=""></p>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop18.png" alt=""></p>
<h3 id="Hadoop-序列化"><a href="#Hadoop-序列化" class="headerlink" title="Hadoop 序列化"></a>Hadoop 序列化</h3><p>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。反序列化就是序列化的逆过程。</p>
<blockquote>
<p>Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后会附带很多额外的信息（各种校验信息、Header、继承体系等）不便于在网络中高传输。</p>
<p>所以 Hadoop 自己开发了一套序列化机制（Writable）。<br>    紧凑：高效使用存储空间；<br>    快速：读写数据的额外开销小；<br>    可扩展：随着通信协议的升级而可升级；<br>    互操作：支持多语言的交互。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义 bean 对象实现序列化接口</span></span><br><span class="line"><span class="comment">// 实现 writable 接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 反序列化时，需要反射调用空参构造函数，所以必须有</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 写序列化方法</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeLong(upFlow);</span><br><span class="line">		out.writeLong(downFlow);</span><br><span class="line">		out.writeLong(sumFlow);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 反序列化方法</span></span><br><span class="line">	<span class="comment">// 反序列化方法读顺序必须和写序列化方法的写顺序必须一致</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow  = in.readLong();</span><br><span class="line">		<span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 编写toString方法，方便后续打印到文本</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Getter / Setter</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h3><p>MapTask 并行度决定机制：</p>
<blockquote>
<p>数据块：Block 是 HDFS 物理上把数据分成一块一块。</p>
<p>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。切片时不考虑数据集整体，而是逐个针对每一个文件单独切片。</p>
</blockquote>
<p>Block 的大小：local 32 1.x 64 2.x 128 </p>
<h4 id="FileInputFormat"><a href="#FileInputFormat" class="headerlink" title="FileInputFormat"></a>FileInputFormat</h4><p>TextInputFormat 是默认的 FilelnputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，Long writable类型。值是这行的内容，不包括任何行终止符（换行符和回车符）Text 类型。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop14.png" alt=""></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 源码 Debug 主要流程</span></span><br><span class="line"></span><br><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 建立连接</span></span><br><span class="line">	connect();	</span><br><span class="line">		<span class="comment">// 创建提交 Job 的代理</span></span><br><span class="line">		<span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">			<span class="comment">// 判断是 LocalJobRunner 还是 YarnRunner</span></span><br><span class="line">			initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line">    submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line">        <span class="comment">// 创建给集群提交数据的 Stag 路径</span></span><br><span class="line">        Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 jobid ，并创建 Job 路径</span></span><br><span class="line">        JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拷贝 jar 包到集群</span></span><br><span class="line">    	copyAndConfigureFiles(job, submitJobDir);	</span><br><span class="line">        rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 计算切片，生成切片规划文件</span></span><br><span class="line">		writeSplits(job, submitJobDir);</span><br><span class="line">		maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">		input.getSplits(job);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 向 Stag 路径写 XML 配置文件</span></span><br><span class="line">		writeConf(conf, submitJobFile);</span><br><span class="line">		conf.writeXml(out);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 提交 Job，返回提交状态</span></span><br><span class="line">		status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片信息 API</span></span><br><span class="line"><span class="comment">// 获取切片文件名称</span></span><br><span class="line">String name = inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSpli = (FileSplit)context. getInputSplit()</span><br></pre></td></tr></table></figure>

<h4 id="CombineText"><a href="#CombineText" class="headerlink" title="CombineText"></a>CombineText</h4><p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p>
<p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 虚拟存储切片最大值设置</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>); <span class="comment">// 4m</span></span><br></pre></td></tr></table></figure>

<p>将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。</p>
<h4 id="KeyValueText"><a href="#KeyValueText" class="headerlink" title="KeyValueText"></a>KeyValueText</h4><blockquote>
<p>KeyValueTextInputFormat 每一行均为一条记录被分隔符分割为 K V。可以通过在驱动类中设定分隔符。默认分隔符是 tab（\t）。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR,<span class="string">"\t"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop15.png" alt=""></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需求：统计输入文件中每一行的第一个单词相同的行数</span></span><br><span class="line"><span class="comment">// Mapper 编写</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 value</span></span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);  </span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 写出 (第一个单词, 1)</span></span><br><span class="line">        context.write(key, v);  </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable();  </span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> sum = <span class="number">0L</span>;  </span><br><span class="line"></span><br><span class="line">        <span class="comment">// 汇总统计</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;  </span><br><span class="line">            sum += value.get();  </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sum);  </span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        context.write(key, v);  </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 设置切割符</span></span><br><span class="line">        conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 获取 job 对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 jar 包位置，关联 mapper 和 reducer</span></span><br><span class="line">        job.setJarByClass(KVTextDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(KVTextMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(KVTextReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 map 输出 k v 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置最终输出 k v 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入数据路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入格式</span></span><br><span class="line">        job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输出数据路径</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交 job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="NLine"><a href="#NLine" class="headerlink" title="NLine"></a>NLine</h4><blockquote>
<p>NLineInputFormat 代表每个 map 进程处理的 InputSplit 不再按 Block块去划分,而是按 NLineInputFormat 指定的行数来划分。即输入文件的总行数 N = 切片数，如果不整除则切片数 = 商 + 1。</p>
</blockquote>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop16.png" alt=""></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对每个单词进行个数统计,要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置每个切片 InputSplit 中划分三条记录</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 NLineInputFormat 处理记录数  </span></span><br><span class="line">job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h4 id="自定义-2"><a href="#自定义-2" class="headerlink" title="自定义"></a>自定义</h4><blockquote>
<p>无论 HDFS 还是 MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义 InputFormat 实现小文件的合并。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将多个小文件合并成一个 SequenceFile 文件</span></span><br><span class="line"><span class="comment"> * SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对的文件格式</span></span><br><span class="line"><span class="comment"> * SequenceFile 里面存储着多个文件，存储的形式为（文件路径 + 名称）key，文件内容 value</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义 InputFormat</span></span><br><span class="line"><span class="comment">// 定义类继承 FileInputFormat</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputformat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 定义是否可切割，此程序不可切片，最终把所有文件封装到了 value 中</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 实现一次读取一个完整文件封装为 K V</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		WholeRecordReader recordReader = <span class="keyword">new</span> WholeRecordReader();</span><br><span class="line">		recordReader.initialize(split, context);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> recordReader;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义 RecordReader 类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Configuration configuration;</span><br><span class="line">    <span class="keyword">private</span> FileSplit split;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> isProgress = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">private</span> BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="comment">// 初始化数据以及配置</span></span><br><span class="line">        <span class="keyword">this</span>.split = (FileSplit)split;</span><br><span class="line">        configuration = context.getConfiguration();</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 核心业务逻辑，即拼接自定义的 K</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 定义缓存区，获取切片长度</span></span><br><span class="line">            <span class="keyword">byte</span>[] contents = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)split.getLength()];</span><br><span class="line"></span><br><span class="line">            FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">            FSDataInputStream fis = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 获取文件系统，通过路径获取文件系统，一波反向操作</span></span><br><span class="line">                Path path = split.getPath();</span><br><span class="line">                fs = path.getFileSystem(configuration);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 读取数据</span></span><br><span class="line">                fis = fs.open(path);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 读取文件内容</span></span><br><span class="line">                IOUtils.readFully(fis, contents, <span class="number">0</span>, contents.length);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 输出文件内容</span></span><br><span class="line">                value.set(contents, <span class="number">0</span>, contents.length);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 获取文件路径及名称</span></span><br><span class="line">                String name = split.getPath().toString();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 设置输出的 key 值</span></span><br><span class="line">                k.set(name);</span><br><span class="line"></span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">            &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">                IOUtils.closeStream(fis);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 本切片已处理完，清标记位</span></span><br><span class="line">            isProgress = <span class="keyword">false</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 进行其它切片的处理</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> k;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 编写 SequenceFileMapper 类处理流程</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key, value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 编写 SequenceFileReducer 类处理流程</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key, values.iterator().next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 编写SequenceFileDriver类处理流程</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> </span></span><br><span class="line"><span class="function">        IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> String[] &#123; <span class="string">"e:/input/inputinputformat"</span>, <span class="string">"e:/output1"</span> &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 job 对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 jar 包存储位置、关联自定义的 mapper 和 reducer</span></span><br><span class="line">        job.setJarByClass(SequenceFileDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(SequenceFileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(SequenceFileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入的 inputFormat</span></span><br><span class="line">        job.setInputFormatClass(WholeFileInputformat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输出的 outputFormat</span></span><br><span class="line">        job.setOutputFormatClass(SequenceFileOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 map 输出端的 K V 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置最终输出端的 K V 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交 job</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h3><p>默认的输出格式是 TextOutputFormat ，它把每条记录写为文本行，它的键和值可以是任意类型，因为 TextOutputFormat调用 toString() 方法把它们转换为字符串。</p>
<p>将 SequenceFileOutputFormat 输出作为后续 MapReduce 任务的输入，因为其格式紧凑，很容易被压缩。</p>
<h4 id="自定义-3"><a href="#自定义-3" class="headerlink" title="自定义"></a>自定义</h4><blockquote>
<p>为了实现控制最终文件的输岀路径和输出格式，可进行自定义。</p>
<p>自定义一个类继承 FileOutputFormat；<br>改写 RecordWriter，具体改写输出数据的方法 write()。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 继承 FileOutputFormat</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span>			</span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个RecordWriter</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FilterRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重写 write()</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    FSDataOutputStream wingoOut = <span class="keyword">null</span>;</span><br><span class="line">    FSDataOutputStream otherOut = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FilterRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        FileSystem fs;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 获取文件系统</span></span><br><span class="line">            fs = FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建输出文件路径</span></span><br><span class="line">            Path wingoPath = <span class="keyword">new</span> Path(<span class="string">"e:/wingo.log"</span>);</span><br><span class="line">            Path otherPath = <span class="keyword">new</span> Path(<span class="string">"e:/other.log"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建输出流</span></span><br><span class="line">            atguiguOut = fs.create(wingoPath);</span><br><span class="line">            otherOut = fs.create(otherPath);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否包含 wingo 输出到不同文件</span></span><br><span class="line">        <span class="keyword">if</span> (key.toString().contains(<span class="string">"wingo"</span>)) &#123;</span><br><span class="line">            wingoOut.write(key.toString().getBytes());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.write(key.toString().getBytes());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        IOUtils.closeStream(wingoOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);	</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将自定义的输出格式组件设置到 job 中</span></span><br><span class="line">job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat</span></span><br><span class="line"><span class="comment">// 而 fileoutputformat 要输出一个 _SUCCESS 文件，所以，在这还得指定一个输出目录</span></span><br></pre></td></tr></table></figure>

<h3 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h3><blockquote>
<p>order(1001 01 1) join pd(01 小米) 👉 result(001 小米 1)</p>
</blockquote>
<h4 id="Bean"><a href="#Bean" class="headerlink" title="Bean"></a>Bean</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 合并后的 Bean 类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> String order_id; <span class="comment">// 订单 id</span></span><br><span class="line">	<span class="keyword">private</span> String p_id; <span class="comment">// 产品 id</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">int</span> amount; <span class="comment">// 产品数量</span></span><br><span class="line">	<span class="keyword">private</span> String pname; <span class="comment">// 产品名称</span></span><br><span class="line">	<span class="keyword">private</span> String flag; <span class="comment">// 表的标记</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">TableBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">TableBean</span><span class="params">(String order_id, String p_id, <span class="keyword">int</span> amount, String pname, String flag)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">		<span class="keyword">this</span>.p_id = p_id;</span><br><span class="line">		<span class="keyword">this</span>.amount = amount;</span><br><span class="line">		<span class="keyword">this</span>.pname = pname;</span><br><span class="line">		<span class="keyword">this</span>.flag = flag;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Getter / Setter</span></span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeUTF(order_id);</span><br><span class="line">		out.writeUTF(p_id);</span><br><span class="line">		out.writeInt(amount);</span><br><span class="line">		out.writeUTF(pname);</span><br><span class="line">		out.writeUTF(flag);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.order_id = in.readUTF();</span><br><span class="line">		<span class="keyword">this</span>.p_id = in.readUTF();</span><br><span class="line">		<span class="keyword">this</span>.amount = in.readInt();</span><br><span class="line">		<span class="keyword">this</span>.pname = in.readUTF();</span><br><span class="line">		<span class="keyword">this</span>.flag = in.readUTF();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> order_id + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + amount + <span class="string">"\t"</span> ;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    String name;</span><br><span class="line">    TableBean bean = <span class="keyword">new</span> TableBean();</span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输入文件切片</span></span><br><span class="line">        FileSplit split = (FileSplit) context.getInputSplit();</span><br><span class="line">        <span class="comment">// 获取输入文件名称</span></span><br><span class="line">        name = split.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取输入数据</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 不同文件分别处理</span></span><br><span class="line">        <span class="keyword">if</span> (name.startsWith(<span class="string">"order"</span>)) &#123;<span class="comment">// 订单表处理</span></span><br><span class="line">            <span class="comment">// 切割</span></span><br><span class="line">            String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="comment">// 封装bean对象</span></span><br><span class="line">            bean.setOrder_id(fields[<span class="number">0</span>]);</span><br><span class="line">            bean.setP_id(fields[<span class="number">1</span>]);</span><br><span class="line">            bean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">            bean.setPname(<span class="string">""</span>);</span><br><span class="line">            bean.setFlag(<span class="string">"order"</span>);</span><br><span class="line"></span><br><span class="line">            k.set(fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123; <span class="comment">// 产品表处理</span></span><br><span class="line">            <span class="comment">// 切割</span></span><br><span class="line">            String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="comment">// 封装bean对象</span></span><br><span class="line">            bean.setP_id(fields[<span class="number">0</span>]);</span><br><span class="line">            bean.setPname(fields[<span class="number">1</span>]);</span><br><span class="line">            bean.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">            bean.setAmount(<span class="number">0</span>);</span><br><span class="line">            bean.setOrder_id(<span class="string">""</span>);</span><br><span class="line"></span><br><span class="line">            k.set(fields[<span class="number">0</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(k, bean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><blockquote>
<p>处理压力大，并且极易产生数据倾斜。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span>	</span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 准备存储订单的集合</span></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">// 准备 bean 对象</span></span><br><span class="line">        TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">        <span class="comment">// 便利 key = 1 的集合</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean bean : values) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">"order"</span>.equals(bean.getFlag())) &#123; <span class="comment">// 订单表</span></span><br><span class="line">                <span class="comment">// 拷贝传递过来的每条订单数据到集合中</span></span><br><span class="line">                TableBean orderBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(orderBean, bean);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                orderBeans.add(orderBean);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">// 产品表</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 此处已在 Map 做好按 p_id 排序工作，最后一定是一个 pd 表信息</span></span><br><span class="line">                    <span class="comment">// 拷贝传递过来的产品表到内存中</span></span><br><span class="line">                    BeanUtils.copyProperties(pdBean, bean);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 表的拼接</span></span><br><span class="line">        <span class="keyword">for</span>(TableBean bean : orderBeans)&#123;</span><br><span class="line">            bean.setPname (pdBean.getPname());</span><br><span class="line">            <span class="comment">// 数据写出去</span></span><br><span class="line">            context.write(bean, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 路径</span></span><br><span class="line">        args = <span class="keyword">new</span> String[]&#123;<span class="string">"e:/input/inputtable"</span>,<span class="string">"e:/output1"</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取配置信息，或者 job 对象实例</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定本程序的 jar 包所在的本地路径</span></span><br><span class="line">        job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定本业务 job 要使用的 Mapper / Reducer 业务类</span></span><br><span class="line">        job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 Mapper 输出数据的 k v 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定最终输出的数据的 k v 类型</span></span><br><span class="line">        job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 job 的输入原始文件所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h3><blockquote>
<p>适用于一张表十分小、一张表很大的场景。</p>
<p>采用 Distributed cache：在 Mapper 的 setup 阶段，将文件读取到缓存集合中。<br>    job.addCacheFile(new URI(“file://e:/cache/pd.txt”));</p>
</blockquote>
<h4 id="Driver-1"><a href="#Driver-1" class="headerlink" title="Driver"></a>Driver</h4><blockquote>
<p>添加缓存文件。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 路径</span></span><br><span class="line">        args = <span class="keyword">new</span> String[]&#123;<span class="string">"e:/input/inputtable"</span>,<span class="string">"e:/output1"</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取配置信息，或者 job 对象实例</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定本程序的 jar 包所在的本地路径</span></span><br><span class="line">        job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定本业务 job 要使用的 Mapper / Reducer 业务类</span></span><br><span class="line">        job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 Mapper 输出数据的 k v 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定最终输出的数据的 k v 类型</span></span><br><span class="line">        job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 job 的输入原始文件所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">		job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///e:/input/inputcache/pd.txt"</span>));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">		job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取缓存的文件</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        String path = cacheFiles[<span class="number">0</span>].getPath().toString();</span><br><span class="line"></span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader</span><br><span class="line">            (<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), <span class="string">"UTF-8"</span>));</span><br><span class="line"></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span>(StringUtils.isNotEmpty(line = reader.readLine()))&#123;</span><br><span class="line">            <span class="comment">// 切割</span></span><br><span class="line">            String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="comment">// 缓存数据到集合 (01 小米)</span></span><br><span class="line">            pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关流</span></span><br><span class="line">        reader.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 截取</span></span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">// 获取产品 id</span></span><br><span class="line">        String pId = fields[<span class="number">1</span>];</span><br><span class="line">        <span class="comment">// 获取商品名称</span></span><br><span class="line">        String pdName = pdMap.get(pId);</span><br><span class="line">        <span class="comment">// 拼接</span></span><br><span class="line">        k.set(line + <span class="string">"\t"</span>+ pdName);</span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(k, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="数据清洗-ETL"><a href="#数据清洗-ETL" class="headerlink" title="数据清洗 ETL"></a>数据清洗 ETL</h3><blockquote>
<p>Hadoop 为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。</p>
</blockquote>
<h4 id="Map-1"><a href="#Map-1" class="headerlink" title="Map"></a>Map</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 1 行数据</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 解析日志，即此条日志是否有意义</span></span><br><span class="line">        <span class="keyword">boolean</span> result = parseLog(line,context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 日志不合法退出</span></span><br><span class="line">        <span class="keyword">if</span> (!result) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 key</span></span><br><span class="line">        k.set(line);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出数据</span></span><br><span class="line">        context.write(k, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析日志</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 截取</span></span><br><span class="line">        String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 日志长度大于 11 的为合法</span></span><br><span class="line">        <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="comment">// 系统计数器，记录一条合法的日志</span></span><br><span class="line">            context.getCounter(<span class="string">"map"</span>, <span class="string">"true"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 记录一条不合法的日志</span></span><br><span class="line">            context.getCounter(<span class="string">"map"</span>, <span class="string">"false"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Driver-2"><a href="#Driver-2" class="headerlink" title="Driver"></a>Driver</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 路径</span></span><br><span class="line">        args = <span class="keyword">new</span> String[]&#123;<span class="string">"e:/input/inputlog"</span>, <span class="string">"e:/output1"</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取配置信息，或者 job 对象实例</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定本程序的 jar 包所在的本地路径</span></span><br><span class="line">        job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定本业务 job 要使用的 Mapper / Reducer 业务类</span></span><br><span class="line">        job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 Mapper 输出数据的 k v 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定最终输出的数据的 k v 类型</span></span><br><span class="line">        job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 job 的输入原始文件所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///e:/input/inputcache/pd.txt"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><blockquote>
<p>适当压缩是提高 Hadoop 运行效率的一种优化策略。</p>
</blockquote>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否，需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
<blockquote>
<p>为了支持多种压缩 / 解压缩算法，Hadoop 引入了编码 / 解码器</p>
</blockquote>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<blockquote>
<p>启动压缩功能的参数配置。</p>
</blockquote>
<table>
<thead>
<tr>
<th>参数</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs</td>
<td>输入压缩</td>
<td>文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>mapper 输出</td>
<td>这个参数设为 true 启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>mapper 输出</td>
<td>企业多使用 LZO 或 Snappy 编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>reducer 输出</td>
<td>这个参数设为 true 启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>reducer 输出</td>
<td>使用标准工具或者编解码器，如 gzip 和 bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>reducer 输出</td>
<td>SequenceFile 压缩类型： NONE 和 BLOCK</td>
</tr>
</tbody></table>
<h4 id="Driver-定义压缩"><a href="#Driver-定义压缩" class="headerlink" title="Driver 定义压缩"></a>Driver 定义压缩</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Map 输出端采用压缩</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启 map 端输出压缩</span></span><br><span class="line">        configuration.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 设置 map 端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec<span class="class">.<span class="keyword">class</span>, <span class="title">CompressionCodec</span>.<span class="title">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce 输出端采用压缩</span></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="YARN-1"><a href="#YARN-1" class="headerlink" title="YARN"></a>YARN</h3><p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop20.png" alt=""></p>
<p><img src="http://welab-wingo.gitee.io/image/2020/04/Hadoop/Hadoop21.png" alt=""></p>
<h4 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h4><blockquote>
<p>Hadoop 作业调度器主要有三种：</p>
<p>FIFO 先到先服务、Capacity Scheduler 和 Fair Schedule。</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-default.xml --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Capacity Scheduler，容量调度器。</p>
<blockquote>
<p>支持多队列，每个队列配置一定的资源量，每个队列采用 FIFO 调度策略；<br>为防止同一用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定；<br>运行任务数 / 计算资源 👉 王最闲的队列里交任务。</p>
</blockquote>
<p>Fair Scheduler，公平调度器。</p>
<blockquote>
<p>支持多队列多用户，每个队列中的资源量可以配置，同队列中的作业公平共享队列中所有资源；<br>每个队列中的 job 按照优先级分配资源，优先级越高分配的资源越多，但是每个 job 都会分配到资源以确保公平；（差额：job 需要的资源 - job 实际获取的资源）<br>在同一个队列中，job 旳资源缺额越大，越先获得资源优先执行。</p>
</blockquote>
<h4 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h4><blockquote>
<p>某些任务过慢，则为其启动多个备份任务，同时运行，谁先运行完成用谁的结果。</p>
<p>每个 Task 只能有一个备份任务；<br>当前 Job 已完成的 Task 必须不小于0.05（5%）；<br>开启推测执行参数设置。mapred-site.xml 文件中默认是打开的。</p>
</blockquote>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2020-07-10T03:39:05.616Z" itemprop="dateUpdated">2020-07-10 11:39:05</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="Wingo">
            Wingo
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>


            


        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2020/04/26/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/Linux%20%E5%9F%BA%E7%A1%80/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Linux 基础</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2020/04/23/Python/sklearn/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">sklearn</h4>
      </a>
    </div>
  
</nav>



    




















</article>



</div>

        <footer class="footer">
    <div class="top">
        

        <p>
            
            <span>得失从缘，心无增减</span>
        </p>
    </div> 
    <div class="bottom">
        <p><span>Wingo &copy; 2020</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>


    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: false, REWARD: false };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>










</body>
</html>
