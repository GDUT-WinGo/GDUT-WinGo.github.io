{"meta":{"title":"Wingo's Blog","subtitle":"","description":"","author":"Wingo","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2020-08-04T14:21:58.399Z","updated":"2020-01-20T15:05:42.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-08-04T14:21:58.399Z","updated":"2020-07-10T04:26:45.605Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"工作零碎知识点记录","slug":"生活杂记/工作零碎知识点记录","date":"2020-09-05T01:18:52.000Z","updated":"2020-09-05T02:54:05.443Z","comments":true,"path":"2020/09/05/生活杂记/工作零碎知识点记录/","link":"","permalink":"http://yoursite.com/2020/09/05/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/%E5%B7%A5%E4%BD%9C%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AE%B0%E5%BD%95/","excerpt":"记录工作中遇到的知识点疏漏，方便自己查漏补缺。","text":"记录工作中遇到的知识点疏漏，方便自己查漏补缺。 ML 常用名词解释大佬：李沐、杨强、陈天奇 横向联邦学习：其核心就是使训练的样本总数增加（多方数据的特征重叠较多） 纵向联邦学习：使训练样本的特征维度增加，在进行模型训练之前需要做数据对齐（多方数据的 id 重叠较多） KS：模型风险区分能力的评估（好坏样本之间的差值，即区分能力） ROC（Receiver Operating Characteristic Curve）：ROC 曲线下各部分的面积求和为 AUC AUC（Area Under Curve）：衡量学习器优劣的一种性能指标 相关原理：混淆矩阵 TOP N 准确率和召回率： Top 1：最大项正确则预测正确 Top 5：概率最大的前 5 项中有正确的即为正确 IV：用于描述信息的特征价值 以 WOE 为基础 WOE 编码（Weight of Evidence）：即证据权重，是对原始自变量的一种编码形式 Pbin = bin 中 1 : 0 的 Rate = A Pall = all 1 : 0 的 Rate = B WOE = A / B BadRate 编码：bin 中坏样本率 CRT（Click Through Rate）：广告点击率预测模型 bin：核心是将连续的特征离散化 等频分箱：每个区间包含大致相等的实例数量 等距分箱：每个区间的长度一致 卡方分箱：相邻的具有类似分布的期间应该合并 卡方值：越低其类分布越相似 在什么情况下将连续的特征离散化之后可以获得更好的效果？ 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个海量离散特征+简单模型同少量连续特征+复杂模型的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以 n 个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 广义线性模型（Generalized Linear Model）：最近在机器学习基础一书中看到深度学习算法就是一种广义的线性模型。 Pearson Coefficient：用于度量两个变量 X 和 Y 之间的线性相关程度。 集成学习： 用于减少方差的 bagging 用于减少偏差的 boosting：AdaBoost（Adaptive Boosting） 用于提升预测结果的 stacking CART：Classification And Regression Tree（分类回归树） GBDT：Gradient Boosting Decision Tree（梯度提升决策树） OLTP：Online Transaction Processing（行式存储） OLAP：Online analytical Processing（列式存储） 梯度下降：考虑坡度最大的方向（一阶） 拟牛顿法：不仅考虑此次坡度，还考虑了下次（二阶） PMML（Predictive Model Markup Language）：预测模型标记语言，各种开发语言都可以使用相应的包，把模型文件转成这种中间格式，而另外一种开发语言，可以使用相应的包导入该文件做线上预测。 矩阵存储格式在矩阵中，若数值为 0 的元素数目远远多于非 0 元素的数目，并且非 0 元素分布没有规律时，则称该矩阵为稀疏矩阵；与之相反，若非 0 元素数目占大多数时，则称该矩阵为稠密矩阵。 稠密度：非零元素的总数 / 矩阵所有元素的总数。 由于稀疏矩阵中重复的 0 非常多，因此在存储矩阵时非常有必要对其压缩，下面介绍了几种常用的稀疏矩阵存储格式。 COOCOO（Coordinate）：仅存储非 0 数据的行、列、值。这种存储方式虽然简单，但会存在行或列重复出现的情况，这些重复值也需要被压缩。 矩阵转为包含 row、column、value 信息的列表 CSR &amp; CSCCSR（Compressed Sparse Row）：仅存储非 0 数据的列、值、行偏移。行偏移是指每行的第一个非 0 数在值（value）中的位置。 CSC（Compressed Sparse Column）：仅存储非 0 数据的行、值、列偏移。列偏移是指每列的第一个非 0 数在值（value）中的位置。 当使用 COO 时，若重复行过多，则可以使用 CSR 来忽略行；若重复列过多，则可以使用 CSC 忽略列 配置 SublimeSublime 配置自定义的 Python 运行环境。 123456&#123; \"cmd\": [\"D:\\\\DevelopTool\\\\Annaconda\\\\envs\\\\python37\\\\python.exe\", \"-u\", \"$file\"], \"file_regex\": \"^[ ]File \\\"(…?)\\\", line ([0-9]*)\", \"selector\": \"source.python\" , \"encoding\": \"cp936\" &#125; Jenkins1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 查看是否有 jenkins 安装包rpm -qa | grep jenkins# 下载并安装 jenkinssudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reposudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key# 修改配置 jenkins 默认配置文件sudo vim /etc/sysconfig/jenkins# 用户改为 root# 端口改为 8081# 更改检查网络状态的 URL： connectionCheckUrl 改为 baidusudo vim /var/lib/jenkins/updates/default.json# 进入客户端页面更改下载源 &gt; 插件的高级 &gt;升级站点的 URL 改为 # https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json# java 安装yum search java-1.8.0# 注意！devel 才有 jdksudo yum install java-1.8.0-openjdk-devel.x86_64# 默认安装目录 /usr/lib/jvm# Maven 安装sudo wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gztar -vxf apache-maven-3.6.3-bin.tar.gz# 配置环境变量sudo vim /etc/profile# MAVEN_HOME=/opt/maven/maven3# export MAVEN_HOME# export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin# 刷新环境配置source /etc/profile# 检验环境是否生效mvn -v# NodeJS 安装# 官网 nodejs.org/dist/latest/ 下载 node 安装包sudo wget [URL]/xxx.tar.gz# 解压到指定目录tar -xvf xxx# 修改环境变量sudo vi /etc/profile# NODE_HOME=xxx# export PATH=$&#123;PATH&#125;:$&#123;NODE_HOME&#125;/bin# Jenkins 配置全局 JDK / Maven# Jenkins 指定的操作用户修改sudo vim /etc/sysconfig/jenkins# Jenkins 的 nohup 命令执行问题sudo touch jenkins-run.shsudo vi jenkins-run.sh# BUILD_ID=dontKillMe# cd /opt/welab/board-service/# nohup java -jar -Dconfig.path=/opt/welab/config.properties board-service-git.jar&gt;running.out 2&gt;&amp;1 &amp;# sleep 3s 插件下载： Localization: Chinese (Simplified)：中文语言包 GIT server：提供 Git 服务 GIT：用于从 Git 拉取项目 Maven Integration：用于构建 Maven 项目 WSL1234567891011121314151617181920212223242526272829303132# PowerShell 管理员Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux# cmd 管理 WSL 命令net start / stop LxssManager# 换源sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup、sudo vim sources.list###deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse#### 更新源sudo apt-get updatesudo apt-get upgrade# 安装 xming 选择 Don’t install an SSH client# 配置显示器echo \"export DISPLAY=:0\" &gt;&gt; ~/.bashrcsource ~/.bashrc# 通过安装 gedit 完成显示环境配置sudo apt install gedit Python 命令行运行 Python 项目时遇到的项目引用问题。 1234# 文件首添加以下代码import syssys.path.append('[项目根目录]')print(sys.path) 语法@property12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value 一个 getter 方法变成属性，只需要加上 @property 就可以了，此时，@property 本身又创建了另一个装饰器 @score.setter，负责把一个 setter 方法变成属性赋值，于是，我们就拥有一个可控的属性操作。 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60)&gt;&gt;&gt; s.score # OK，实际转化为s.get_score()60&gt;&gt;&gt; s.score = 9999Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 还可以定义只读属性，只定义 getter 方法，不定义 setter 方法就是一个只读属性。 12345678910111213class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2014 - self._birth 上面的 birth 是可读写属性，而 age 就是一个只读属性，因为 age 可以根据 birth 和当前时间计算出来。 @staticmethod123456789class C(object): @staticmethod def f(): print('runoob');C.f(); # 静态方法无需实例化cobj = C()cobj.f() # 也可以实例化后调用 args and *kwargs 普通形式的可变参数列表 k-v 形式的可变参数列表 @six.add_metaclass(abc.ABCMeta)声明一个抽象基础类 变量命名 _instance_var：实例变量 __private_var：私有实例变量（外部访问报错） __class__：python 的自有变量 wait 语句with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的”清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等 关键字raise：调起异常 global：每个 python 函数拥有对应的 __globals__ 字典，该字典与函数所属模块的 __dict__ 字典完全相同。函数的全局变量也会从这个字典中获取 1234567def main(): global a a = 4 print(main.__globals__.keys()) print(main.__globals__['a'])# dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__annotations__', '__builtins__', '__file__', '__cached__', 'foo', 'f', 'dis', 'main', 'a'])# 4 self : 代表实例对象本身 cls：代表类对象本身（用于类方法 @staticmethod / @classmethod） __init__ &amp; __new__：实际上，__init__ 函数并不是真正意义上的构造函数，__init__ 方法做的事情是在对象创建好之后初始化变量。真正创建实例的是 __new__ 方法 if __name__ == &#39;__main__&#39;：我们简单的理解就是：如果模块是被直接运行的，则代码块被运行，如果模块是被导入的，则代码块不被运行。 __init__.py__init__.py 文件的作用是将文件夹变为一个 Python 模块，Python 中的每个模块的包中，都有 __init__.py 文件。 通常 __init__.py 文件为空，但是我们还可以为它增加其他的功能。我们在导入一个包时，实际上是导入了它的 __init__.py 文件。这样我们可以在 __init__.py 文件中批量导入我们所需要的模块，而不再需要一个一个的导入。 yield12345678910111213141516def foo(): print(\"starting...\") while True: res = yield 4 print(\"res:\",res)g = foo()print(next(g))print(\"*\"*20)print(next(g)'''starting...4********************res: None4''' 这里需要了解的重点是，调用 next() 后 g 定义的方法才开始运行。 程序在运行到 yield 关键字时会直接 return，之后程序通知，注意 ，此时赋值操作还未进行。 再次调用 next() 程序从停止的地方再次开始运行，此时开始进行赋值操作，但值已经被 return 出去了，所以赋值为 Node。 while 循环再次 return 4 然程序停止。 12345678910111213141516def foo(): print(\"starting...\") while True: res = yield 4 print(\"res:\",res)g = foo()print(next(g))print(\"*\"*20)print(g.send(7))'''starting...4********************res: 74''' 通过 send 函数可以进行动态的赋值。 Linux 命令解释wget（World Wide Web Get）是一种下载工具，类似于迅雷。 rpm（Redhat Package Management）是安装包管理器。 yum 也是包管理器，可以自动下载和安装依赖的 rpm 软件。 SparkSpark RDDRDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。 通过读取文件生成 RDD。 通过并行化的方式创建 RDD。 还可以通过读取数据库等数据库操作获取 RDD。 算子Spark 支持两个类型（算子）操作：Transformation 和 Action。 Transformation主要做的是就是将一个已有的 RDD 生成另外一个 RDD。Transformation 具有 lazy 特性（延迟加载）。Transformation 算子的代码不会真正被执行。只有当我们的程序里面遇到一个 action 算子的时候，代码才会真正的被执行。这种设计让 Spark 更加有效率地运行。 Action触发代码的运行，我们一段 spark 代码里面至少需要有一个 action 操作。 依赖 RDD 的宽依赖和窄依赖 由于 RDD 是粗粒度的操作数据集，每个 Transformation 操作都会生成一个新的 RDD，所以 RDD 之间就会形成类似流水线的前后依赖关系；RDD 和它依赖的父 RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 窄依赖是指每个父 RDD的 一个 Partition 最多被子 RDD的 一个 Partition 所使用，例如 map、filter、union 等操作都会产生窄依赖；（独生子女）。 宽依赖是指一个父 RDD 的 Partition 会被多个子 RDD 的 Partition 所使用，例如 groupByKey、reduceByKey、sortByKey 等操作都会产生宽依赖；（超生）。","categories":[],"tags":[]},{"title":"FATE 学习","slug":"联邦学习/FATE 学习","date":"2020-08-29T04:05:58.000Z","updated":"2020-08-29T04:17:18.638Z","comments":true,"path":"2020/08/29/联邦学习/FATE 学习/","link":"","permalink":"http://yoursite.com/2020/08/29/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/FATE%20%E5%AD%A6%E4%B9%A0/","excerpt":"联邦学习系列课程笔记。","text":"联邦学习系列课程笔记。 背景 算力提升 数据爆发 数据孤岛 数据监管 联邦学习 数据隔离 无损 对等 共同获益 分类 横向联邦：特征相同，通过联邦学习获取更多的样本，解决单边样本量不同的问题。 纵向联邦：数据样本 ID 一致（数据对齐），通过联邦学习丰富样本特征，提高模型性能。","categories":[{"name":"联邦学习","slug":"联邦学习","permalink":"http://yoursite.com/categories/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"FATE","slug":"FATE","permalink":"http://yoursite.com/tags/FATE/"}]},{"title":"机器学习基础教程","slug":"机器学习/机器学习基础教程","date":"2020-08-18T12:27:37.000Z","updated":"2020-08-30T13:53:33.202Z","comments":true,"path":"2020/08/18/机器学习/机器学习基础教程/","link":"","permalink":"http://yoursite.com/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/","excerpt":"深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。","text":"深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。 引言机器学习（machine learning）是从数据中提取知识。它是统计学、人工智能和计算机科学交叉的研究领域，也被称为预测分析（predictive analytics）或统计学习（statistical learning）。 最成功的机器学习算法是能够将决策过程自动化的那些算法，这些决策过程是从已知示例中泛化得出的。 机器学习可以分为两大类： 监督学习算法：从输入 / 输出对中进行学习的机器学习算法。 无监督学习算法：只有输入数据是已知的，没有为算法提供输出数据。 无论是监督学习任务还是无监督学习任务，将输入数据表征为计算机可以理解的形式都是十分重要的。在机器学习中，这里的每个实体或每一行被称为一个样本（sample）或数据点，而每一列（用来描述这些实体的属性）则被称为特征（feature）。 构建良好的数据表征的方法称为特征提取（feature extraction）或特征工程（feature engineering）。 环境搭建Anaconda 管理 Python 环境，已经预先安装好了 Python 科学计算包。 123456789101112conda --versionconda create -n python37 python=3.7conda env listconda activate python37conda remove --name python36 --all# 第三方包pip install requestspip uninstall requests# 查看环境包信息conda list# 若直接安装 Python 可用 pip 安装将要用到的所有包pip install numpy scipy matplotlib ipython scikit-learn pandas 必要的库和工具的使用。 NumPy123456789## Numpy 数组import numpy as npx = np.array([[1, 2, 3], [4, 5, 6]])print(\"x:\\n&#123;&#125;\".format(x))'''x:[[1 2 3] [4 5 6]]''' SciPy12345678910111213141516171819202122232425262728293031323334353637## Scipy 是用于科学计算的函数集合# 最重要的是 scipy.sparse 给出稀疏矩阵from scipy import sparseimport numpy as np# 创建一个二维 NumPy 数组，对角线为 1，其余都为 0eye = np.eye(4)print(\"NumPy array:\\n&#123;&#125;\".format(eye))'''NumPy array:[[ 1. 0. 0. 0.] [ 0. 1. 0. 0.] [ 0. 0. 1. 0.] [ 0. 0. 0. 1.]]'''# 将 NumPy 数组转换为CSR格式的SciPy稀疏矩阵# 只保存非零元素sparse_matrix = sparse.csr_matrix(eye)print(\"\\nSciPy sparse CSR matrix:\\n&#123;&#125;\".format(sparse_matrix))'''SciPy sparse CSR matrix: (0, 0) 1.0 (1, 1) 1.0 (2, 2) 1.0 (3, 3) 1.0'''## 直接创建稀疏矩阵 COO 格式# 创建 [1. 1. 1. 1.]data = np.ones(4)# 创建 [0 1 2 3]row_indices = np.arange(4)# 创建 [0 1 2 3]col_indices = np.arange(4)# 直接创建稀疏矩阵，仅提供非 0 的格子与存储的数据eye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))print(\"COO representation:\\n&#123;&#125;\".format(eye_coo)) matplotlib1234567891011# 使用魔法命令来显示图像 否则需要 plt.show%matplotlib inlineimport numpy as npimport matplotlib.pyplot as plt# 在 -10 和 10 之间生成一个数列，共 100 个数x = np.linspace(-10, 10, 100)# 用正弦函数创建每个 x 的 y 值y = np.sin(x)# plot 函数绘制一个数组关于另一个数组的折线图 (marker=\"x\" 看起来是用字母 x 在曲线上标记一下而已)plt.plot(x, y, marker=\"x\") pandaspandas 是用于处理和分析数据的 Python 库。它基于一种叫作 DataFrame 的数据结构，这种数据结构模仿了 R 语言中的 DataFrame。 简单来说，一个 pandas DataFrame 是一张表格，类似于 Excel 表格。pandas 中包含大量用于修改表格和操作表格的方法，尤其是可以像 SQL 一样对表格进行查询和连接。 12345678910111213141516import pandas as pdfrom IPython.display import display## 利用字典创建 DataFramedata = &#123; 'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"], 'Location' : [\"New York\", \"Paris\", \"Berlin\", \"London\"], 'Age' : [24, 13, 53, 33]&#125;# 创建表格data_pandas = pd.DataFrame(data)# 打印表格display(data_pandas)# 选择年龄大于 30 的所有行display(data_pandas[data_pandas.Age &gt; 30]) 鸢尾花分类前提：有一些鸢尾花的测量数据，这些花之前已经被植物学专家鉴定为属于 setosa、versicolor 或 virginica 三个品种之一。另外，这些测量数据可以确定每朵鸢尾花所属的品种。 目标：构建一个机器学习模型，可以从这些已知品种的鸢尾花测量数据中进行学习，从而能够预测新鸢尾花的品种。 分析：因为我们有已知品种的鸢尾花的测量数据，所以这是一个监督学习问题。我们要在多个选项中预测其中一个（鸢尾花的品种）。这是一个分类（classification）问题，可能的输出（鸢尾花的不同品种）叫作类别（class）。数据集中的每朵鸢尾花都属于三个类别之一，所以这是一个三分类问题。单个数据点（一朵鸢尾花）的预期输出是这朵花的品种。对于一个数据点来说，它的品种叫作标签（label）。 取得数据1234567891011121314151617181920212223from sklearn.datasets import load_iris# 加载数据iris_dataset = load_iris()# 数据集包含了哪些信息print(\"Keys of iris_dataset: \\n&#123;&#125;\".format(iris_dataset.keys()))'''Keys of iris_dataset:dict_keys(['target_names', 'feature_names', 'DESCR', 'data', 'target'])'''# 数据集的描述信息 DESCRprint(iris_dataset['DESCR'][:193] + \"\\n...\")# 数据集包含了 3 个品种，也就是分类print(\"Target names: &#123;&#125;\".format(iris_dataset['target_names']))# 数据集的特征列表print(\"Feature names: \\n&#123;&#125;\".format(iris_dataset['feature_names']))# 样本数据在 data 里，是 numpy 的 array，其中 shape 记录了有多少样本，每行样本有几个属性print(\"Type of data: &#123;&#125;\".format(type(iris_dataset['data'])))print(\"Shape of data: &#123;&#125;\".format(iris_dataset['data'].shape))# 样本数据print(\"First five rows of data:\\n&#123;&#125;\".format(iris_dataset['data'][:5]))# 分类数据在 target 里，也是 numpy arrayprint(\"Shape of target: &#123;&#125;\".format(iris_dataset['target'].shape)) 划分数据我们想要利用这些数据构建一个机器学习模型，用于预测新测量的鸢尾花的品种。但在将模型应用于新的测量数据之前，我们需要知道模型是否有效，也就是说，我们是否应该相信它的预测结果。 我们不能将用于构建模型的数据用于评估模型。因为我们的模型会一直记住整个训练集，所以对于训练集中的任何数据点总会预测正确的标签。这种“记忆”无法告诉我们模型的泛化（generalize）能力如何（换句话说，在新数据上能否正确预测）。 通常的做法是将收集好的带标签数据（此例中是 150 朵花的测量数据 分成两部分。一部分数据用于构建机器学习模型，叫作训练数据（training data）或训练集（training set）。其余的数据用来评估模型性能，叫作测试数据（test data）、测试集（test set）或留出集（hold-out set）。 12345678910111213# 切分样本from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitiris_dataset = load_iris()# train_test_split 函数利用伪随机数生成器将数据集打乱# 为保证多次运行输出一致，这里指定 random_state = 0（随机生成器种子）X_train, X_test, y_train, y_test = \\ train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)# 75% 25%print(X_train.shape, y_train.shape)print(X_test.shape, y_test.shape) 观察数据在构建机器学习模型之前，通常最好检查一下数据，看看如果不用机器学习能不能轻松完成任务，或者需要的信息有没有包含在数据中。此外，检查数据也是发现异常值和特殊值的好方法。 检查数据的最佳方法之一就是将其可视化。一种可视化方法是绘制散点图（scatter plot）。数据散点图将一个特征作为 x 轴，另一个特征作为 y 轴，将每一个数据点绘制为图上的一个点。用这种方法难以对多于 3 个特征的数据集作图。解决这个问题的一种方法是绘制散点图矩阵（pair plot），从而可以两两查看所有的特征。 1234567891011121314151617from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport pandas as pdimport mglearniris_dataset = load_iris()X_train, X_test, y_train, y_test = \\ train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)# 利用 X_train 中的数据创建 DataFrame# 利用 iris_dataset.feature_names 中的字符串对数据列进行标记iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)# 利用 DataFrame 创建散点图矩阵，按 y_train 着色# mglearn 为自定义图像美化库grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), \\ marker='o', hist_kwds=&#123;'bins': 20&#125;, s=60, alpha=.8, cmap=mglearn.cm3) 这个画散点矩阵图的函数说实话我看不太懂。 模型构建好了，现在让我们开始构建真实的机器学习模型了。让我们先从一个比较容易理解的算法：KNN 分类器算法（k 近邻分类器）。 k 近邻算法中 k 的含义是，我们可以考虑训练集中与新数据点最近的任意 k 个邻居（比如说，距离最近的 3 个或 5 个邻居），而不是只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。 1234567891011121314151617181920212223242526272829from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifieriris_dataset = load_iris()# 切分 75% 的训练集和 25% 的测试集, 随机打乱数据的种子固定为 0X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)# K 邻近分类器, 算法取最近 1 个点的标签作为分类 即 k=1knn = KNeighborsClassifier(n_neighbors=1)# 训练模型knn.fit(X_train, y_train)# 构造一个测试的数据, 需要作为一行放在矩阵里X_new = np.array([[5, 2.9, 1, 0.2]])# 预测分类y_new = knn.predict(X_new)# 分类是整形 0, 1, 2 分别对应相应的类型print(y_new)# 对应成分类的名字print(iris_dataset.target_names[y_new])# 在测试集做预测y_pred = knn.predict(X_test)# 计算预测的精度, 其中 y_pred==y_test 得到的是一个 bool 向量, np.mean 返回 True 的比例print(\"Test set score: &#123;:.2f&#125;\".format(np.mean(y_pred == y_test)))# 也可以直接用模型的 score 方法来完成精度计算print(\"Test set score: &#123;:.2f&#125;\".format(knn.score(X_test, y_test))) 小结 监督学习与非监督学习 可能的品种叫做分类，具体每个样本的品种就叫做标签 数据集需划分为训练集和测试集 数据集包括了 X 和 y，其中 X 是特征的二维数组，y 是标签的一维数组 本章用到了 scikit-learn 中任何机器学习算法的核心方法：fit、predict、score 监督学习记住，每当想要根据给定输入预测某个结果，并且还有输入 / 输出对的示例时，都应该使用监督学习。 基本概念分类与回归 简单来说，分类是预测标签，包括二分类与多分类。回归是预测连续值，比如预测收入、房价。 泛化、过拟合与欠拟合 随着模型算法逐渐复杂，其在训练集上的预测精度将提高，但在测试集上的预测精度将降低，因此模型的复杂度需要折衷。 模型过于复杂，将导致模型泛化能力差，即过拟合。 模型过于简单，将导致模型精度在训练集表现就很差，更不用说测试集的表现了，此时即欠拟合。 模型复杂度与数据集大小的关系 数据点的值变化范围越大，则可以应用更加复杂的模型，预测的表现也会越好。更多的训练数据往往伴随着更大范围的特征值变化，因此可以应用更复杂的模型算法。 但注意，如果是非常类似的数据点，无论数据集多大也是无济于事的。 算法现在开始介绍最常用的机器学习算法，并解释这些算法如何从数据中学习以及如何预测。 许多算法都有分类和回归两种形式。 KNN前面的鸢尾花的例子已经简单的介绍过 KNN 的原理，这里我们先用一张图来简单的复习一下，这张图表示的是 n_neighbors=3 的情况（3 个邻居）。 好了，我们将在现实世界的乳腺癌数据集上进行研究。先将数据集分成训练集和测试集，然后用不同的邻居个数对训练集和测试集的性能进行评估。 12345678910111213141516171819202122232425262728293031323334from sklearn.datasets import load_breast_cancerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_splitimport matplotlib.pyplot as plt# 加载数据cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=66)# 记录不同 n_neighbors 情况下，模型的训练集精度与测试集精度的变化training_accuracy = [] test_accuracy = []# n_neighbors 取值从 1 到 10 neighbors_settings = range(1, 11)for n_neighbors in neighbors_settings: # 模型对象 clf = KNeighborsClassifier(n_neighbors=n_neighbors) # 训练 clf.fit(X_train, y_train) # 记录训练集精度 training_accuracy.append(clf.score(X_train, y_train)) # 记录测试集精度 test_accuracy.append(clf.score(X_test, y_test))# 画出 2 条曲线，横坐标是邻居个数，纵坐标分别是训练集精度和测试集精度plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")plt.ylabel(\"Accuracy\")plt.xlabel(\"n_neighbors\")plt.legend() 通过前面的介绍，我们可以知道是否有乳腺癌是一个二分类的问题，这里的 KNN 属于分类算法形式。但其实 KNN 也可以用于回归算法形式。 我们先看看 wave 数据集在 n_neighbors = 3 的图示情况。在使用多个近邻时，预测结果为这些近邻的平均值。 12345678910111213141516from sklearn.neighbors import KNeighborsRegressor# 加载数据集 （自定义的 wave 数据集）X, y = mglearn.datasets.make_wave(n_samples=40)# 将 wave 数据集分为训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)# 模型实例化 并将邻居个数设为 3 这里用的是回归模型reg = KNeighborsRegressor(n_neighbors=3) # 利用训练数据和训练目标值来拟合模型 reg.fit(X_train, y_train)# predict 测试集print(\"Test set predictions:\\n&#123;&#125;\".format(reg.predict(X_test)))# 评估模型print(\"Test set R^2: &#123;:.2f&#125;\".format(reg.score(X_test, y_test))) 到这里，我们不难发现，KNN 在 n_neighbors 的值越大时，模型越简单，约适合简单的数据。在测试 KNN 分类形式时，我们做了 n_neighbors 从 1~10 的测试精确度与预测精确度的变化对比，从上面的图中可以很好的看出这一规律。其实也很好理解，毕竟判断所需要的邻居越多，那模型的鲁棒性肯定是越强的，那表现大概率也是趋于稳定的。 虽然 k 近邻算法很容易理解，但由于预测速度慢且不能处理具有很多特征的数据集，所以在实践中往往不会用到。接下来我们来看看实践中常用的模型。 回归线性模型线性模型利用输入特征的线性函数（linear function）进行预测。 对于回归问题，线性模型预测的一般公式是：ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b。有许多种不同的线性回归模型，区别在于模型如何学习到参数 w 和 b，以及如何控制模型复杂度。 线性回归线性回归，或者普通最小二乘法（ordinary least squares，OLS），是回归问题最简单也最经典的线性方法。线性回归寻找参数 w 和 b，使得对训练集的预测值与真实的回归目标值 y 之间的均方误差最小。 均方误差（mean squared error）是预测值与真实值之差的平方和除以样本数。线性回归没有参数，这是一个优点，但也因此无法控制模型的复杂度。 来看看 wave 数据集的线性回归模型预测的图示。 123456789101112131415161718192021222324from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitimport mglearn# 生成 60 个样本数据, 一维特征X, y = mglearn.datasets.make_wave(n_samples=60)# 切分数据集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)# 训练线性回归模型lr = LinearRegression().fit(X_train, y_train)# coef_ 就是斜率 w 即每个特征对应一个权重print(\"lr.coef_: &#123;&#125;\".format(lr.coef_))# intercept_ 是截距 bprint(\"lr.intercept_: &#123;&#125;\".format(lr.intercept_))# 训练集精度print(\"Training set score: &#123;:.2f&#125;\".format(lr.score(X_train, y_train)))# 测试机精度print(\"Test set score: &#123;:.2f&#125;\".format(lr.score(X_test, y_test)))'''Training set score: 0.67Test set score: 0.66''' 你可能注意到了 coef_ 和 intercept_ 结尾处奇怪的下划线。sklearn 总是将从训练数据中得出的值保存在以下划线结尾的属性中。这是为了将其 与用户设置的参数区分开。 从这个模拟的小数据集 wave 的训练中得到的训练精确度和测试精确度非常接近，这表示可能存在欠拟合的问题。对于简单的数据（比如本例子中的一维数据集）来说，过拟合的风险很小，因为模型非常简单。 然而，对于更高维的数据集（即有大量特征的数据集），线性模型将变得更加强大，过拟合的可能性也会变大。接下来让我我们来看一下 LinearRegression 在更复杂的数据集上的表现。 1234567891011121314151617from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) lr = LinearRegression().fit(X_train, y_train)print(\"Training set score: &#123;:.2f&#125;\".format(lr.score(X_train, y_train)))print(\"Test set score: &#123;:.2f&#125;\".format(lr.score(X_test, y_test)))'''Training set score: 0.95Test set score: 0.61''' 在这个维度较高的数据集中，我们在训练集上的预测非常准确，但测试集上的精确度就要低很多。训练集和测试集之间的性能差异是过拟合的明显标志，因此我们应该试图找到一个可以控制复杂度的模型。标准线性回归最常用的替代方法之一就是岭回归（ridge regression），下面来看一下。 岭回归采用线性回归同样的公式，但是模型约束学习得到的 w 系数尽可能的接近于 0，即每个特征对输出的影响尽可能小，从而避免过拟合。这个约束叫做正则化，岭回归用到的是 L2 正则化。 L1 正则化是指权值向量 w 中各个元素的绝对值之和，通常表示为 ||w||1。 L2 正则化是指权值向量 w 中各个元素的平方和然后再求平方根，通常表示为 ||w||2。 B 站视频详细讲解 123456789101112131415161718from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Ridgeimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)# 默认值 alpha=1ridge = Ridge(alpha=1).fit(X_train, y_train)print(\"Training set score: &#123;:.2f&#125;\".format(ridge.score(X_train, y_train)))print(\"Test set score: &#123;:.2f&#125;\".format(ridge.score(X_test, y_test)))```Training set score: 0.89Test set score: 0.75``` Ridge 在训练集上的分数要低于 LinearRegression，但在测试集上的分数更高。 这和我们的预期一致。 岭回归泛化能力优于线性回归，带来的就是训练集精度下降，测试集精度上升。其基本原理是对影响力大的 w 项进行了惩罚。 该模型支持 alpha 参数，该参数默认为 1，调大 alpha 增大约束会进一步下降训练集精度，可能加强泛化能力；相反，调小 alpha 则减少了约束，训练集精度上升，可能降低泛化能力。 12345678910111213141516# 利用上面的数据尝试 alpha=10 的情况ridge10 = Ridge(alpha=10).fit(X_train, y_train)print(\"Training set score: &#123;:.2f&#125;\".format(ridge10.score(X_train, y_train)))print(\"Test set score: &#123;:.2f&#125;\".format(ridge10.score(X_test, y_test)))'''Training set score: 0.79Test set score: 0.64'''# alpha=0.1 的情况ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)print(\"Training set score: &#123;:.2f&#125;\".format(ridge10.score(X_train, y_train)))print(\"Test set score: &#123;:.2f&#125;\".format(ridge10.score(X_test, y_test)))'''Training set score: 0.93Test set score: 0.77''' 岭回归会对模型的进行约束，所以在训练集较少的时候表现要优于线性回归。但需要注意的是，当数据量越来越多的时候，正则化变得不那么重要，并且岭回归和线性回归将具有相同的性能。 Lasso除了 Ridge，还有一种正则化的线性回归是 Lasso，不同之处在于 Lasso 用的方法为 L1 正规化。L1 正则化的结果是，使用 Lasso 时某些系数刚好为 0。这说明某些特征被模型完全忽略。这可以看作是一种自动化的特征选择。某些系数刚好为 0，这样模型更容易解释，也可以呈现模型最重要的特征。 同样的的，我们将 Lasso 应用在波士顿房价数据集上。 1234567891011121314151617181920from sklearn.linear_model import Lassofrom sklearn.model_selection import train_test_splitimport numpy as npimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) lasso = Lasso().fit(X_train, y_train)print(\"Training set score: &#123;:.2f&#125;\".format(lasso.score(X_train, y_train)))print(\"Test set score: &#123;:.2f&#125;\".format(lasso.score(X_test, y_test)))# lasso.coef_ 是 w 斜率向量 数一下有几个特征的系数不为 0print(\"Number of features used: &#123;&#125;\".format(np.sum(lasso.coef_ != 0)))'''Training set score: 0.29Test set score: 0.21Number of features used: 4''' 该模型只用到了 105 个特征中的 4 个，其他的 w 系数都是 0。从以上这个结果来看，该模型预测精度很差，属于欠拟合，需要减少模型的 alpha 参数，即放松正则化 L1。这么做的同时，我们还需要增加 max_iter 的值（运行迭代的最大次数）。 1234567891011121314151617181920from sklearn.linear_model import Lassofrom sklearn.model_selection import train_test_splitimport numpy as npimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # 增大 max_iter 的值 否则模型会警告我们 说应该增大 max_iterlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train) print(\"Training set score: &#123;:.2f&#125;\".format(lasso001.score(X_train, y_train))) print(\"Test set score: &#123;:.2f&#125;\".format(lasso001.score(X_test, y_test))) print(\"Number of features used: &#123;&#125;\".format(np.sum(lasso001.coef_ != 0)))'''Training set score: 0.90Test set score: 0.77Number of features used: 33''' 在实践中，在两个模型中一般首选岭回归。但如果特征很多，你认为只有其中几个是重要的，那么选择 Lasso 可能更好。同样，如果你想要一个容易解释的模型，Lasso 可以给出更容易理解的模型，因为它只选择了一部分输入特征。scikit-learn 还提供了 ElasticNet 类，结合了 Lasso 和 Ridge 的惩罚项。在实践中，这种结合的效果最好，不过代价是要调节两个参数：一个用于 L1 正则化，一个用于 L2 正则化。 分类线性模型线性模型也广泛应用于分类问题。我们首先来看二分类。这时可以利用下面的公式进行预测： ŷ = w[0] * x[0] + w[1] * x[1] + ...+ w[p] * x[p] + b &gt; 0 设置了阈值 0，y 小于 0 则预测为类别 -1，大于 0 则预测为类类别 +1。 不同的线性分类算法的区别包括 2 点： w 和 b 对训练集拟合好坏的度量方式（损失函数） 是否使用正则化以及使用哪种正则化 常见线性分类算法包括： LogisticRegression：Logistic 回归分类器（注意只是名字叫回归，但是分类算法） LinearSVC：线性支持向量机分类器 这两个分类算法默认应用的是 L2 正则化。 SVM=Support Vector Machine 支持向量机 SVC=Support Vector Classification 支持向量机用于分类 SVC=Support Vector Regression 支持向量机用于回归分析 LR &amp; SVC让我们在构造的小数据集上尝试着对比这两种线性分类算法。 1234567891011121314151617181920212223242526from sklearn.linear_model import LogisticRegressionfrom sklearn.svm import LinearSVCimport mglearnimport matplotlib.pyplot as pltX, y = mglearn.datasets.make_forge()# subplots(m,n,figsize)函数 把 n 个图画在 m 行里，每个图片的长宽由 figsize 指定# 返回的第二个值是每个图的绘制位置fig, axes = plt.subplots(1, 2, figsize=(10, 3))# 利用 zip 组合：让 LinearSVC 画在第一个图片中，LogisticRegression 画在第二个图片中# zip() 函数用于将可迭代的对象作为参数# 将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。# 权衡参数默认 C=1for model, ax in zip([LinearSVC(C=1, penalty=\"l2\"), LogisticRegression(C=1, penalty=\"l2\")], axes): # 训练模型 clf = model.fit(X, y) # 画出了这个线性 model 的图像 是一个斜线 mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=.7) # 特征一 特征二 输入的标签 图像位置 是一个离散点图 mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax) ax.set_title(\"&#123;&#125;\".format(clf.__class__.__name__)) ax.set_xlabel(\"Feature 0\") ax.set_ylabel(\"Feature 1\")axes[0].legend() 两个模型得到了相似的决策边界。注意，两个模型中都有两个点的分类是错误的。两个模型都默认使用 L2 正则化，就像 Ridge 对回归所做的那样。 对于 LogisticRegression 和 LinearSVC，决定正则化强度的权衡参数叫作 C。C 值越大，对应的正则化越弱。换句话说，如果参数 C 值较大，那么 LogisticRegression 和 LinearSVC 将尽可能将训练集拟合到最好，而如果 C 值较小，那么模型更强调使系数向量（w）接近于 0。 参数 C 的作用还有另一个有趣之处。较小的 C 值可以让算法尽量适应大多数数据点，而较大的 C 值更强调每个数据点都分类正确的重要性。C 越大越有可能导致模型的过拟合。 与回归的情况类似，用于分类的线性模型在低维空间中看起来可能非常受限，决策边界只能是直线或平面。同样，在高维空间中，用于分类的线性模型变得非常强大，当考虑更多特征时，避免过拟合变得越来越重要。 然我们在乳腺癌数据集上详细分析逻辑回归。 1234567891011121314from sklearn.linear_model import LogisticRegressionfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitcancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=42)logreg = LogisticRegression().fit(X_train, y_train)print(\"Training set score: &#123;:.3f&#125;\".format(logreg.score(X_train, y_train)))print(\"Test set score: &#123;:.3f&#125;\".format(logreg.score(X_test, y_test)))'''Training set score: 0.955Test set score: 0.958''' 在训练集和测试集上的精度（性能）都很好，而且基本一样，这种情况可以尝试加强对训练集拟合看是否能带来进一步提升。将 C 调大以减弱正则化。 1234567logreg = LogisticRegression(C=100).fit(X_train, y_train)print(\"Training set score: &#123;:.3f&#125;\".format(logreg.score(X_train, y_train)))print(\"Test set score: &#123;:.3f&#125;\".format(logreg.score(X_test, y_test)))'''Training set score: 0.974Test set score: 0.965''' 从以上结果可以发现，模型的精度得到了进一步的提升。 如果想要一个可解释性更强的模型，使用 L1 正则化可能更好，因为它约束模型只使用少数几个特征 多分类许多线性分类只适用于二分类问题，不能轻易推广到多类别问题（除了 LR）。将二分类算法推广到多分类算法的一种常见方法是一对其余 one-vs.-rest 方法。 one-vs.rest：对每个类别都学习一个二分类模型，将这个类别与所有其他类别尽量分开，这样就生成了与类别个数一样多的二分类模型。在测试点上运行所有二类分类器来进行预测。在对应类别上分数最高的分类器胜出，将这个类别标签返回作为预测结果。 我们用一个简单的自定义数据集来体验一下 one-vs.-rest 方法 1234567import mglearnfrom sklearn.datasets import make_blobsX, y = make_blobs(random_state=42)mglearn.discrete_scatter(X[:, 0], X[:, 1], y)plt.xlabel(\"Feature 0\")plt.ylabel(\"Feature 1\")plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"]) 这个由高斯分布中采样得出的数据如下图所示。 现在，在这个数据集上训练一个 LinearSVC 分类器。 12345678from sklearn.svm import LinearSVClinear_svm = LinearSVC().fit(X, y)print(\"Coefficient shape: \", linear_svm.coef_.shape)print(\"Intercept shape: \", linear_svm.intercept_.shape)'''Coefficient shape: (3, 2)Intercept shape: (3,)''' coef_ 的形状是 (3, 2)，说明 coef_ 每行包含三个类别之一的系数向量，每列包含某个特征（这个数据集有 2 个特征）对应的系数值。现在 intercept_ 是一维数组，保存每个类别的截距。 接下来让我们将这 3 个二分类器所代表的直线进行可视化。 1234567891011mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)mglearn.discrete_scatter(X[:, 0], X[:, 1], y)# 生成数列line = np.linspace(-15, 15)for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']): plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)plt.ylim(-10, 15)plt.xlim(-10, 8)plt.xlabel(\"Feature 0\")plt.ylabel(\"Feature 1\")plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1', 'Line class 2'], loc=(1.01, 0.3)) 线性模型总结线性模型主要参数就是正则化参数，包括 L1 / L2，以及回归的 alpha 以及分类的 C 值。 alpha 越大或者 C 越小，则正则化越强，可以理解为 w 系数都很小，模型很简单，对训练集精度也会下降。 线性模型无论训练还是预测都很快，但是大数据集需要考虑 solver=&#39;sag&#39; 加速训练。 L1 正则化因为会让很多 w 系数为 0，使模型更简单，更容易理解。 朴素贝叶斯分类器与线性模型相似，但速度更快，泛化能力较弱的一种分类器。 朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。 一共有三种类型的模型。 GaussianNB: 特征可以是任意连续数据。 BernoulliNB：特征必须是 2 分类的数据。 MultinomialNB：特征是计数性质的数据。 GaussianNB 适合高维数据，后两者适合文本领域的稀疏数据。后两个模型支持 alpha 参数，调大该值可以略微提高精度。 决策树让我们来看一个简单的 deep=2 的树的决策边界与相应的树的图示。 随着树的深度的增加，模型肯定会越来越复杂。那我们该如何控制决策树的复杂度呢？ 防止过拟合有两种常见的策略：一种是及早停止树的生长，也叫预剪枝（pre-pruning）；另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫后剪枝（post-pruning）或剪枝（pruning）。预剪枝的限制条件可能包括限制树的最大深度、限制叶结点的最大数目，或者规定一个结点中数据点的最小数目来防止继续划分。 sklearn 只实现了预剪枝，没有实现后剪枝。 让我们在乳腺癌数据集上更详细地看一下预剪枝的效果。 123456789101112131415161718192021from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_breast_cancer# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=42)# 决策树分类tree = DecisionTreeClassifier(random_state=0)# 训练tree.fit(X_train, y_train)# 精度print(\"Accuracy on training set: &#123;:.3f&#125;\".format(tree.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(tree.score(X_test, y_test)))'''Accuracy on training set: 1.000Accuracy on test set: 0.937''' 我们用数据进行了建模，并没有进行预剪枝的处理，这个训练精度为 100% 的结果是可以预测到的，这是因为叶结点都是纯的，树的深度很大，足以完美地记住训练数据的所有标签。测试集精度比之前讲过的线性模型略低，线性模型的精度约为 95%。 如果我们不限制树的深度，其复杂度和深度将可以无限大。故未进行预剪枝的树很容易过拟合，对新数据的泛化能力不高。接下来，我们将树的深度控制在 4 来训练模型。 1234567891011# 决策树分类tree = DecisionTreeClassifier(max_depth=4, random_state=0)# 训练tree.fit(X_train, y_train)# 精度print(\"Accuracy on training set: &#123;:.3f&#125;\".format(tree.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(tree.score(X_test, y_test)))'''Accuracy on training set: 0.988Accuracy on test set: 0.951''' 对于决策树模型，我们可以利用 tree 模块的 export_graphviz 函数来将树可视化。这个函数会生成一个 .dot 格式的文件，这是一种用于保存图形的文本文件格式。 123456789# 导出from sklearn.tree import export_graphvizexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\",\"benign\"], feature_names=cancer.feature_names, impurity=False, filled=True)# 读入import graphvizwith open(\"tree.dot\") as f: dot_graph = f.read()graphviz.Source(dot_graph) 查看整个树是非常费劲的，我们常用特征重要性（feature importance）来为每个特征对树的决策的重要性进行排序。对于每个特征来说，它都是一个介于 0 和 1 之间的数字，其中 0 表示根本没用到，1 表示完美预测目标值。特征重要性的求和始终为 1。 1234567print(\"Feature importances:\\n&#123;&#125;\".format(tree.feature_importances_))'''Feature importances:[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.01 0.048 0. 0. 0.002 0. 0. 0. 0. 0. 0.727 0.046 0. 0. 0.014 0. 0.018 0.122 0.012 0. ] ''' 回归的决策树与分类的决策树是很相似的，但在将基于树的模型用于回归时，都是不能外推（extrapolate）的，也不能在训练数据范围之外进行预测。 这是因为回归是为了预测一个连续值，但是树的叶节点只能保存训练集内出现过的目标值，因此对于训练集外的数据是无法进行目标预测的，只能得到一个训练集内出现过的结果。 决策树集成集成（ensemble）是合并多个机器学习模型来构建更强大模型的方法。现已证明的对大量分类和回归的数据集都是有效的模型有：是随机森林（random forest）和梯度提升决策树（gradient boosted decision tree），二者都是以决策树为基础的。 随机森林前面我们说过，决策树的一个主要缺点在于经常对训练数据过拟合。随机森林是解决这个问题的一种方法。随机森林本质上是许多决策树的集合，其中每棵树都和其他树略有不同。 随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。 随机森林的名字来自于将随机性添加到树的构造过程中，以确保每棵树都各不相同。随机森林中树的随机化方法有两种： 一种是通过选择用于构造树的数据点。 另一种是通过选择每次划分测试的特征。 想要构造一棵树，首先要对数据进行自助采样（bootstrap sample）。也就是说，从 n_samples 个数据点中有放回地（即同一样本可以被多次抽取）重复随机抽取一个样本，共抽取 n_samples 次。这样会创建一个与原数据集大小相同的数据集，但有些数据点会缺失（大约三分之一），有些会重复。 想要利用随机森林进行预测，算法首先对森林中的每棵树进行预测。对于回归问题，我们可以对这些结果取平均值作为最终预测。对于分类问题，则用到了软投票（soft voting）策略。也就是说，每个算法做出软预测，给出每个可能的输出标签的概率。对所有树的预测概率取平均值，然后将概率最大的类别作为预测结果。 让我们来看一个 5 个树的随机森林分类。 123456789101112from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_splitX, y = make_moons(n_samples=100, noise=0.25, random_state=3)# 依据标签 y 按原数据 y 中各类比例 分配给 train 和 test# 使得 train 和 test 中各类数据的比例与原数据集一样X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)forest = RandomForestClassifier(n_estimators=5, random_state=2)forest.fit(X_train, y_train)print(\"Accuracy on training set: &#123;:.3f&#125;\".format(forest.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(forest.score(X_test, y_test))) 作为随机森林的一部分，树被保存在 estimator_ 属性中。 与决策树类似，随机森林也可以给出特征重要性，计算方法是将森林中所有树的特征重要性求和并取平均。一般来说，随机森林给出的特征重要性要比单棵树给出的更为可靠。是随机森林比单棵树更能从总体把握 数据的特征。 梯度提升回归树梯度提升回归树是另一种集成方法，通过合并多个决策树来构建一个更为强大的模型。名字中含有回归，但这个模型既可以用于回归也可以用于分类。与随机森林方法不同，梯度提升采用连续的方式构造树，每棵树都试图纠正前一棵树的错误。默认情况下，梯度提升回归树中没有随机化，而是用到了强预剪枝。梯度提升树通常使用深度很小（1到 5 之间）的树，这样模型占用的内存更少，预测速度也更快。 梯度提升背后的主要思想是合并许多简单的模型（在这个语境中叫作弱学习器），比如深度较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越来越多，可以不断迭代提高性能。 与随机森林相比，它通常对参数设置更为敏感，但如果参数设置正确的话，模型精度更高。除了预剪枝与集成中树的数量之外，梯度提升的另一个重要参数是 learning_rate（学习率），用于控制每棵树纠正前一棵树的错误的强度。较高的学习率意味着每棵树都可以做出较强的修正，这样模型更为复杂。通过增大 n_estimators 来向集成中添加更多树，也可以增加模型复杂度，因为模型有更多机会纠正训练集上的错误。 让我们在乳腺癌数据集上应用 GradientBoostingClassifier 试试看。 12345678910111213141516171819202122232425262728293031323334353637from sklearn.ensemble import GradientBoostingClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitcancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)gbrt = GradientBoostingClassifier(random_state=0)gbrt.fit(X_train, y_train)print(\"Accuracy on training set: &#123;:.3f&#125;\".format(gbrt.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(gbrt.score(X_test, y_test)))'''Accuracy on training set: 1.000Accuracy on test set: 0.958'''# 训练精度太高 可能过拟合# 调小学习率从而降低迭代过程中的修正强度 默认 learning_rate=0.1# gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)gbrt.fit(X_train, y_train)print(\"Accuracy on training set: &#123;:.3f&#125;\".format(gbrt.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(gbrt.score(X_test, y_test)))'''Accuracy on training set: 0.988Accuracy on test set: 0.965'''# 可以看到泛化能力提高了# 我们也可以通过预剪枝来提升泛化能力 默认 max_depth=3gbrt = GradientBoostingClassifier(max_depth=1, random_state=0)gbrt.fit(X_train, y_train)print(\"Accuracy on training set: &#123;:.3f&#125;\".format(gbrt.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(gbrt.score(X_test, y_test)))'''Accuracy on training set: 0.991Accuracy on test set: 0.972''' 由于梯度提升和随机森林两种方法在类似的数据上表现得都很好，因此一种常用的方法就是先尝试随机森林，它的鲁棒性很好。如果随机森林效果很好，但预测时间太长，或者机器学习模型精度小数点后第二位的提高也很重要，那么切换成梯度提升通常会有用。 如果你想要将梯度提升应用在大规模问题上，可以研究一下 xgboost 包及其 Python 接口。这个库在许多数据集上的速度都比 scikit-learn 对梯度提升的实现要快。 核支持向量机线性模型在特征少的情况下非常受限，比如二维特征情况下可能很难利用一条线区分 2 个分类。 为了继续使用之前讲过的线性模型，可以通过基于已有特征进行组合或者变换添加非线性特征（比如对某个特征求平方作为新特征），更高的维度可以解决线性模型的限制，达到不错的效果。 但问题是我们不知道对已有特征如何进行变换与组合对模型是有效的。 总之，能够将已有数据向更高维变换的话，模型就能够表现的更好。 核技巧 多项式核：在一 定阶数内计算原始特征所有可能的多项式（比如 feature1 ** 2 * feature2 ** 5）。 高斯核：对应无限维特征空间。 12345678910111213141516171819202122232425from sklearn.svm import SVC# 加载数据X, y = mglearn.tools.make_handcrafted_dataset()# 训练，用 RBF 核(高斯核)完成高维映射# gamma 控制宽度 决定点于点的靠近指多大距离 越小则核半径越大# C 参数是正则化参数 限制每个点的重要性# 默认情况下，C=1，gamma=1/n_featuressvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y) # 画分类的分界线mglearn.plots.plot_2d_separator(svm, X, eps=.5)# 画样本点mglearn.discrete_scatter(X[:, 0], X[:, 1], y)# 画出支持向量，支持向量的类别标签由 dual_coef_ 的正负号给出sv = svm.support_vectors_sv_labels = svm.dual_coef_.ravel() &gt; 0# 画支持向量点mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)plt.xlabel(\"Feature 0\")plt.ylabel(\"Feature 1\") 由图可以看出 SVM 是非线性的。上图中较大的点代表的是支持向量（位于类别之间边界上的那些点），想要对新样本点进行预测，需要测量它与每个支持向量之间的距离。分类决策是基于它与支持向量之间的距离以及在训练过程中学到的支持向量重要性（保存在 SVC 的 dual_coef_ 属性中）来做出的。 让我们通过图片来理解一下 SVC 的两个参数的作用。 左侧的图决策边界非常平滑，越向右的图决策边界更关注单个点。小的 gamma 值表示决策边界变化很慢，生成的是复杂度较低的模型，而大的 gamma 值则会生成更为复杂的模型。 左上角的图中，决策边界看起来几乎是线性的，误分类的点对边界几乎没有任何影响。再看左下角的图，增大 C 之后这些点对模型的影响变大，使得决策边界发生弯曲来将这些点正确分类。 虽然 SVM 的表现通常都很好，但它对参数的设定和数据的缩放非常敏感。特别地，它要求所有特征有相似的变化范围。所以在使用 SVM 时我们通常要进行数据预处理：每个特征进行缩放，使其大致都位于同一范围。核 SVM 常用的缩放方法就是将所有特征缩放到 0 和 1 之间。 （较复杂，待整理） 神经网络一类被称为神经网络的算法最近以深度学习的名字再度流行。虽然深度学习在许多机器学习应用中都有巨大的潜力，但深度学习算法往往经过精确调整，只适用于特定的使用场景。 这里只讨论一些相对简单的方法，即用于分类和回归的多层感知机（multilayer perceptron，MLP），它可以作为研究更复杂的深度学习方法的起点。MLP 也被称为（普通）前馈神经网络，有时也简称为神经网络。 MLP 可以被视为广义的线性模型。输入特征经过多次线性变换得到输出。每一个隐层包含多个隐单元，每个隐单元是由前一层的特征经过线性计算后，应用一个非线性函数（叫做激活函数）得到的。计算出前一个隐层内的所有隐单元，作为下一个隐层的特征输入，如此往复。 深度学习入门 我们先用一个小的数据集来尝试一下神经网络。 12345678910111213141516171819202122232425262728from sklearn.neural_network import MLPClassifierfrom sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_splitimport mglearnimport matplotlib.pyplot as plt# 数据集X, y = make_moons(n_samples=100, noise=0.25, random_state=3)# 切分X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)# 训练神经网络 默认使用 100 个隐结点# solver 参数指定神经网络如何学习 w 系数# 默认 adam 对数据缩放敏感 lbfgs 对数据缩放不敏感 sgd 有大量参数需要调节mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[100]).fit(X_train, y_train)# 绘制模型分类边界mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)# 画样本点mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)plt.xlabel(\"Feature 0\")plt.ylabel(\"Feature 1\")# 精度print(mlp.score(X_train, y_train))print(mlp.score(X_test, y_test)) MLP 的可以利用 L2 惩罚使权重趋向于零，从而降低拟合与模型的复杂度。MLPClassifier 中调节 L2 惩罚的参数是 alpha（与线性回归模型中的相同），它的默认值很小（弱正则化）。 我们还是用图示的方式来理解这两个参数对 MLP 模型训练的影响。 从图中我们可以看出，参数 n_hidden 和 alpha 越大，模型越简单，曲线越平滑。 注意，神经网络对特征的范围也很敏感，也要求所有输入特征的变化范围相似，最理想的情况是均值为 0、方差为 1。 接下里我们用 cancer 这个比较大的数据集来对模型进行训练。首先用默认参数进行训练。 1234567891011121314151617181920from sklearn.neural_network import MLPClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)# 训练神经网络mlp = MLPClassifier(random_state=0).fit(X_train, y_train)# 精度print(\"Accuracy on training set: &#123;:.2f&#125;\".format(mlp.score(X_train, y_train)))print(\"Accuracy on test set: &#123;:.2f&#125;\".format(mlp.score(X_test, y_test)))'''Accuracy on training set: 0.91Accuracy on test set: 0.91''' 精度不错，但没达到预期（这里看起来也有点欠拟合了的样子），因为神经网络对输入特征要求范围相似，最理想情况是均值为 0，方差为 1，需要进行数据预处理，对数据进行缩放： 12345678910111213141516171819202122232425262728293031from sklearn.neural_network import MLPClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)# 计算训练集中每个特征的平均值 mean_on_train = X_train.mean(axis=0) # 计算训练集中每个特征的标准差 std_on_train = X_train.std(axis=0)# 减去平均值，然后乘以标准差的倒数 如此运算之后 mean=0 std=1X_train_scaled = (X_train - mean_on_train) / std_on_train # 对测试集做相同的变换(使用训练集的平均值和标准差) X_test_scaled = (X_test - mean_on_train) / std_on_train # 训练 这里我们提高一下最大迭代次数 否则模型会有警告mlp = MLPClassifier(random_state=0, max_iter=1000)mlp.fit(X_train_scaled, y_train)# 精度print(\"Accuracy on training set: &#123;:.3f&#125;\".format(mlp.score(X_train_scaled, y_train)))print(\"Accuracy on test set: &#123;:.3f&#125;\".format(mlp.score(X_test_scaled, y_test)))'''Accuracy on training set: 0.995Accuracy on test set: 0.965''' 分类器的不确定度估计分类器能够给出预测的不确定度估计。一般来说，你感兴趣的不仅是分类器会预测一个测试点属于哪个类别，还包括它对这个预测的置信程度。 举个很经典的实际案例：不同类型的错误会在现实应用中导致非常不同的结果。想象一个用于测试癌症的医疗应用。假阳性预测可能只会让患者接受额外的测试，但假阴性预测却可能导致重病没有得到治疗。 sklearn 中有两个函数可用于获取分类器的不确定度估计：decision_function 和 predict_proba。大多数分类器（但不是全部）都至少有其中一个函数，很多分类器两个都有。 接下来我们来看这两个函数对一个模拟的二维数据的作用。 小结与展望先从线性模型、朴素贝叶斯、最邻近等简单模型开始，对数据有了解后再使用随机森林、梯度提升决策树、SVM、神经网络。 K 邻近（KNN）：适用于小型数据集，是很好的基准模型，很容易解释。 线性模型：非常可靠的首选算法，适用于非常大的数据集，也适用于高维数据。 朴素贝叶斯：只适用于分类问题。比线性模型速度还快，适用于非常大的数据集和高维数据。精度通常要低于线性模型。 决策树：速度很快，不需要数据缩放，可以可视化，很容易解释。 随机森林：几乎总是比单棵决策树的表现要好，鲁棒性很好，非常强大。不需要数据缩放。不适用于高维稀疏数据。 梯度提升决策树：精度通常比随机森林略高。与随机森林相比，训练速度更慢，但预测速度更快，需要的内存也更少。比随机森林需要更多的参数调节。 支持向量机：对于特征含义相似的中等大小的数据集很强大。需要数据缩放，对参数敏感。 神经网络：可以构建非常复杂的模型，特别是对于大型数据集而言。对数据缩放敏感，对参数选取敏感。大型网络需要很长的训练时间。 无监督学习与预处理训练算法时只有输入数据，没有已知输出。因为训练数据没有分类标签，所以很难用精度去评估模型有效。因此，无监督算法通常用于探索数据的规律，而不是自动化系统。 无监督学习类型无监督变换（unsupervised transformation）：创建数据新的表示的算法，与数据的原始表示相比，新的表示可能更容易被人或其他机器学习算法所理解。 常用于降维（dimensionality reduction），它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新方法，用较少的特征就可以概括其重要特性。降维的一个常见应用是为了可视化将数据降为二维。 无监督变换的另一个应用是找到构成数据的各个组成部分。这方面的一个例子就是对文本文档集合进行主题提取。这里的任务是找到每个文档中讨论的未知主题，并学习每个文档中出现了哪些主题。这可以用于追踪社交媒体上的话题讨论，比如选举、枪支管制或 流行歌手等话题。 聚类算法（clustering algorithm）：将数据划分成不同的组，每组包含相似的物项。 思考向社交媒体网站上传照片的例子。为了方便你整理照片，网站可能想要将同一个人的照片分在一组。但网站并不知道每张照片是谁，也不知道你的照片集中出现了多少个 人。明智的做法是提取所有的人脸，并将看起来相似的人脸分在一组。但愿这些人脸对应同一个人，这样图片的分组也就完成了。 无监督学习的挑战无监督学习的一个主要挑战就是评估算法是否学到了有用的东西。通常来说，评估无监督算法结果的唯一方法就是人工检查。 因此，如果数据科学家想要更好地理解数据，那么无监督算法通常可用于探索性的目的，而不是作为大型自动化系统的一部分。无监督算法的另一个常见应用是作为监督算法的预处理步骤。学习数据的一种新表示，有时可以提高监督算法的精度，或者可以减少内存占用和时间开销。 预处理与缩放让我们先来看看对数据集缩放核预处理的各种方法的处理结果。 StandardScaler 确保每个特征的平均值为 0，方差为 1，使所有特征位于同一量级。 RobustScaler 使用的是中位数和四分位数 ，而不是平均值和方差。这样 RobustScaler 会忽略与其他点有很大不同的数据点（异常值，outlier）（比如测量误差）。 MinMaxScaler 移动数据，使所有特征都刚好位于 0 到 1 之间。对于二维数据集来说，所有的数据都包含在 x 轴 0 到 1 与 y 轴 0 到 1 组成的矩形中。 Normalizer 用到一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于 1。换句话说，它将一个数据点投射到半径为 1 的圆上（对于更高维度的情况，是球面）。这意味着每个数据点的缩放比例都不相同（乘以其长度的倒数）。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。 对训练数据和测试数据进行相同的缩放 注意！！！所有的缩放器总是对训练集核测试集应用完全相同的变换。也就是说，transform 方法总是减去训练集的最小值，然后除以训练集的范围，而这两个值可能与测试集的最小值和范围并不相同。为了让监督模型能够在测试集上运行，对训练集和测试集应用完全相同的变换是很重要的。 降维、特征提取与流形学习利用无监督学习进行数据变换最简单也最常用的一种算法就是主成分分析（principal component analysis，PCA）。 主成分分析主成分分析（principal component analysis，PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。 我们对 cancer 数据集利用 PCA 从 30 个维降到 2 个维度。 1234567891011121314151617181920212223242526from sklearn.decomposition import PCAfrom sklearn.datasets import load_breast_cancerfrom sklearn.preprocessing import StandardScaler# 数据集cancer = load_breast_cancer()# 标准化scaler = StandardScaler()scaler.fit(cancer.data)X_scaled = scaler.transform(cancer.data)# 降维到 2 的PCApca = PCA(n_components=2)# 训练 PCApca.fit(X_scaled)# 进行特征提取变换X_pca = pca.transform(X_scaled)# 降维前的特征个数print(\"Original shape: &#123;&#125;\".format(str(X_scaled.shape))) # 降维后的特征个数print(\"Reduced shape: &#123;&#125;\".format(str(X_pca.shape)))'''Original shape: (569, 30)Reduced shape: (569, 2)''' 因为 PCA 后只有 2 个维度，所以可以绘图做可视化分析。 重要的是要注意，PCA 是一种无监督方法，在寻找旋转方向时没有用到任何类别信息。它只是观察数据中的相关性。 PCA 的一个缺点在于，通常不容易对图中的两个轴做出解释。主成分对应于原始数据中的方向，所以它们是原始特征的组合，但这些组合往往非常复杂。 来看一个人脸识别的例子。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn.datasets import fetch_lfw_peopleimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_split# 加载数据 # 一共有 3023 张图像，每张大小为 87 像素 ×65 像素，分别属于 62 个不同的人people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)# 3023 张人脸照片作为样本输入print(\"people.images.shape: &#123;&#125;\".format(people.images.shape))# 3023 张图片对应的人名print(\"Number of classes: &#123;&#125;\".format(len(people.target_names)))'''people.images.shape: (3023, 87, 65)Number of classes: 62'''# 3023 列的 false 向量mask = np.zeros(people.target.shape, dtype=np.bool)# 遍历所有的分类(也就是人名，target 取值是 0~61)# 每个分类保留 50 个样本 防止某个 target 的数据集过多造成倾斜for target in np.unique(people.target): mask[np.where(people.target == target)[0][:50]] = TrueX_people = people.data[mask] # data 是已经把每张图片 n*m 的 2 维压成 1 维的数据格式y_people = people.target[mask]# 现在留下的样本，每个人不会超过50张# 特征缩放到 0~1 之间, 因为是RGB颜色X_people = X_people / 255# 切分数据# print(X_people, y_people)X_train, X_test, y_train, y_test = train_test_split( X_people, y_people, stratify=y_people, random_state=0)# KNN分类, 最近邻分类(只考虑最近的节点)knn = KNeighborsClassifier(n_neighbors=1)knn.fit(X_train, y_train)print(\"Test set score of 1-nn: &#123;:.2f&#125;\".format(knn.score(X_test, y_test)))'''Test set score of 1-nn: 0.27''' 这个结果对于一个 62 分类问题来说也不算太大，但还是由改进的空间。 这里就可以用到 PCA。想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图像时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大不同，使用这种原始表示很难获取到面部特征。 例如，如果使用像素距离，那么将人脸向右移动一个像素将会发生巨大的变化，得到一个完全不同的表示。 我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启用 PCA 的白化（whitening）选项，它将主成分缩放到相同的尺度。 PCA 的白化（whitening）选项，它将主成分缩放到相同的尺度。变换后的结果与使用 StandardScaler 相同。 我们可以考虑利用 PCA 提取主成分，作为 100 个新特征输入到模型。对数据做了特征提取后再用 KNN 进行建模。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from sklearn.datasets import fetch_lfw_peopleimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.decomposition import PCA# 加载数据people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)# 3023 张人脸照片作为样本输入print(\"people.images.shape: &#123;&#125;\".format(people.images.shape))# 3023 张图片对应的人名print(\"Number of classes: &#123;&#125;\".format(len(people.target_names)))# 3023 列的 false 向量mask = np.zeros(people.target.shape, dtype=np.bool)# 遍历所有的分类(也就是人名，target 取值是 0~61)# 每个分类保留 50 个样本for target in np.unique(people.target): mask[np.where(people.target == target)[0][:50]] = TrueX_people = people.data[mask] # data 是已经把每张图片 n*m 的 2 维压成 1 维的数据格式y_people = people.target[mask]# 现在留下的样本，每个人不会超过50张# 特征缩放到 0~1 之间, 因为是 RGB 颜色X_people = X_people / 255# 切分数据# print(X_people, y_people)X_train, X_test, y_train, y_test = train_test_split( X_people, y_people, stratify=y_people, random_state=0)# PCA 提取 100 个主成分作为新的特征, 基于训练集 fit, 应用到训练集和测试集pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)print(\"X_train_pca.shape: &#123;&#125;\".format(X_train_pca.shape))# KNN 分类, 最近邻分类(只考虑最近的节点)knn = KNeighborsClassifier(n_neighbors=1)knn.fit(X_train_pca, y_train)# 精度print(\"Test set accuracy: &#123;:.2f&#125;\".format(knn.score(X_test_pca, y_test)))'''Test set accuracy: 0.36''' 在对数据进行 PCA 处理后，KNN 模型的精度确实得到了提升。 非负矩阵分解非负矩阵分解（non-negative matrix factorization，NMF）是另一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于 PCA，也可以用于降维。与 PCA 相同，我们试图将每个数据点写成一些分量的加权求和。 但在 PCA 中，我们想要的是正交分量，并且能够解释尽可能多的数据方差；而在 NMF 中，我们希望分量和系数均为非负，也就是说，我们希望分量和系数都大于或等于 0。因此，这种方法只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。 （常用的降维方法为 PCA，NMF 待整理） 用 t-SNE 进行流形学习虽然 PCA 通常是用于变换数据的首选方法，使你能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。有一类用于可视化的算法叫作流形学习算法（manifold learning algorithm），它允许进行更复杂的映射，通常也可以给出更好的可视化。其中特别有用的一个就是 t-SNE 算法。 新特征能够根据原始数据中数据点之间的远近程度将不同类比明确分开。在可视化用途比 PCA 更有效，但只能用于做可视化，无法像 PCA 一样应用到测试集上。 聚类聚类（clustering）是将数据集划分成组的任务，这些组叫做簇。聚类算法给每个数据点分配一个数字，表示数据点属于哪个簇。 k 均值聚类k 均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心（cluster center）。算法交替执行以下两个步骤：将每个数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化（收敛），那么算法结束。 123456789101112131415from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeans# 生成模拟的二维数据X, y = make_blobs(random_state=1)# 构建聚类模型, 指定 3 个簇中心kmeans = KMeans(n_clusters=3) kmeans.fit(X)# 打印每个数据所属的簇标签, 因为 n_clusters=3 所以就是 0~2print(kmeans.labels_)# 也支持 predict 方法来计算一个新数据点属于哪个簇标签print(kmeans.predict(X)) 簇标签没有先验意义，我们并不知道每一个簇代表什么，只能人为观察。 kmeans 类似于 PCA 可以进行特征变换，首先 kmeans 进行 fit 找到所有簇中心后，进而对训练集 / 测试集特征进行 transform，从而将原始数据点变换为到各个簇中心的距离，作为新的特征。 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeansfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_moonsfrom sklearn.ensemble import GradientBoostingClassifier# 数据集X, y = make_moons(n_samples=200, noise=0.05, random_state=0)# 切分X_train, X_test, y_train, y_test = train_test_split(X, y)# 分成10簇kmeans = KMeans(n_clusters=10, random_state=0)kmeans.fit(X_train)# 转换原有特征为到簇中心的距离train_distance_features = kmeans.transform(X_train)test_distance_features = kmeans.transform(X_test)# 跑模型gbdt = GradientBoostingClassifier()gbdt.fit(train_distance_features, y_train)# 精度print(gbdt.score(test_distance_features, y_test)) kmeans 最初的簇中心是随机产生的，算法输出依赖于随机种子，sklearn 会默认跑 10 次选最好的 1 次。另外，kmeans 对簇形状有假设，人工确定簇个数是很难琢磨的。 凝聚聚类算法首先声明每个点是自己的簇，然后不断合并相似的簇，直到簇数量达到目标。因为算法是不断合并簇的逻辑，所以它对新数据无法做出所属簇的预测。 scikit-learn 中实现了以下三种选项。 ward ：默认选项。ward 挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。 average：average 链接将簇中所有点之间平均距离最小的两个簇合并。 complete： complete 链接（也称为最大链接）将簇中点之间最大距离最小的两个簇合并。 ward 适用于大多数数据集。如果簇中的成员个数非常不同（比如其中一个比其他所有都大得多），那么 average 或 complete 可能效果更好。 1234567891011121314151617from sklearn.datasets import make_blobsfrom sklearn.cluster import AgglomerativeClusteringfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_moonsimport mglearn# 数据集X, y = make_blobs(random_state=1)# 凝聚聚类为 3 个簇agg = AgglomerativeClustering(n_clusters=3)assignment = agg.fit_predict(X)# 输出数据点分布以及所属簇mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)plt.xlabel(\"Feature 0\")plt.ylabel(\"Feature 1\") 凝聚聚类生成了所谓的层次聚类（hierarchical clustering）。聚类过程迭代进行，每个点都从一个单点簇变为属于最终的某个簇。每个中间步骤都提供了数据的一种聚类（簇的个数也不相同）。 可以将层次聚类可视化为树状图进行分析。但 sklearn 目前没有绘制树状图的功能，这里我们利用 Scipy 库。 DBSCAN 聚类DBSCAN（density-based spatial clustering of applications with noise，即具有噪声的基于密度的空间聚类应用）的主要优点是它不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点，DBSCAN 也不允许对新的测试数据进行预测。 DBSCAN 有两个参数：min_samples 和 eps。如果在距一个给定数据点 eps 的距离内至少有 minsamples 个数据点，那么这个数据点就是核心样本。DBSCAN 将彼此距离小于 eps 的核心样本放到同一个簇中。 小结无监督学习可以用于探索性的数据分析与预处理。 预处理以及分解方法在数据准备中具有重要作用。 通常来说，很难量化无监督算法的有用性，但这不应该妨碍你使用它们来深入理解数据。 数据表示与特征工程到现在，我们已经知道了模型的形成是依赖数据的特征的，并且不同的算法对特征的敏感度不同。特征可以分为两大类型：分类特征与离散特征。 对于某个特定应用来说，如何找到最佳数据表示，这个问题被称为特征工程（feature engineering），它是数据科学家和机器学习从业者在尝试解决现实世界问题时的主要任务之一。用正确的方式表示数据，对监督模型性能的影响比所选择的精确参数还要大。 one-hot 编码（虚拟变量）如果特征不是连续值，而是一些分类特征，来自一系列固定的可能取值（非数字），那么直接用于类似 Logistic 回归分类模型是没有意义。显然，在应用 Logistic 回归时，我们需要换一种方式来表示数据。下一节将会说明我们如何解决这一问题。 将 1 个特征的多种取值，改为多个特征的 0 / 1 取值，其中有一个特征为 1，其他为 0。 12345678910111213141516from sklearn.preprocessing import OneHotEncoder# fit 训练 one hot, 返回非稀疏矩阵# 当 transform 时遇到没见过的特征值则对应 one-hot 编码全部为 0enc = OneHotEncoder(sparse=False, handle_unknown='ignore')# 每一列代表一种特征的可能性# 第一列：[0, 1, 0, 1] 只有 0 和 1 两种情况 one-hot 占两列# 第二列：[0, 1, 2, 0] 有 0 1 2 三种情况 one-hot 占三列# 第三列：[3, 0, 1, 2] 有 0 1 2 3 四种情况 one-hot 占四列X = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]enc.fit(X) # one-hot 编码新数据X_one_hot = enc.transform([[1,4,2]]) print(X_one_hot)# OUT：[[0. 1. 0. 0. 0. 0. 0. 1. 0.]] 也可指定对哪些 one-hot，而对其他连续特征可以不做处理。 12345678910from sklearn.preprocessing import OneHotEncoder# 指定了只有前 2 个特征需要离散化enc = OneHotEncoder(categorical_features=[0,1], sparse=False, handle_unknown='ignore')X = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]enc.fit(X) # one-hot 编码新数据X_one_hot = enc.transform([[1,4,2]]) print(X_one_hot) 我们也可以利用 ColumnTransformer 来作为统一的特征处理方法，将字符串编码为整形。 1234567891011121314151617181920from sklearn.preprocessing import OrdinalEncoderfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.compose import ColumnTransformerimport numpy as npX = [['male', 0, 3], ['male', 1, 0], ['female', 2, 1], ['female', 0, 2]]# 字符串编码为整形sex_enc = OrdinalEncoder(dtype = np.int)# 独热编码one_hot_enc = OneHotEncoder(sparse=False, handle_unknown='ignore', dtype=np.int)# 对第 0 列的字符串做整形转换, 然后对所有列做 one-hotcol_transformer = ColumnTransformer(transformers = [('sex_enc', sex_enc, [0]), ('one_hot_enc', one_hot_enc, list(range(0,3)))])# 训练编码col_transformer.fit(X)X_trans = col_transformer.transform(X)print(X_trans) 分箱、离散化、线性模型与树对于只有 1 个连续特征的线性模型，它的表现就是 y=w*x 的线，效果不好。这时候可以考虑将特征取值划分范围，这就是分箱，每个箱子的数值区间不同，提高了线性模型的表现力。 这样就把 1 个连续特征变成了离线特征，也就是在哪个区间里，可以继续通过 one-hot 编码成多个 0/1 特征。 分箱对线性模型有提升效果，对树模型效果不会很好可能还会下降。 交互特征与多项式特征把原始特征之间进行组合和扩充，对线性模型有提升效果，比如：添加特征的平方或立方，或者把两两特征相乘。添加交互 / 多项式特征之后的线性模型，与没有交互特征的树模型 / 复杂模型的性能就比较相近了。 单变量非线性变换对于简单模型（线性、朴素贝叶斯）来说，如果数据集的数据分布存在大量的小值以及个别非常大的值，会导致线性模型很难处理。这种情况出现在一些计数性质的特征上，比如点赞次数。 此时对该特征应用 log(x+1) 或者 exp （对数 / 指数）来调节特征值得比例，可以改进线性模、SVM、神经网络的效果。 123# 对跨度较大的特征做 logX_train_log = np.log(X_train + 1)X_test_log = np.log(X_test + 1) 对线性模型提升最明显，对树模型意义不大，对 SVM / KNN / 神经网络有可能受益。 自动化特征选择前面提到的方法都是增加特征（线性 👉 非线性），现在是通过分析数据来减少特征，得到一个泛化更好，更简单的模型，也就是特征选择。 单变量统计每次考虑一个特征，观察特征与目标值之间是否存在统计显著性，但是没法综合考虑多个特征。算法可以指定保留一定数量的重要特征。 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.datasets import load_breast_cancerfrom sklearn.feature_selection import SelectPercentilefrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegression# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0, test_size=.5)print(X_train.shape)# 自动选择 50% 的特征留下来select = SelectPercentile(percentile=50)select.fit(X_train, y_train)# 训练集提取特征X_train_selected = select.transform(X_train)print(X_train_selected.shape)# 测试集提取特征X_test_selected = select.transform(X_test)# 原始特征模型，看精度lr = LogisticRegression()lr.fit(X_train, y_train)print(\"Score with all features: &#123;:.3f&#125;\".format(lr.score(X_test, y_test))) # 特征选择的模型, 看精度lr.fit(X_train_selected, y_train)print(\"Score with only selected features: &#123;:.3f&#125;\".format(lr.score(X_test_selected, y_test)))'''out：(284, 30)(284, 15)Score with all features: 0.954Score with only selected features: 0.954特征删了一半 但对模型的精度丝毫没有产生影响''' 基于模型的特征选择线性模型、决策树模型在训练的过程中自然的完成了对特征重要程度的学习。所以可以基于这些模型进行特征选择，再将选择后的特征作为另外一个模型的输入。 12345678910111213141516171819202122232425262728293031323334from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import RandomForestClassifier# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0, test_size=.5)# 从随机森林模型的训练的结果中进行特征选择, 取中位数偏上重要的特征，也就是保留一半最重要的特征# threshold：滤波器select = SelectFromModel( RandomForestClassifier(n_estimators=100, random_state=42), threshold=\"median\")# 特征选择训练select.fit(X_train, y_train)# 留下选择后的特征X_train_l1 = select.transform(X_train)print(\"X_train.shape: &#123;&#125;\".format(X_train.shape))print(\"X_train_l1.shape: &#123;&#125;\".format(X_train_l1.shape))# 选择出来的特征完成训练lr = LogisticRegression()lr.fit(X_train_l1, y_train)# 测试集特征选择X_test_l1 = select.transform(X_test)print(\"Test score: &#123;:.3f&#125;\".format(lr.score(X_test_l1, y_test))) 模型综合度量了所有特征的重要性，所以比单变量统计强大的多。 迭代特征选择（RFE，Recursive Feature Elimination）利用模型进行多轮特征选择，每轮筛掉1个最不重要的特征。 下面用随机森林做特征选择，选择出来的特征用线性 LR 做训练，发现线性模型精度很好。 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.feature_selection import RFEfrom sklearn.ensemble import RandomForestClassifier# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0, test_size=.5)# 基于迭代的特征选择, 保留15个特征select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=15)# 训练特征选择select.fit(X_train, y_train)# 训练集特征选择X_train_rfe = select.transform(X_train)# 测试集特征选择X_test_rfe = select.transform(X_test)# 选择后的特征训练模型lr = LogisticRegression()lr.fit(X_train_rfe, y_train)# 测试集精度print(\"Test score: &#123;:.3f&#125;\".format(lr.score(X_test_rfe, y_test) )) 模型评估与改进到目前为止，为了评估我们的监督模型，我们使用 train_test_split 函数将数据集划分为 、训练集和测试集，在训练集上调用 fit 方法来构建模型，并且在测试集上用 score 方法来评估这个模型，对于分类问题而言，就是计算正确分类的样本所占的比例。 请记住，之所以将数据划分为训练集和测试集，是因为我们想要度量模型对前所未见的新数据的泛化性能。我们对模型在训练集上的拟合效果不感兴趣，而是想知道模型对于训练过程中没有见过的数据的预测能力。 交叉验证单次划分数据集并不稳定和全面，因此我们需要对数据集进行多次划分，训练多个模型进行综合评估，这叫交叉验证。 K 折交叉这是最常见的交叉验证方式，将数据均匀划分成 K 份，每次用 1 份做测试集，剩余做训练集。 12345678910111213141516171819from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import cross_val_score# 数据集iris = load_iris()# 模型logreg = LogisticRegression()# K折交叉验证scores = cross_val_score(logreg, iris.data, iris.target, cv=3)print(\"Cross-validation scores: &#123;&#125;\".format(scores))print(\"Average cross-validation score: &#123;:.2f&#125;\".format(scores.mean()))'''Cross-validation scores: [0.96078431 0.92156863 0.95833333]Average cross-validation score: 0.95''' 分层 K 折交叉K 折交叉划分数据的方式是从头开始均分成 K 份，如果样本数据的分类分布不均匀，那么就会导致 K 折交叉策略失效。 在分层交叉验证中，我们划分数据，使每个折中类别之间的比例与整个数据集中的比例相同。sklearn 会根据模型是回归还是分类决定使用标准 K 折还是分层 K 折，不需我们关心，只需要了解。 留一法交叉验证可以将留一法交叉验证看作是每折只包含单个样本的 k 折交叉验证。对于每次划分，你选择单个数据点作为测试集。这种方法可能非常耗时，特别是对于大型数据集来说，但在小型数据集上有时可以给出更好的估计结果。 打乱划分交叉验证在打乱划分交叉验证中，每次划分为训练集取样 train_size 个点，为测试集取样 test_size 个 （不相交的）点。将这一划分方法重复 n_iter 次。 分组交叉验证利用分组保证测试集与训练集的样本不同。（为了准确评估模型对新的人脸的泛化能力，我们必须确保训练集和测试集中包含不同人的图像。） 网格搜索利用网格搜索，实现模型的自动化调参，找到最佳泛化性能的参数。 在尝试调参之前， 重要的是要理解参数的含义。找到一个模型的重要参数（提供最佳泛化性能的参数）的取值是一项棘手的任务，但对于几乎所有模型和数据集来说都是必要的。由于这项任务如此常见，所以 sklearn 中有一些标准方法可以帮你完成。最常用的方法就是网格搜索（grid search），它主要是指尝试我们关心的参数的所有可能组合。 带交叉验证的网格搜索1234567891011121314151617181920212223242526272829303132from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, train_test_splitfrom sklearn.svm import SVC# 数据集iris = load_iris()# 切分数据X_train, X_test, y_train, y_test = train_test_split( iris.data, iris.target, random_state=0)# 2 种网格param_grid = [ # （高斯）径向基函数核（英語：Radial basis function kernel） # gamma 越大，支持向量越少，gamma 值越小，支持向量越多。支持向量的个数影响训练与预测的速度 # 第1个网格 &#123;'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]&#125;, # 第2个网格 &#123;'kernel': ['linear'],'C': [0.001, 0.01, 0.1, 1, 10, 100]&#125;]# 在 2 个网格中, 找到 SVC 模型的最佳参数, 这里 cv=5 表示每一种参数组合进行 5 折交叉验证计算得分grid_search = GridSearchCV(SVC(), param_grid, cv=5)# fit 找到最佳泛化的参数grid_search.fit(X_train, y_train)# 查看精度print(\"泛化精度:\", grid_search.score(X_test, y_test))# 打印最佳参数print(\"Best parameters: &#123;&#125;\".format(grid_search.best_params_)) 交叉验证与网格搜索的嵌套可以采用先交叉划分数据集，再进行 K 折网格搜索，这就是嵌套的意思。 1234567891011121314151617from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_splitfrom sklearn.svm import SVC# 数据集iris = load_iris()param_grid = [ &#123;'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]&#125;, &#123;'kernel': ['linear'],'C': [0.001, 0.01, 0.1, 1, 10, 100]&#125;]grid_search = GridSearchCV(SVC(), param_grid, cv=5)# 外层 K 折scores = cross_val_score(grid_search, iris.data, iris.target, cv=5)# 打印精度print(scores) 评估指标与评分首先，需要牢记的一点，精度不是唯一目标！！！我们需要考虑商业指标，对商业的影响。 二分类指标二分类一共有 2 种类型，一种称为正类（positive），一种称为反类（negative）。 根据样本分类与模型预测分类，可以产生 4 种组合： TP：预测是正类，样本是正类 FP：预测是正类，样本是反类 TN：预测是反类，样本是反类 FN：预测是反类，样本是正类 123456789101112131415161718192021222324252627282930313233343536373839404142434445from sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix,f1_scorefrom sklearn.metrics import classification_report# 数据集digits = load_digits()# 转换成 2 分类, 即目标数字是否等于 9y = digits.target == 9# 切分数据集X_train, X_test, y_train, y_test = train_test_split( digits.data, y, random_state=0)# 模型lr = LogisticRegression()# 训练lr.fit(X_train, y_train)# 预测y_pred = lr.predict(X_test)# 混淆矩阵print(confusion_matrix(y_test, y_pred))# 打印 f-scoreprint(f1_score(y_test, y_pred))# 打印所有指标print(classification_report(y_test, y_pred))'''Out:[[399 4] [ 7 40]]0.8791208791208791 precision recall f1-score support False 0.98 0.99 0.99 403 True 0.91 0.85 0.88 47 micro avg 0.98 0.98 0.98 450 macro avg 0.95 0.92 0.93 450weighted avg 0.98 0.98 0.98 450''' 混淆矩阵的分布： 预测为反类 预测为正类 实际为反类 TN FP 实际为正类 FN TP 正确率 Accuracy：Accuracy = (TN+TP) / (TN+TP+FN+FP) 精确率 Precision：Precision = TP / (TP + FP) 精确率的商业目标就是限制假正例的数量，可能因为假正例会带来很严重的影响。 召回率 Recall：Recall = TP / (TP + FN) 精确率和召回率是矛盾的，如果模型预测所有都是正类，那么召回率就是100%；此时，就会出现很多假正类，精确率就很差。因此需要综合 2 个指标进行折衷，就是 f-score 或 f-measure。 F = 2 * precision * recall / (precision + recall) 阈值：阈值（默认为 50%）越高，意味着模型需要更加确信才能做出正类的判断。 准确率-召回率曲线曲线越靠近右上角，则分类器越好。右上角的点表示对于同一个阈值，准确率和召回率都很高。曲线从左上角开始，这里对应于非常低的阈值，将所有样本都划为正类。提高阈值可以让曲线向准确率更高的方向移动，但同时召回率降低。继续增大阈值，大多数被划为正类的点都是真正例，此时准确率很高，但召回率更低。随着准确率的升高，模型越能够保持较高的召回率，则模型越好。 ROC 与 AUC受试者工作特征曲线（receiver operating characteristics curve），简称为 ROC 曲线（ROC curve）。与准确率 - 召回率曲线类似，ROC 曲线考虑了给定分类器的所有可能的阈值，但它显示的是假正例率（false positive rate，FPR）和真正例率（true positive rate，TPR），而不是报告准确率和召回率。 与准确率 - 召回率曲线一样，我们通常希望使用一个数字来总结 ROC 曲线，即曲线下的面积［通常被称为 AUC（area under the curve），这里的曲线指的就是 ROC 曲线］。 多分类指标用分类报告来观察各个分类的指标就很不错。 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_report# 数据集digits = load_digits()# 切分X_train, X_test, y_train, y_test = train_test_split( digits.data, digits.target, random_state=0)# 训练lr = LogisticRegression().fit(X_train, y_train)# 预测pred = lr.predict(X_test)# 精度print(\"Accuracy: &#123;:.3f&#125;\".format(lr.score(X_test, y_test)))# 精确度、召回率、f1 指标print(classification_report(pred, y_test))'''Out:Accuracy: 0.953 precision recall f1-score support 0 1.00 1.00 1.00 37 1 0.91 0.89 0.90 44 2 0.93 0.95 0.94 43 3 0.96 0.90 0.92 48 4 1.00 0.97 0.99 39 5 0.98 0.98 0.98 48 6 1.00 0.96 0.98 54 7 0.94 1.00 0.97 45 8 0.90 0.93 0.91 46 9 0.94 0.96 0.95 46 micro avg 0.95 0.95 0.95 450 macro avg 0.95 0.95 0.95 450weighted avg 0.95 0.95 0.95 450''' 宏（macro）平均：计算未加权的按类别 f- 分数。它对所有类别给出相同的权重，无论类别中的样本量大小。 加权（weighted）平均：以每个类别的支持作为权重来计算按类别 f- 分数的平均值。分类报告中给出的就是这个值。 微（micro）平均：计算所有类别中假正例、假反例和真正例的总数，然后利用这些计数来计算准确率、召回率和 f- 分数。 如果你对每个样本等同看待，那么推荐使用微平均 f1- 分数；如果你对每个类别等同看待，那么推荐使用宏平均 f1- 分数。 回归指标对于回归问题来说，使用 score 方法评估即可，因为他没有分类的正反问题。score底层使用的是 R^2，它是评估回归模型的很好的指标。 模型指标选择网格搜索评估和 K 折交叉验证最佳模型参数默认是基于精度评判的，我们可以指定基于其他指标（精确度、召回率、f1）。 12345678910111213141516171819202122232425262728293031323334353637383940from sklearn.datasets import load_digitsfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_splitfrom sklearn.svm import SVCfrom sklearn.metrics import classification_report# 数据集digits = load_digits()# 切分成 2 分类问题, 数字是否等于 9X_train, X_test, y_train, y_test = train_test_split( digits.data, digits.target == 9, random_state=0)# 2 种网格param_grid = &#123;'gamma': [0.001, 0.01, 0.1, 1, 10, 100]&#125;# 在 2 个网格中, 找到 SVC 模型的最佳参数, 每一组参数进行 3 折评估, 使用 f1-score 作为评估依据grid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring='f1')# 搜索最佳参数grid_search.fit(X_train, y_train)# 打印最佳参数print(grid_search.best_params_)# 打印最佳参数的 f1-scoreprint(grid_search.best_score_)# 打印在测试集上的各种指标print(classification_report(grid_search.predict(X_test), y_test))'''Out:&#123;'gamma': 0.001&#125;0.9771729298313027 precision recall f1-score support False 1.00 0.99 1.00 405 True 0.94 0.98 0.96 45 micro avg 0.99 0.99 0.99 450 macro avg 0.97 0.99 0.98 450weighted avg 0.99 0.99 0.99 450''' 算法链和管道大多数机器学习应用不仅需要应用单个算法，而且还需要将许多不同的处理步骤和机器学习模型链接在一起。我们一般使用 Pipeline 类来简化构建变换和模型链，Pipeline 类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型（比如分类器）链接在一起。 构建管道12345from sklearn.pipeline import Pipelinepipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])pipe.fit(X_train, y_train)print(\"Test score: &#123;:.2f&#125;\".format(pipe.score(X_test, y_test)))# Test score: 0.95 利用管道，我们减少了预处理 + 分类过程 所需要的代码量。但是，使用管道的主要优点在于，现在我们可以在 cross_val_score 或 GridSearchCV 中使用这个估计器。 网格搜索中使用管道12345678910111213141516171819202122from sklearn.svm import SVCfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipeline# 加载并划分数据cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)# 先缩放再跑模型的pipelinepipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])# 5折网格搜索param_grid = &#123;'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]&#125;grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)print(\"Best cross-validation accuracy: &#123;:.2f&#125;\".format(grid.best_score_))print(\"Test set score: &#123;:.2f&#125;\".format(grid.score(X_test, y_test)))print(\"Best parameters: &#123;&#125;\".format(grid.best_params_)) 这里采用 5 折网格参数搜索，但是在 pipeline 情况下，需要把搜索参数增加对应步骤的名字作为前缀，这样才会被 pipeline 中的某个步骤使用，因此，要想搜索 SVC 的 C 参数，必须使用 svm__C 作为参数网格字典的键，对 gamma 参数也是同理。 通用管道接口123456789101112131415161718192021222324252627282930313233343536from sklearn.svm import SVCfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipeline# 加载并划分数据cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)# 先缩放再跑模型的 pipelinepipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])# 5 折网格搜索param_grid = &#123;'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]&#125;grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)# 最佳泛化的训练结果print(grid.best_estimator_)# 最佳结果中的 svm 步骤print(grid.best_estimator_.named_steps['svm'])'''Out:Pipeline(memory=None, steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svm', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))])SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)''' 可以看到具有最佳泛化的 pipeline，其 2 个步骤的训练结果也对应的保存了起来，我们可以取出其中的某一步骤的训练结果。 网格搜索&amp;预处理添加交互多项式特征的预处理步骤。 12345678910111213141516171819202122232425262728from sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.datasets import load_bostonfrom sklearn.linear_model import Ridge# 加载并划分数据boston = load_boston()X_train, X_test, y_train, y_test = train_test_split( boston.data, boston.target, random_state=0)# 缩放数据 + 生成多项式特征 + 岭回归pipe = Pipeline([(\"scaler\", StandardScaler()), (\"ploy\", PolynomialFeatures()), ('ridge', Ridge())])# 前缀指定各个步骤的搜索参数param_grid = &#123;'ploy__degree': [1, 2, 3], 'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]&#125;# 网格搜索grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)# 精度print(grid.score(X_test, y_test))# 最佳泛化的一组参数print(grid.best_params_) 网格搜索&amp;模型选择12345678910111213141516171819202122232425262728293031323334353637383940414243444546from sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.datasets import load_breast_cancerfrom sklearn.linear_model import Ridgefrom sklearn.ensemble import RandomForestClassifier# 加载并划分数据cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)# 定义一下 pipeline 的 2 个步骤pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])# 对 SVC 模型进行 gamma 参数搜索、以及是否预处理的比较# 对随机森林分类器进行 max_features 参数搜索、并且不进行预处理param_grid = [ &#123;'classifier': [SVC()], 'preprocessing': [StandardScaler(), None], 'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100], 'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100] &#125;, &#123;'classifier': [RandomForestClassifier(n_estimators=100)], 'preprocessing': [None], 'classifier__max_features': [1, 2, 3] &#125;]# 网格搜索grid = GridSearchCV(pipe, param_grid, cv=5)grid.fit(X_train, y_train)# 精度print(grid.score(X_test, y_test))# 最佳泛化的一组参数print(grid.best_params_)'''out:0.9790209790209791&#123;'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'preprocessing': StandardScaler(copy=True, with_mean=True, with_std=True)&#125;''' 从结果可以看出，StandardScaler 缩放 + SVC 分类模型的效果要好于随机森林分类。 处理文本数据在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。这些术语来自于信息检索（information retrieval，IR）和自然语言处理（natural language processing，NLP）的社区，它们主要针对文本数据。 词袋下面利用 CountVectorizer 对原始本文输入进行词袋统计，从而转换成稀疏的特征向量作为模型输入。 123456789101112131415161718192021222324252627from sklearn.feature_extraction.text import CountVectorizer# 2 行文本数据bards_words =[\"The fool doth think he is wise,\", \"but the wise man knows himself to be a fool\"]# 对每个文档分词, 生成样本中所有词的表, 但是忽略掉那些只出现在 1 个样本中的单词, 并且忽略掉停词vect = CountVectorizer(min_df=2, stop_words='english')vect.fit(bards_words)# 打印词表print(vect.vocabulary_)# 打印特征向量的构成print(vect.get_feature_names())# 文本数据转换成词袋特征向量bag_of_words = vect.transform(bards_words)# 转成稠密矩阵输出 2 行特征向量print(bag_of_words.toarray())'''out:&#123;'fool': 0, 'wise': 1&#125;['fool', 'wise'][[1 1] [1 1]]''' min_df 令词表仅保留了在不同文档中出现过至少 2 次的单词，另外 stop_words 指定忽略掉英文中的停词，其实上述过程就是舍弃我们认为不重要的单词。 tf-idf 缩放数据（term frequency–inverse document frequency）即词频-逆向文档频率，如果一个单词在某个特定文档中经常出现，但在许多文档中却不常出现，那么这个单词可能是对文档的很好的描述。 123456789101112131415161718192021222324252627282930from sklearn.feature_extraction.text import TfidfVectorizer# 2 行文本数据bards_words =[\"The fool doth think he is wise,\", \"but the wise man knows himself to be a fool\"]# 文本统计 tf-idfvect = TfidfVectorizer()vect.fit(bards_words)# 打印词表print(vect.vocabulary_)# 打印特征向量的构成print(vect.get_feature_names())# 将输入文本分词，并用每个分词的 tf-idf 作为特征值tfidf_X = vect.transform(bards_words)# 转成稠密矩阵输出print(tfidf_X.toarray())'''&#123;'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0&#125;['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise'][[0. 0. 0.42567716 0.30287281 0.42567716 0. 0.42567716 0. 0. 0.30287281 0.42567716 0. 0.30287281] [0.36469323 0.36469323 0. 0.25948224 0. 0.36469323 0. 0.36469323 0.36469323 0.25948224 0. 0.36469323 0.25948224]] '''","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"ML 理论","slug":"数学推导/ML 理论","date":"2020-08-15T09:55:40.000Z","updated":"2020-08-30T03:47:28.793Z","comments":true,"path":"2020/08/15/数学推导/ML 理论/","link":"","permalink":"http://yoursite.com/2020/08/15/%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/ML%20%E7%90%86%E8%AE%BA/","excerpt":"作为一个学渣，在看书中总会遇到许多不懂的基础知识，在这里做个汇总方便查看，这里是数学推导的线性代数篇。 学习机器学习相关数学公式的推导，记录自我推导过程以及推导过程中所产生的疑问，查漏补缺，以形成一套自我的 ML 数学理论体系为目标。","text":"作为一个学渣，在看书中总会遇到许多不懂的基础知识，在这里做个汇总方便查看，这里是数学推导的线性代数篇。 学习机器学习相关数学公式的推导，记录自我推导过程以及推导过程中所产生的疑问，查漏补缺，以形成一套自我的 ML 数学理论体系为目标。 频率派 VS. 贝叶斯派","categories":[{"name":"数学推导","slug":"数学推导","permalink":"http://yoursite.com/categories/%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/"}],"tags":[{"name":"ML 理论","slug":"ML-理论","permalink":"http://yoursite.com/tags/ML-%E7%90%86%E8%AE%BA/"}]},{"title":"深度学习入门","slug":"机器学习/深度学习入门","date":"2020-08-08T02:16:57.000Z","updated":"2020-09-05T02:49:52.745Z","comments":true,"path":"2020/08/08/机器学习/深度学习入门/","link":"","permalink":"http://yoursite.com/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/","excerpt":"深度学习入门：基于 Python 的理论与实现。","text":"深度学习入门：基于 Python 的理论与实现。 必备知识在正式开始学习学习深度学习之前，我们先来了解一些必须要掌握的基本知识。（Python 的基础语法请参照本人的另一篇博客：父与子编程之旅） AnacondaAnaconda 可以统一管理开发环境，以及便捷的获取及管理包。 12345678# 创建一个名为 py37 的开发环境，并指定 python 版本conda create -n py37 python=3.7# 激活 py37 环境conda activate py37# 退出 py37 环境conda deactivate py37# 删除conda remove -n py37 --all NumPy在深度学习的实现中，经常出现数组和矩阵的计算。NumPy 的数组类 numpy.array 中提供了很多便捷的方法，在实现深度学习时，我们将使用这些方法。本节我们来简单介绍一下后面会用到的 NumPy。 123456789101112131415## 导入 NumPy 库import numpy as np## 生成 NumPy 数组x = np.array([1.0, 2.0, 3.0])print(x) # [ 1. 2. 3.]type(x) # &lt;class 'numpy.ndarray'&gt;## NumPy 数组的算术运算x = np.array([1.0, 2.0, 3.0])y = np.array([2.0, 4.0, 6.0])x + y # array([ 3., 6., 9.])x - y # array([ -1., -2., -3.])x * y # array([ 2., 8., 18.])x / y # array([ 0.5, 0.5, 0.5]) 注意，在上面的 NumPy 算数运算中，数组 x 和数组 y 的元素个数是相同的（对应元素计算），如果元素个数不同，程序就会报错，所以元素个数保持一致非常重要。 此外，NumPy 数组不仅可以进行 element-wise 运算，也可以和单一的数值（标量）组合器来进行运算。 此时，需要在 NumPy 数组的各个元素和标量之间进行运算。 这个功能也被称为广播。 12345678910111213141516171819202122232425262728293031x = np.array([1.0, 2.0, 3.0])x / 2.0 # array([ 0.5, 1. , 1.5])## NumPy 的 N 维数组A = np.array([[1, 2], [3, 4]])print(A)'''[[1 2] [3 4]]'''A.shape # (2, 2) 第一个维度有两个元素 第二维度有两个元素A.dtype # dtype('int64') 矩阵元素的数据类型## 矩阵的算术运算B = np.array([[3, 0],[0, 6]])A + B'''array([[ 4, 2], [ 3, 10]])'''A * B'''array([[ 3, 0], [ 0, 24]])'''# 与标量相乘的广播功能A * 10'''array([[ 10, 20], [ 30, 40]])''' NumPy 数组 np.array 可以生成 N 维数组，即可以生成一维数组、 二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量， 将二维数组称为矩阵。另外，可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为矩阵，将三维数组及三维以上的数组称为张量或多维数组。 1234567891011121314151617181920212223242526## 访问元素# 索引访问X = np.array([[51, 55], [14, 19], [0, 4]])print(X)'''[[51 55] [14 19] [ 0 4]]'''X[0] # 第 0 行 array([51, 55])X[0][1] # (0,1) 的元素 55# for 语句访问for row in X: print(row)'''[51 55][14 19][ 0 4]'''# 数组访问X = X.flatten() # 将 X 转换为一维数组print(X) # [51 55 14 19 0 4]X[np.array([0, 2, 4])] # 获取索引为 0、2、4 的元素 array([51, 14, 0])# 运用标记法，从 X 中抽出大于 15 的元素X &gt; 15 # array([ True, True, False, True, False, False], dtype=bool)X[X&gt;15] # array([51, 55, 19]) 取出 True 对应的元素 矩阵的运算。（注意！！！矩阵的点乘得遵循一定的法则 (A, B) · (B, A) = (A, A)） 12345678910111213A = np.array([[1,2,3], [4,5,6]])np.ndim(A) # 2 有两个维度A.shape # (2, 3)B = np.array([[1,2], [3,4], [5,6]])B.shape # (3, 2)np.dot(A, B) # 点乘，矩阵的乘法'''array([[22, 28], [49, 64]])'''B = np.array([[1,2], [3,4], [5,6]]np.ndim(B) # 2B.shape # (3, 2) Matplotlib深度学习实验中，图形的绘制和数据的可视化非常重要。Matplotlib 是用于绘制图形的库，使用 Matplotlib 可以轻松地绘制图形和实现数据的可视化。这里，我们来介绍一下图形的绘制方法和图像的显示方法。 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt## 绘制 sin 以及 cos 函数的图形# 生成数据x = np.arange(0, 6, 0.1) # 以 0.1 为步长单位，生成 0 到 6 的数据y1 = np.sin(x)y2 = np.cos(x)# 绘制图形plt.plot(x, y1, label=\"sin\")plt.plot(x, y2, linestyle = \"--\", label=\"cos\") # 用虚线绘制plt.xlabel(\"x\") # x 轴标签plt.ylabel(\"y\") # y 轴标签plt.title('sin &amp; cos') # 标题plt.legend() # 自动对上诉添加的标签布局，复杂时可以自定义plt.show() pyplot 中还提供了用于显示图像的方法 imshow()。另外，可以使用 matplotlib.image 模块的 imread() 方法读入图像。 1234567## 显示图像import matplotlib.pyplot as pltfrom matplotlib.image import imreadimg = imread('lena.png') # 读入图像（设定合适的路径！）plt.imshow(img)plt.show() 感知机感知机可以说是神经网络（深度学习）的起源的算法，因此， 学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。 感知机接收多个输入信号，输出一个信号。 简单逻辑电路形式一：x1w1 + x2w2 ≤ θ 👉 y = 0 （θ 代表阈值、x 代表输入信号、w 代表权重） 形式二：x1w1 + x2w2 + b ≤ 0 （其中 b = -θ） 👉 y = 0 （b 代表偏置） 123456789101112131415161718192021222324252627282930313233343536373839404142## 与门# 简单实现 def AND(x1, x2): w1, w2, theta = 0.5, 0.5, 0.7 tmp = x1*w1 + x2*w2 if tmp &lt;= theta: return 0 elif tmp &gt; theta: return 1# 使用权重和偏置的实现def AND(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1## 非门：与门反过来即可def NAND(x1, x2): x = np.array([x1, x2]) w = np.array([-0.5, -0.5]) # 仅权重和偏置与 AND 不同！ b = 0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1## 或门def OR(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) # 仅权重和偏置与AND不同！ b = -0.2 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1## 异或门（无法用上面类似的形式进行实现） 为什么异或门无法用上述相似的形式（线性）进行实现呢？先来看看异或门的结果分部。 通过分析上图可以发现，无法用线性空间（直线）将 0 和 1 这两种情况进行区域的划分。但用非线性的空间（曲线）可以，那如何划分非线性空间呢？答案是：叠加感知机。即多层感知机。 多层感知机通过组合与门、与非门、或门实现异或门。 123456# 异或门def XOR(x1, x2): s1 = NAND(x1, x2) s2 = OR(x1, x2) y = AND(s1, s2) return y 神经网络在上一章中，我们使用感知机实现了简单的逻辑电路。从实现的逻辑中我们可以发现，即使是复杂的函数，感知机也隐含着能够代表它的可能性。但有一个很令人头疼的问题，那便是权重的设定。在实现简单逻辑电路等简单函数时，我们可以通过观察归纳得出符合条件的权重，但如果函数特别的复杂，那么光靠人脑去猜测其权重显然是不合适的。 神经网络的出现就是为了解决这个问题的。具体地讲，神经网络的一 个重要性质是它可以自动地从数据中学习到合适的权重参数。 激活函数激活函数是连接感知机和神经网络的桥梁。 隐藏（中间层）的激活函数。 Sigmoid 函数12def sigmoid(x): return 1 / (1 + np.exp(-x)) 阶跃函数12345678910# 简单实现def step_function(x): if x &gt; 0: return 1 else: return 0# 支持 NumPy 数组实现def step_function(x): y = x &gt; 0 return y.astype(np.int) # 把数组 y 的元素类型从 boole 型转换为 int 型 ReLU 函数12def relu(x): return np.maximum(0, x) 输出层的激活函数。 如何选择输出层的激活函数？ 一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数， 多元分类问题可以使用 softmax 函数。 机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题（寻找决策边界）。比如，区分图像中的人是男性还是女性的问题就是分类问题。而回归问题是根据某个输入预测一个（连续的） 数值的问题（找到最优拟合）。 恒等函数12def identity_function(x): return x softmax 函数exp(x) 是表示 ex 的指数函数（e 是纳皮尔常数2.7182 …），容易出现很大的值从而导致数值的溢出问题。在进行 softmax 的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果，为了防止溢出，一般会使用输入信号中的最大值。 如上所示，softmax 函数的输出是 0.0 到 1.0 之间的实数。并且，softmax 函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质。正因为有了这个性质，我们才可以把 softmax 函数的输出解释为概率。 123456789101112def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y# 优化def softmax(a): c = np.max(a) exp_a = np.exp(a - c) # 溢出对策 sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y 简单实现了解了激活函数，让我们来看一个简单的神经网络。这个神经网络省略了偏置和激活函数，只有权重。 我们需要了解的重点是：神经网络的运算可以作为矩阵运算打包进行。 隐藏（中间）层的激活函数表示为：h() 输出层的激活函数表示为：σ() 1234567891011X = np.array([1, 2])X.shape # (2,)W = np.array([[1, 3, 5], [2, 4, 6]])print(W)'''[[1 3 5] [2 4 6]]'''W.shape # (2, 3)Y = np.dot(X, W)print(Y) # [ 5 11 17] 如上所示，使用 np.dot （多维数组的点积），可以一次性计算出 Y 的结果。 这意味着，即便 Y 的元素个数为 100 或 1000，也可以通过一次运算就计算出结果！如果不使用 np.dot，就必须单独计算 Y 的每一个元素（或者说必须使用 for 语句），非常麻烦。因此，通过矩阵的乘积一次性完成计算的技巧，在实现的层面上可以说是非常重要的。 现在我们将偏置还有激活函数考虑进来。 12345678910111213141516171819202122232425262728## 第一层处理# 设置信号、权重、偏置为某一任意值X = np.array([1.0, 0.5])W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])B1 = np.array([0.1, 0.2, 0.3])print(W1.shape) # (2, 3)print(X.shape) # (2,)print(B1.shape) # (3,)A1 = np.dot(X, W1) + B1 # 一次得出第一层的所有的 aZ1 = sigmoid(A1)print(A1) # [0.3, 0.7, 1.1] # 第一层的结果print(Z1) # [0.57444252, 0.66818777, 0.75026011] # sigmoid 函数进行处理## 第二层处理，与第一层类似W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])B2 = np.array([0.1, 0.2])print(Z1.shape) # (3,)print(W2.shape) # (3, 2)print(B2.shape) # (2,)A2 = np.dot(Z1, W2) + B2Z2 = sigmoid(A2)## 第二层到输出层的处理# 输出层的激活函数与之前隐藏层的有所不同def identity_function(x): return x # 恒等函数W3 = np.array([[0.1, 0.3], [0.2, 0.4]])B3 = np.array([0.1, 0.2])A3 = np.dot(Z2, W3) + B3Y = identity_function(A3) # 或者 Y = A3，此处按照规范使用一个返回自身的激活函数 最后我们将本章的代码流程进行整理，方便阅读。 12345678910111213141516171819202122232425## 权重及偏置的初始化def init_network(): network = &#123;&#125; # 保存所有初始化参数的字典 network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) network['b1'] = np.array([0.1, 0.2, 0.3]) network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) network['b2'] = np.array([0.1, 0.2]) network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) network['b3'] = np.array([0.1, 0.2]) return network## 封住了将输入信号转换为输出信号的处理过程def forward(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = identity_function(a3) return ynetwork = init_network()x = np.array([1.0, 0.5])y = forward(network, x)print(y) # [ 0.31682708 0.69627909] 手写数字识别介绍完神经网络的结构之后，现在我们来试着解决实际问题。 假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的推理处理。这个推理处理也称为神经网络的前向传播（forward propagation）。 求解机器学习问题的步骤可以分为学习和推理两个阶段。 首先，在学习阶段进行模型的学习， 然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。 一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。 并且，即便使用 softmax 函数，输出值最大的神经元的位置也不会变。因此， 神经网络在进行分类时，输出层的 softmax 函数可以省略。 在输出层使用 softmax 函数是因为它和神经网络的学习有关系。 1234567891011121314151617181920212223242526272829303132## 让我们先尝试着将数据集 MINST 进行导入import sys, ossys.path.append(os.pardir) # 为了导入父目录中的文件而进行的设定from dataset.mnist import load_mnist# 读入 MNIST 数据集（第一次调用会花费几分钟）# flatten 将形状压成一维# normalize 将输入图像正规化为 0.0～1.0 的值# (训练图像 ,训练标签)，(测试图像，测试标签)(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)# 输出各个数据的形状print(x_train.shape) # (60000, 784)print(t_train.shape) # (60000,)print(x_test.shape) # (10000, 784)print(t_test.shape) # (10000,)## 接下来试着使用 PIL（Python Image Library） 模块来显示图片import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom PIL import Imagedef img_show(img): # 定义显示图片的函数 pil_img = Image.fromarray(np.uint8(img)) pil_img.show()(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)img = x_train[0]label = t_train[0]print(label) # 5print(img.shape) # (784,)img = img.reshape(28, 28) # 把图像的形状变成原来的尺寸print(img.shape) # (28, 28)img_show(img) 好了，到这一步我们已经对这个数据集有一定的了解，接下来我们来思考一下对这个 MINST 数据集实现神经网络的推理处理。 神经网络的输入层有 784 个神经元，输出层有 10 个神经元。输入层的 784 这个数字来源于图像大小的 28 × 28 = 784，输出层的10 这个数字来源于 10 类别分类（数字 0 到 9，共 10 个类别）。此外，这个神经网络有 2 个隐藏层，第 1 个隐藏层有 50 个神经元，第 2 个隐藏层有 100 个神经元。这个 50 和 100 可以设置为任何值。 1234567891011121314151617181920212223242526272829303132333435## 获取测试数据（测试集与标签集）def get_data(): (x_train, t_train), (x_test, t_test) = \\ load_mnist(normalize=True, flatten=True, one_hot_label=False) return x_test, t_test## 获取权重（数据集自带的）def init_network(): # Binary Mode 返回 Bytes with open(\"sample_weight.pkl\", 'rb') as f: network = pickle.load(f) return network## 预测过程（模型）def predict(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = softmax(a3) return y## 评价识别精度x, t = get_data()network = init_network()accuracy_cnt = 0for i in range(len(x)): y = predict(network, x[i]) p = np.argmax(y) # 获取概率最高的元素的索引 if p == t[i]: accuracy_cnt += 1 # 记录正确的次数print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x))) 把数据限定到某个范围内的处理称为正规化（normalization）。此外，对神经网络的输入数据进行某种既定的转换称为预处理（preprocessing）。 很多预处理都会考虑到数据的整体分布。比如，利用数据整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。除此之外，还有将数据整体的分布形状均匀化的方法，即数据白化（whitening）等。 批处理在上面的预测中，我们一次只预测一个输入。其实还有一种更简单快捷的方式：批处理。 批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。 12345678910x, t = get_data()network = init_network()batch_size = 100 # 批数量accuracy_cnt = 0for i in range(0, len(x), batch_size): [0, 100, 200 ...] x_batch = x[i:i+batch_size] # 切片 [0, 100) [100, 200) y_batch = predict(network, x_batch) p = np.argmax(y_batch, axis=1) # 取得所有第一个维度上的最大值 accuracy_cnt += np.sum(p == t[i:i+batch_size]) # 批量判断是否准确并累加print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x))) 神经网络的学习上一章我们了解神经网络的推测过程，在推测的过程中我们使用到了权重与偏置参数，这些参数在上一章中并没有提及是如何得出的。 而神经网络的学习，其学习过程便是指从训练数据中自动获取最优权重参数的过程。 本章中，为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。为了找出尽可能小的损失函数的值，本章我们将介绍利用了函数斜率的梯度法。 从数据中学习在实际的神经网络中，参数的数量可能是成千上万个，在层数更深的深度学习中，参数的数量甚至可能上亿，想要人工决定这些参数的值是不可能的。 所以，我们要完成一件非常了不起的事情：数据自动决定权重参数的值。 一般来讲，需要在数据中提取特征量，这里所说的特征量是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。特征的量的提取可以分为人工提取（机器学习）与机器提取（深度学习）两大类。 机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。 为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。 泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。顺便说一下，只对某个数据集过度拟合的状态称为过拟合（Over Fitting）。避免过拟合也是机器学习的一个重要课题。 损失函数损失函数是表示神经网络性能的恶劣程度的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。 均方误差12345678910def mean_squared_error(y, t): return 0.5 * np.sum((y-t)**2) # y 代表神经网络输出 t 代表监督数据# 设 2 为正确解t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]# 2 的概率最高的情况（0.6）y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]mean_squared_error(np.array(y), np.array(t)) # 0.097500000000000031# 7 的概率最高的情况（0.6）y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]mean_squared_error(np.array(y), np.array(t)) # 0.59750000000000003 交叉熵误差1234567891011121314151617181920212223242526272829def cross_entropy_error(y, t): delta = 1e-7 # 微小值保护 np.log(0) 会变为负无限大的 -inf return -np.sum(t * np.log(y + delta))t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]cross_entropy_error(np.array(y), np.array(t))0.51082545709933802y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]cross_entropy_error(np.array(y), np.array(t))2.3025840929945458## mini-batch 版本实现def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] # 用 batch 的个数进行正规化，计算单个数据的平均交叉熵误差 return -np.sum(t * np.log(y + 1e-7)) / batch_size# 监督数据是标签形式，即非 one-hotdef cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] # np.arange(batch_size) 生成从 0 到 batch_size-1 的数组 # y[np.arange(batch_size), t] 生成 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]] return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size 在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。 数值微分梯度法的核心就是数值微分，在了解梯度法之前我们先来讨论一下数值微分中的导数和偏导数。 导数只存在一个变量的情况。 123def numerical_diff(f, x): h = 1e-4 # 0.0001 return (f(x+h) - f(x-h)) / (2*h) # 中心差分 偏导数和梯度存在多个变量。 由全部变量的偏导数汇总而成的向量称为梯度（gradient）。 1234567891011121314def numerical_gradient(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) # 生成和 x 形状相同的数组 for idx in range(x.size): # 每次只对一个变量进行中心差分求导 tmp_val = x[idx] # f(x+h)的计算 x[idx] = tmp_val + h fxh1 = f(x) # f(x-h)的计算 x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 还原值 return grad 实际上， 梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向。通过巧妙地使用梯度来寻找函数最小值 （或者尽可能小的值）的方法就是梯度法。 在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进， 如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进， 逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。 不断的前进的过程中，我们需要考虑一次前进多少？这就是神经网络的学习中的学习率。 1234567# 梯度下降法 学习率为 0.01 学习次数为 100 次def gradient_descent(f, init_x, lr=0.01, step_num=100): x = init_x for i in range(step_num): grad = numerical_gradient(f, x) x -= lr * grad return x 像学习率这样的由人工设定的参数称为超参数。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。 神经网络的梯度神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度。 123456789101112131415161718192021222324252627282930313233343536373839import sys, ossys.path.append(os.pardir)import numpy as npfrom common.functions import softmax, cross_entropy_errorfrom common.gradient import numerical_gradientclass simpleNet: def __init__(self): self.W = np.random.randn(2,3) # 用高斯分布进行初始化 def predict(self, x): return np.dot(x, self.W) def loss(self, x, t): z = self.predict(x) y = softmax(z) loss = cross_entropy_error(y, t) # 交叉熵误差 return loss## 求这个简单神经网络的损失net = simpleNet()print(net.W) # 权重参数'''[[ 0.47355232 0.9977393 0.84668094], [ 0.85557411 0.03563661 0.69422093]])'''x = np.array([0.6, 0.9])p = net.predict(x)print(p) # [ 1.05414809 0.63071653 1.1328074]np.argmax(p) # 最大值的索引t = np.array([0, 0, 1]) # 正确解标签net.loss(x, t) # 0.92806853663411326## 求梯度f = lambda w: net.loss(x, t) # 此处传参 w 为伪参数，求梯度中会进行传参dW = numerical_gradient(f, net.W)print(dW)'''[[ 0.21924763 0.14356247 -0.36281009] [ 0.32887144 0.2153437 -0.54421514]]''' 到目前为止，我们已经得到了这个神经网络的损失函数所对应的梯度了，让我们来观察一下这个梯度矩阵。首先，形状上梯度矩阵与权重矩阵的形状相同，并且应该是对应的关系：w(2,3) 对应的梯度大约为 -0.5，这意味着这个方向上的权重每增加 h，损失函数的值将减小 0.5*h 求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可。 学习算法的实现步骤 1（mini-batch）： 从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小 mini-batch 的损失函数的值。 步骤 2（计算梯度）： 为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。 梯度表示损失函数的值减小最多的方向。 步骤 3（更新参数）： 将权重参数沿梯度方向进行微小更新。 步骤 4（重复）： 重复步骤1、步骤2、步骤3。 因为这里使用的数据是随机选择的 mini batch 数据，所以又称为随机梯度下降法 SGD（Stochastic Gradient Descent）。 下面，我们来实现手写数字识别的神经网络。这里以 2 层神经网络（隐藏层为 1 层的网络）为对象，使用 MNIST 数据集进行学习。 12345678910111213141516171819202122232425262728293031323334353637383940## 两层神经网络的类import sys, ossys.path.append(os.pardir)from common.functions import *from common.gradient import numerical_gradientclass TwoLayerNet: def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 初始化权重 self.params = &#123;&#125; self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def predict(self, x): W1, W2 = self.params['W1'], self.params['W2'] b1, b2 = self.params['b1'], self.params['b2'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 y = softmax(a2) return y # x:输入数据, t:监督数据 def loss(self, x, t): y = self.predict(x) return cross_entropy_error(y, t) def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis=1) t = np.argmax(t, axis=1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy # x:输入数据, t:监督数据 def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = &#123;&#125; grads['W1'] = numerical_gradient(loss_W, self.params['W1']) grads['b1'] = numerical_gradient(loss_W, self.params['b1']) grads['W2'] = numerical_gradient(loss_W, self.params['W2']) grads['b2'] = numerical_gradient(loss_W, self.params['b2']) return grads mini-batch 的实现 12345678910111213141516171819202122232425import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_laobel = True)train_loss_list = []# 超参数iters_num = 10000 # 循环次数train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)for i in range(iters_num): # 获取 mini-batch，随机获取 100 个 batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 计算梯度 grad = network.numerical_gradient(x_batch, t_batch) # grad = network.gradient(x_batch, t_batch) # 高速版! # 更新参数 for key in ('W1', 'b1', 'W2', 'b2'): network.params[key] -= learning_rate * grad[key] # 记录学习过程 loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) 这里给出的算法记录了每一次的损失，但其实这是没有必要的，我们其实只要对一次 epoch 做一次评价就可以。 epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被（宏观上）使用过一次时的更新次数。 1234567891011121314151617181920212223242526272829303132333435import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = \\ load_mnist(normalize=True, one_hot_ laobel = True)train_loss_list = []train_acc_list = []test_acc_list = []# 平均每个 epoch 的重复次数iter_per_epoch = max(train_size / batch_size, 1)# 超参数iters_num = 10000batch_size = 100learning_rate = 0.1network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)for i in range(iters_num): # 获取 mini-batch batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 计算梯度 grad = network.numerical_gradient(x_batch, t_batch) # grad = network.gradient(x_batch, t_batch) # 高速版! # 更新参数 for key in ('W1', 'b1', 'W2', 'b2'): network.params[key] -= learning_rate * grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) # 计算每个 epoch 的识别精度 if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc)) 误差反向传播在前面的章节中，我们用数值微分计算了神经网络的权重参数的梯度。数值微分简单易实现，但在计算上是比较耗费时间的。本章我们将学习一个高效计算权重参数的梯度方法：误差反向传播。 首先我们通过图片来理解这一方法。 上图表示的是基于反向传播的导数传递。 让我们来看看图中的各数值代表什么。在这个例子中，反向传播从右向左传递导数的值（1 👉 1.1 👉2.2）。这表示支付金额关于苹果价格的导数的值是 2.2，即苹果每上涨 ð 元，最终的支付金额会增加 2.2ð 元。 通过观察我们可以发现一个更为通用的法则，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数（链式法则）。 反向传播的节点除了 x 还有 +。加法节点的变化在传播的过程中不对上一层的链式传播数值产生影响。 来看一个比较复杂的例子。 苹果个数对最终价格的影响为：110。 推导过程：从右到左 715 = 650 x 1.1 这里得到影响因子 1.1 👉 加法节点直接继承上一层的影响因子 1.1 👉 200 = 2 x 100 这里得到影响因子 1.1 x 100 = 110。 代码实现在这里我们用 Python 实现上述的例子。乘法节点为乘法层（MulLayer），加法节点称为加法层（AddLayer）。 乘法层123456789101112131415class MulLayer: def __init__(self): self.x = None self.y = None # 正向传播 def forward(self, x, y): self.x = x self.y = y out = x * y return out # 反向传播 dout 为上游传来的导数 def backward(self, dout): dx = dout * self.y # 翻转 x 和 y dy = dout * self.x return dx, dy 那我们买苹果的计算图的正向传播可以用以下代码表示。 12345678910apple = 100apple_num = 2tax = 1.1# layermul_apple_layer = MulLayer()mul_tax_layer = MulLayer()# forwardapple_price = mul_apple_layer.forward(apple, apple_num)price = mul_tax_layer.forward(apple_price, tax)print(price) # 220 关于各个变量的导数可由 backward() 求出 12345# backwarddprice = 1dapple_price, dtax = mul_tax_layer.backward(dprice)dapple, dapple_num = mul_apple_layer.backward(dapple_price)print(dapple, dapple_num, dtax) # 2.2 110 200 加法层12345678910class AddLayer: def __init__(self): pass def forward(self, x, y): out = x + y return out def backward(self, dout): dx = dout * 1 dy = dout * 1 return dx, dy 我们来用 Python 代码实现一下买苹果和橘子的计算图。 1234567891011121314151617181920212223apple = 100apple_num = 2orange = 150orange_num = 3tax = 1.1# layermul_apple_layer = MulLayer()mul_orange_layer = MulLayer()add_apple_orange_layer = AddLayer()mul_tax_layer = MulLayer()# forwardapple_price = mul_apple_layer.forward(apple, apple_num) # (1)orange_price = mul_orange_layer.forward(orange, orange_num) # (2)all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)price = mul_tax_layer.forward(all_price, tax) #( 4)# backwarddprice = 1dall_price, dtax = mul_tax_layer.backward(dprice) # (4)dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)print(price) # 715print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650 激活函数层的实现现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 ReLU 层和 Sigmoid 层。 ReLU 层 如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。 123456789101112class Relu: def __init__(self): self.mask = None def forward(self, x): self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self, dout): dout[self.mask] = 0 dx = dout return dx ReLU 类有实例变量 mask。这个变量 mask 是由 True / False 构成的 NumPy 数 组，它会把正向传播时的输入 x 的元素中小于等于 0 的地方保存为 True，其他地方（大于 0 的元素）保存为 False。 如果正向传播时的输入值小于等于 0，则反向传播的值为 0。 因此，反向传播中会使用正向传播时保存的 mask，将从上游传来的 dout 的 mask 中的元素为 True 的地方设为 0。 Sigmoid 层 这里 / 用倒数的 x 计算。 对结果进行进一步的整理。 让我们将推导的过程隐蔽，我们可以得到一个 Sigmoid 节点。 12345678910class Sigmoid: def __init__(self): self.out = None def forward(self, x): out = 1 / (1 + np.exp(-x)) self.out = out return out def backward(self, dout): dx = dout * (1.0 - self.out) * self.out return dx Affine / Softmax 层的实现在很前面的章节中（忘了赶紧回去给我复习：神经网络 / 简单实现]]），我们利用公式 Y = np.dot(X,W) + B 计算出神经元的加权。然后 Y 经过激活函数转换后，传递给下一层，这就是神经网络正向传播过程。 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为仿射变换。因此，这里将进行仿射变换的处理实现为 Affine 层。 让我们来看看将这个过程转换为计算图是什么样子的。 正向传播时，偏置会被加到每一个数据上。因此，反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。 Affine 层：神经网络的正向传播中，进行的矩阵的乘积运算，在几何学领域被称为仿射变换。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。 12345678910111213141516class Affine: def __init__(self, W, b): self.W = W self.b = b self.x = None self.dW = None self.db = None def forward(self, x): self.x = x out = np.dot(x, self.W) + self.b return out def backward(self, dout): dx = np.dot(dout, self.W.T) self.dW = np.dot(self.x.T, dout) self.db = np.sum(dout, axis=0) # 公式 3 把 b 当成 return dx Softmax-with-Loss 层在神经网络的学习中，我们一般需要将输出正规化后输入损失函数进行处理，一般会使用 Softmax 层；当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。 Softmax-with-Loss：这里的 Loss 指交交叉熵误差层 Cross Entropy Error。 这图看到人就傻了，让我们来简化一下。 让我们来看看各个字母所代表的含义。（y1, y2, y3） 是 Softmax 层的输出，（t1, t2, t3） 是监督数据，所以 （y1 − t1, y2 − t2, y3 − t3） 是 Softmax 层的输出和教师标签的差分。所以 （y1 − t1, y2 − t2, y3 − t3） 是 Softmax 层的输出和教师标签的差分（即输出与标签的误差）。 使用交叉熵误差作为 softmax 函数的损失函数后，反向传播得到 （y1 − t1, y2 − t2, y3 − t3） 这样漂亮的结果。实际上，这样漂亮的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。 回归问题中输出层使用恒等函数，损失函数使用平方和误差，也是出于同样的理由（忘了就给我翻回去复习！！！）。也就是说，使用平方和误差作为恒等函数的损失函数，反向传播才能得到 （y1 − t1, y2 − t2, y3 − t3） 这样漂亮的结果。 1234567891011121314class SoftmaxWithLoss: def __init__(self): self.loss = None # 损失 self.y = None # softmax 的输出 self.t = None # 监督数据（one-hot vector） def forward(self, x, t): self.t = t self.y = softmax(x) self.loss = cross_entropy_error(self.y, self.t) return self.loss def backward(self, dout=1): batch_size = self.t.shape[0] dx = (self.y - self.t) / batch_size return dx 总结现在来进行神经网络的实现。这里我们要把 2 层神经网络实现为 TwoLayerNet。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import sys, ossys.path.append(os.pardir)import numpy as npfrom common.layers import *from common.gradient import numerical_gradientfrom collections import OrderedDictclass TwoLayerNet: # 输入层的神经元数、隐藏层的神经元数、输出层的神经元数、初始化权重时的高斯分布的规模 def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 初始化权重 params 为参数字典 self.params = &#123;&#125; self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) # 生成层 layers 有序字典型变量 保存神经网络的层 self.layers = OrderedDict() self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) self.layers['Relu1'] = Relu() self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) # 神经网络的最后一层 self.lastLayer = SoftmaxWithLoss() def predict(self, x): for layer in self.layers.values(): x = layer.forward(x) return x # x:输入数据, t:监督数据 def loss(self, x, t): y = self.predict(x) return self.lastLayer.forward(y, t) # 计算识别精度 def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis=1) if t.ndim != 1 : t = np.argmax(t, axis=1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy # x:输入数据, t:监督数据 通过数值微分计算关于权重的梯度 def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = &#123;&#125; grads['W1'] = numerical_gradient(loss_W, self.params['W1']) grads['b1'] = numerical_gradient(loss_W, self.params['b1']) grads['W2'] = numerical_gradient(loss_W, self.params['W2']) grads['b2'] = numerical_gradient(loss_W, self.params['b2']) return grads # 通过误差反向传播计算关于权重参数的梯度 def gradient(self, x, t): # forward self.loss(x, t) # backward dout = 1 dout = self.lastLayer.backward(dout) layers = list(self.layers.values()) layers.reverse() for layer in layers: dout = layer.backward(dout) # 设定 grads = &#123;&#125; grads['W1'] = self.layers['Affine1'].dW grads['b1'] = self.layers['Affine1'].db grads['W2'] = self.layers['Affine2'].dW grads['b2'] = self.layers['Affine2'].db return grads 注意，将神经网络的层保存在 OrderedDict 这一点非常重要（正向与反向都需要按顺序调用各层）。 可能在座的各位会有一个疑问：既然我们有了计算较为简单的误差反向传播，为什么还要由数值微分的方法呢？误差反向传播的计算虽然简单，但是实现较为复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。 1234567891011121314151617# 梯度确认import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet# 读入数据(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)x_batch = x_train[:3]t_batch = t_train[:3]grad_numerical = network.numerical_gradient(x_batch, t_batch)grad_backprop = network.gradient(x_batch, t_batch)# 求各个权重的绝对误差的平均值for key in grad_numerical.keys(): diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) ) print(key + \":\" + str(diff)) 最后，我们利用反向传播误差来实现一下神经网络的学习（就是把前面的数值微分替换成反向误差传播）。 12345678910111213141516171819202122232425262728293031323334import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet# 读入数据(x_train, t_train), (x_test, t_test) = \\load_mnist(normalize=True, one_hot_label=True)network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)iters_num = 10000train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1train_loss_list = []train_acc_list = []test_acc_list = []iter_per_epoch = max(train_size / batch_size, 1)for i in range(iters_num): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 通过误差反向传播法求梯度 grad = network.gradient(x_batch, t_batch) # 更新 for key in ('W1', 'b1', 'W2', 'b2'): network.params[key] -= learning_rate * grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(train_acc, test_acc) 与学习相关的技巧本章将介绍一下神经网络学习中的一些重要观点，让我们可以更高效地进行神经网络地学习，提高识别精度。 参数更新我们已经知道，神经网络的学习目的就是找到使损失函数的尽可能小的参数，这个寻找最优参数的问题称为最优化。随机梯度下降法（stochastic gradient descent）， 简称 SGD 就是一个简单最优化的方法。但是，根据不同的问题，也存在比 SGD 更加聪明的方法。接下来我们会介绍这些方法。 SGD先来看看 SGD 吧。 SGD 可以简单的用上面的图表示（右边的式子更新左边的式子）。 1234567891011class SGD: def __init__(self, lr=0.01): self.lr = lr def update(self, params, grads): for key in params.keys(): params[key] -= self.lr * grads[key]# 优化器optimizer = SGD()# ...optimizer.update(params, grads) MomentumMomentum 是动量的意思，和物理有关。 v 对应物理上的速度。αv 这一项表示在物体不受任何力时，该项承担使物体逐渐减速的任务（α 设定为 0.9 之类的值），对应物理上的地面摩擦或空气阻力。 对这两个公式的大致理解：小的叠加，大的抵消（减少震荡）。 AdaGrad前面提到的学习过程，其学习率是固定的，有些优化方式是动态的调节学习率，比如：学习率衰减。逐渐减小学习率的想法，相当于将全体参数的学习率值一起降低。 而 AdaGrad 进一步发展了这个想法，针对一个一个的参数，赋予其定制的值。 AdaGrad 会为参数的每个元素适当地调整学习率，与此同时进行学习。 h 保存了以前所有梯度的平方和，我们用这个参数来动态的调整梯度的变化程度。 123456789101112class AdaGrad: def __init__(self, lr=0.01): self.lr = lr self.h = None def update(self, params, grads): if self.h is None: self.h = &#123;&#125; for key, val in params.items(): self.h[key] = np.zeros_like(val) for key in params.keys(): self.h[key] += grads[key] * grads[key] params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) 权重初始值权重的初始值也决定了这个神经网络的学习是否可以成功，让我们来看看权重初始值的一些推荐值。 然我们先来考虑一下权值应该越大越好？还是越小越好？若权值越大，这个模型的泛化能力就差，所以权值应该是一个较小的数，在这之前的权重初始值都是像 0.01 * np.random.randn(10, 100) 这样，使用由高斯分布生成的值乘以 0.01 后得到的值（标准差为0.01的高斯分布）。 那初始权值可以设为 0 吗？答案是不可的，若初始权重都为 0，这使得神经网络拥有许多不同的权重的意义丧失了。为了防止权重均一化（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。 各层的激活值的分布都要求有适当的广度。通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者表现力受限的问题，导致学习可能无法顺利进行。 现在，在一般的深度学习框架中，Xavier 初始值已被作为标准使用。Xavier 初始值是以激活函数是线性函数为前提而推导出来的。因为 sigmoid 函数和 tanh 函数左右对称，且中央附近可以视作线性函数，所以适合使用 Xavier 初始值。 其实，权重初始值的设定与激活函数也有一定的关系。当激活函数使用 ReLU 时，一般推荐使用 ReLU 专用的初始值 He 初始值。 Batch Normalization前面讲到我们需要在各层拥有适当的广度，这样才可以使学习顺利进行。那么，为了使各层拥有适当的广度，强制性地调整激活值的分布会怎样呢？Batch Normalization 就是基于这种想法产生的。 正则化过拟合学习后生成的模型泛化能力弱：对样本数据识别良好，对测试数据识别很差。 权值衰退权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。 Dropout作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的 L2 范数的权值衰减方法。该方法可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情 况下，我们经常会使用 Dropout 方法。 Dropout 是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。 可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习，进而达到集成学习的效果。 超参数的验证神经网络中，除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。超参数的决定过程中常常伴随着很多试错。 因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。我们使用这个验证数据来评估超参数的好坏。 卷积神经网络卷积神经网络（Convolutional Neural Network，CNN）常被用于图像识别、语音识别等场合，在图像识别的比赛中，基于深度学习的方法几乎都以 CNN 为基础。本章就让我们来一睹 CNN 的 风采。 整体结构先让我们大致了解一下 CNN 的框架，其实它和神经网络一样都是通过组装层来构建的。不过，在 CNN 中还出现了新的层：卷积层（Convolution 层）和池化层（Pooling 层）。 CNN 中出现了一些特有的术语，比如填充、步幅等。此外，各层中传递的数据是有形状的数据（比如，3 维数据），这与之前的全连接网络不同，因此刚开始学习 CNN 时可能会感到难以理解。 卷积层前面的神经网络中我们使用了全连接层（Affine 层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。但是，全连接层牺牲掉了数据的形状。。比如，输 入数据是图像时，图像通常是高、长、通道方向上的 3 维形状。但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。 3 维的形状，其中可能包含由重要的空间信息。比如，空间上邻近的像素为相似的值、RBG 的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3 维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。 而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以 3 维数据的形式接收输入数据，并同样以 3 维数据的形式输出至下一层。因此， 在 CNN 中，可以（有可能）正确理解图像等具有形状的数据。 另外，CNN 中，有时将卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 卷积运算卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的滤波器运算。 将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。 填充在卷积运算中，我们经常会向输入数据的周围填入固定的数据（比如 0 等），这成为填充。填充通常用于保持数据在卷积运算后仍然保持空间大小不变的将数据传给下一层。 步幅应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是 1，如果将步幅设为 2，则情况如下图所示。 综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。这里，假设输入大小为 (H, W)，滤波器大小为 (FH, FW)，输出大小为 (OH, OW)，填充为 P，步幅为 S。可以得到如下关系。 三维数据的卷积计算3 维数据和 2 维数据时相比，可以发现纵深方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 需要注意的是，在 3 维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。在这个例子中，输入数据和滤波器的通道数一致，均为 3。 滤波器大小可以设定为任意值（不过，每个通道的滤波器大小要全部相同）。 通过应用 FN 个滤波器，输出特征图也生成了 FN 个。如果将这 FN 个特征图汇集在一起，就得到了形状为 (FN, OH, OW) 的方块。将这个方块传给下一层，就是 CNN 的处理流。 这里需要注意的是，作为 4 维数据，滤波器的权重数据要按 (output_channel, input_ channel, height, width) 的顺序书写。比如，通道数为 3、大小为 5 × 5 的滤波器有 20 个时，可以写成 (20, 3, 5, 5)。 3 维的卷积运算也存在偏置项，让我们来看看添加偏置的情况。 批处理让我们来看看 3 维卷积计算的批处理示意图。 池化层池化是缩小高、长方向上的空间的运算。比如：Max 池化如下图所示。 除了 Max 池化之外，还有 Average 池化等。相对于 Max 池化是从目标区域中取出最大值，Average 池化则是计算目标区域的平均值。在图像识别领域，主要使用 Max 池化（默认）。 代码实现在实现具体的代码之前，让我们来看看计算的过程的转化图示。 im2col 会考虑滤波器大小、步幅、填充，将输入数据展开为 2 维数组。 123456789101112131415import sys, ossys.path.append(os.pardir)from common.util import im2colx1 = np.random.rand(1, 3, 7, 7)## im2col (input_data, filter_h, filter_w, stride=1, pad=0)# input_data 由（数据量，通道，高，长）的 4 维数组构成的输入数据# filter_h 滤波器的高 filter_w 滤波器的长# stride 步幅 pad 填充col1 = im2col(x1, 5, 5, stride=1, pad=0)# batch = 1print(col1.shape) # (9, 75)x2 = np.random.rand(10, 3, 7, 7)# batch = 10col2 = im2col(x2, 5, 5, stride=1, pad=0)print(col2.shape) # (90, 75) 了解完 im2col，让我们利用它来实现卷积层。 123456789101112131415161718class Convolution: def __init__(self, W, b, stride=1, pad=0): self.W = W self.b = b self.stride = stride self.pad = pad def forward(self, x): FN, C, FH, FW = self.W.shape N, C, H, W = x.shape out_h = int(1 + (H + 2*self.pad - FH) / self.stride) out_w = int(1 + (W + 2*self.pad - FW) / self.stride) col = im2col(x, FH, FW, self.stride, self.pad) col_W = self.W.reshape(FN, -1).T # 滤波器的展开 (10, 3, 5, 5) 👉 (10, 75) out = np.dot(col, col_W) + self.b out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) return out","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"父与子的编程之旅","slug":"Python/父与子编程之旅","date":"2020-07-31T22:08:14.000Z","updated":"2020-08-04T14:29:36.111Z","comments":true,"path":"2020/08/01/Python/父与子编程之旅/","link":"","permalink":"http://yoursite.com/2020/08/01/Python/%E7%88%B6%E4%B8%8E%E5%AD%90%E7%BC%96%E7%A8%8B%E4%B9%8B%E6%97%85/","excerpt":"《父与子的编程之旅：与小卡特一起学 Python》：如何使用 Python 来告诉计算机要做什么？","text":"《父与子的编程之旅：与小卡特一起学 Python》：如何使用 Python 来告诉计算机要做什么？ 基础知识点斜杠与反斜杠1234567# Python 2print 3 / 2 # 1print 3.0 / 2 # 1.5# Python 3print 3 // 2 # 1printt 3 / 2 # 1.5 \\ 表示本行代码还未结束，新起一行继续代码。 次方12print 3 * 3 * 3 * 3 * 3 # 243print 3 ** 5 # 指数 e 记法3.8E16 或者 3.8e16 👉 3.8×1016。 123c = 2.6e75d = 1.2e74print c + d # 2.72e+75 改变类型Python 实际上并没有把一个东西从一种类型“转换”成另一种类型。它只是由原来的东西创建一个新东西，而且这个新东西正是你想要的类型。 float()、int() 和 str() 后面有小括号，因为它们不是 Python 关键字（如 print）——它们只是 Python 的内置函数（function）。 type() 函数查看类型。 输入输出字符串类型raw_input() 函数从用户那里得到一个字符串。正常情况下会从键盘得到这个输入，也就是说，用户要键入输入。在 Python 3 中，raw_input() 改名为 input()了。它与 Python 2 中的 raw_input() 完全一样。 1234print \"Enter your name: \", # 逗号表示在同一行键入答案someName = raw_input()# Enter your name: WarrensomeName = raw_input (\"Enter your name: \") 在 Python 3 中，通过在行尾添加逗号使得打印的内容都在同一行的方法不再有效。而且，在 Python 3 中使用 print() 时，要打印的内容必须被包含在一对括号中。 12345# Python 3print( \"My\" , end=\" \")print( \"name\" , end=\" \")print( \"is\" , end=\" \")print( \"Dave.\" , end=\" \") 数字类型12345temp_string = raw_input()float_type = float(temp_string)# 更简练的写法float_type = float(raw_input())int_type = int(raw_input()) 得到数字输入还有一种方法。Python 2 有一个名叫 input() 的函数，可以直接提供一个数，所以不必使用 int() 或 float() 来转换。 注意，Python 3 中去除了 input() 函数（可以直接获取数字而不需要进行转换），只有 raw_input()。更令人感到混乱的是，Python 2 中的 raw_input() 在 Python 3 中改名为 input()，但功能不变，仍然只会得到字符串。因为我们很清楚怎样从一个字符串创建一个数，所以建议使用 raw_input()，而不要用 Python 2 中的 input()。 网络数据1234import urllib2file = urllib2.urlopen('http://helloworldbook2.com/data/message.txt')message = file.read()print message 条件与循环条件1234567if condition01: # conditionA and / or conditionB result01elif condition02: # not conditionC result02# ...else resultN 计数循环12345678for looper in [1, 2, 3, 4, 5]: # 循环值列表，等价于 rang(1, 6) 此处的 looper 为循环变量名 print looper# Python 2print range(1, 5) # [1, 2, 3, 4]# Python 3print range(1, 5) # range(1, 5)# range(5) 👉range(0, 5)# range(10, 0, -2) 按步长计数 在 Python 3 中 range() 函数不会提供一个数字列表，而是会提供一个“可迭代”（iterable）的东西，你可以使用循环来遍历它。如果在 for 循环中使用 range()，则其工作方式是完全一样的，只是内部机制略有不同而已。 12for letter in \"Hi there\": # 符串就像一个字符列表 print letter 条件循环12while someInput == 3: print \"This is a 3\" 列表有时候可以把一堆东西存储在一起，放在某种“组”或者“集合”中，这样一来，就可以一次对整个集合做某些处理，也能更容易地记录一组东西。有一类集合叫做列表（list），另一类叫做字典（dictionary）。 列表添加1234567family = ['Mom', 'Dad', 'Junior', 'Baby'] # 列表family.append('Cat') # 向列表末尾添加一个元素print family[0] # Mom 从列表获取元素print family[1:3] # ['Dad', 'Junior', 'Baby'] 分片 👉 列表print family[:3] # ['Mom', 'Dad', 'Junior', 'Baby']print family[1:] # ['Dad', 'Junior', 'Baby', 'Cat']family.insert(2, 'Dog') # 向列表添加元素 ['Mom', 'Dad', 'Dog', Junior', 'Baby', 'Cat'] 列表可以包含 Python 能存储的任何类型的数据，这包括数字、字符串、对象，甚至可以包含其他列表。并不要求列表中的元素是同种类型或同一种东西。 123letters = ['a', 'b', 'c', 'd', 'e']letters.append(['f', 'g', 'h']) # ['a', 'b', 'c', 'd', 'e', ['f', 'g', 'h']]letters.extend(['f', 'g', 'h']) # ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] 列表删除123456letters = ['a', 'b', 'c', 'd', 'e']letters.remove('c') # 无需知道元素的具体位置del letters[2] # 允许通过索引删除元素del pop[2] # 允许通过索引删除元素 pop() 默认弹出列表最后一个元素print letters # ['a', 'b', 'd', 'e']letters = ['a', 'b', 'c', 'd', 'e'] 列表查找1234if 'a' in letters: print \"found 'a' in letters\"else: print \"didn't find 'a' in letters\" 列表排序12345678letters = ['d', 'a', 'e', 'c', 'b']print letters # ['d', 'a', 'e', 'c', 'b']letters.sort() # 改变原始列表，而不是创建一个新的有序列表，正序，可用分片 [:] 创建副本# new_list = original_list 这种写法是无法创建副本的，只是给原列表加多了一个标签# print letters.sort() 为错误用法：Noneprint letters # ['a', 'b', 'c', 'd', 'e']letters.reverse() # 逆序 letters.sort (reverse = True)print letters # ['e', 'd', 'c', 'b', 'a'] 元组元组（tuple）属于不可改变的列表。 1my_tuple = (\"red\", \"green\", \"blue\") 数据表数据表本质上就是一个双重列表。 12classMarks = [ [55,63,77,81], [65,61,67,72], [97,95,92,88] ]print classMarks[0][2] # 77 字典Python 字典（dictionary）是一种将两个东西关联在一起的方式。被关联在一起的两个东西分别称为键（key）和值（value）。字典中的每个项（item）或条目（entry）都有一个键和一个值，它们合起来被称为键值对（key-value pair）。一个字典就是一些键值对的集合。 123456789phoneNumbers = &#123;&#125;phoneNumbers[\"John\"] = \"555-1234\" # 方式一phoneNumbers = &#123;\"John\": \"555-1234\"&#125; # 方式二print phoneNumbers[\"John\"] # '555-1234' 列表通过缩索引访问，字典通过键值访问phoneNumbers.keys() # 列出所有键，返回一个列表phoneNumbers.values() # 列出所有值，返回一个列表del phoneNumbers[\"John\"] # 删除条目phoneNumbers.clear() # 清空条目\"Bob\" in phoneNumbers # False 确认键值是否存在 和列表一样，字典中的条目也可以是任意类型，包括简单类型（整数、浮点数、字符串）和集合（列表、字典）以及复合类型（对象）。但事实上，这句话并不完全对。字典的键只可以使用不可变类型（布尔、整数、浮点数、字符串和元组）。你不能使用一个列表或者字典来当作键，因为它们是可变类型。 字典和列表有一个不同之处，就是字典是无序的。但让字典有序输出还是可以的。 12for key in sorted(phoneNumbers.keys()): print key, phoneNumbers[key] 高级部分函数最简单地讲，函数就是可以完成某个工作的代码块。这是可以用来构建更大程序的一个小部分。可以把这个小部分与其他部分放在一起，就像用积木搭房子一样。 1234567def printMyAddress(myName, myPhoneNum): # 创建或定义函数要使用 Python 的 def 关键字 print myName print myPhoneNum print \"123 Main Street\" printprintMyAddress(\"wingo\", \"0123456789\") # 函数调用print \"Done the function\" 虽然可以向函数传毒 N 个函数，但是如果你的函数有超过 5 到 6 个参数，可能就应该考虑采用别的做法了。一种做法是把所有参数收集到一个列表中，然后把这个列表传递到函数。这样一来，就只是传递一个变量（列表变量），只不过其中包含有一组值。这样可以让你的代码更易读。 1234def calculateTax(price, tax_rate): taxTotal = price + (price * tax_rate) return taxTotal # 返回值totalPrice = calculateTax(7.99, 0.06) # 函数调用，返回值接收 全局变量与局部变量：若试图在函数内部改变全局变量，你会得到一个新的局部变量，全局变量并不会被改变。若要在函数内部强制使用全局变量，可以使用 global 关键字。 12def calculateTax(price, tax rate): global my_price # 告诉 Python 你想使用全局版本的 my_price 对象在 Python 中，一个对象的特征称为属性（attribute），动作称为方法（method）。 对象 = 属性 + 方法 12345class Ball: # 告诉 Python 建立一个类 def bounce(self): # 定义一个方法 if self.direction = \"down\": self.direction = \"up\" 以上的代码是一个球的类定义，其中只有一个方法 bounce()。不过，属性呢？嗯，属性并不属于类，它们属于各个实例。因为每个实例可以有不同的属性（可以用初始化方法设置）。 12345myBall = Ball() # 创建一个实例对象myBall.direction = \"down\"myBall.color = \"green\"myBall.size = \"small\" 初始化创建类定义时，可以定义一个特定的方法，名为 __init__()，只要创建这个类的一个新实例，就会运行这个方法。可以向 __init__() 方法传递参数，这样创建实例时就会把属性设置为你希望的值。 12345678910class Ball: def __init__ (self, color, size, direction) # 创建对象时的初始化方法 self.color = color self.size = size self.direction = direction def bounce(self): if self.direction = \"down\": self.direction = \"up\" 魔法方法Python 中的对象有一些“魔法”方法，当然它们并不是真的有魔法！这些只是在你创建类时 Python 自动包含的一些方法。Python 程序员通常把它们叫做特殊方法（special method）。 12345678910class Ball: def __init__ (self, color, size, direction) # 创建对象时的初始化方法 魔法方法 self.color = color self.size = size self.direction = direction def __str__ (self): # 自定义 print myBall 的具体输出内容 msg =\"Hi, I'm a \" self.size + \" \" + self.color + \" ball!\" return msg 多态和继承多态：同一个方法的不同行为。 继承：向父类学习。 可以使用 pass 关键字作为一个占位符建立一个代码桩，使待定义的方法能够通过测试。 模块可以被不同的程序使用，提取一些通用的功能便于使用。 12345678910## 定义模块# this is the file \"my_module.py\"# we're going to use it in another programdef c_to_f(celsius): fahrenheit = celsius * 9.0 / 5 + 32 return fahrenheit## 引入模块使用import my_modulefahrenheit c_to_f(celsius) # my_module.c_to_f(celsius) 标准模块Python 提供了大量标准模块，可以用来完成很多工作，比如查找文件、报时（或计时）、生成随机数，以及很多其他功能。这称为 Python 标准库。 123456789import time time.sleep(2)sleep(2) # 报错## 睡眠from time import sleep # 命名空间sleep(2)## 随机数import randomprint random.randint(0, 100) # 4 打印格式化12345678910111213141516171819202122## 换行符 \\nprint \"Hello \\nWorld\" # python# Hello## World## 制表符 \\t ：每 8 个字符之后有一个制表点（tab stop）。从下一个制表点开始print 'ABC\\tXYZ' # ABC XYZ## 转义字符 \\print 'hi\\\\there' # hi\\there## 在字符串中插入变量name = 'Warren Sande'print 'My name is %s and I wrote this book' % name # %s、%f 和 %i 都称为格式字符串# %.2f 浮点数四舍五入到两位小数number = -98.76print '% .2f' % number # 此处的格式为数字前空一格，方便正负数对齐 -98.76# %e 格式字符串打印 E 记法。它总是打印 6 位小数，除非你另作要求。print '%.8e' % number # 1.23456000e+01 保留八位，不足的用 0 填充# Python 自动选择浮点数记法或 E 记法，可以使用 %g 格式字符串# 当有格式字符串时，打印 % 使用 %% 转义# 多个格式字符串print 'I got %.1f in math and %.1f in science' % (math, science)print 'I got &#123;0:.1f&#125; in math, &#123;1:.1f&#125; in science'.format(math, science) # 2.6 以后的新方法 字符串分解字符串123name_string = 'Sam,Brad,Alex,Cameron,Toby,Gwen,Jenn,Connor'names = name_string.split(',') # 根据需要填写分解标记# ['Sam','Brad','Alex','Cameron','Toby','Gwen','Jenn','Connor'] 联接字符串12word_list = ['My', 'name', 'is', 'Warren']long_string = ' '.join(word_list) # 'My name is Warren' 搜索字符串12345678# startwithif name.startswith(\"Frank\"): print \"Can I call you Frank?\"# inaddr1 = '657 Maple Lane'if 'Maple' in addr: position = addr.index('Maple') # 获取索引 4 print \"found 'Maple' at index\", position 删除部分字符串123name = 'Warren Sande'short_name = name.strip('de') # 如果没有告诉 strip() 要剥除哪一部分，它就会去除所有空白符short_name # 'Warren San' 改变大小写123stringA = \"Hello\"stringB = stringA.lower() # hellostringC = stringA.upper() # HELLO 文件操作读12345678## 读文本文件my_file = open('notes.txt', 'r')lines = my_file.readline() # 按行读取，并记住目前在什么位置my_file.seek(0) # 回到第 0 行lines = my_file.readlines() # 读取所有行my_file.close() # 处理完文件一定要关闭，否则此文件将一直被占用## 读取二进制文件my_music_file = open('bg_music.mp3', 'rb') # 为文件模式增加一个 b 来打开二进制文件 写1234567891011## 写文件：追加模式new_file = open(\"my_new_notes.txt\", 'a')new_file.write(\"Eat supper\\n\")# 写文件：替换模式new_file = open(\"my_new_notes.txt\", 'w')new_file.write(\"Eat supper\\n\")new_file.close()# 使用 print 写文件my_file = open(\"new_file.txt\", 'w')print &gt;&gt; my_file, \"Hello there, neighbor!\"my_file.close() 保存12345678910import picklemy_list = ['Fred', 73, 'Hello there', 81.9876e-13]pickle_file = open('my_pickled_list.pk', 'w')pickle.dump(my_list, pickle_file) # 用 dump() 把列表“倒”在 pickle 文件中pickle_file.close()## 还原pickle_file = open('my_pickled_list.pk', 'r')recovered_list = pickle.load(pickle_file) # ['Fred', 73, 'Hello there', 8.1987599999999997e-012]pickle_file.close()","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"Spring 实战学习笔记","slug":"后台技术/Spring 实战学习笔记","date":"2020-05-20T07:12:26.000Z","updated":"2020-07-10T04:07:13.921Z","comments":true,"path":"2020/05/20/后台技术/Spring 实战学习笔记/","link":"","permalink":"http://yoursite.com/2020/05/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20%E5%AE%9E%E6%88%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"《Spring 实战》阅读笔记。","text":"《Spring 实战》阅读笔记。 bean 的装配在 XML 中进行显式配置；在 Java 中进行显式配置；隐式的 bean 发现机制和自动装配。 自动化装配 bean组件扫描 component scanningSpring 会自动发现应用上下文中所创建的 bean；自动装配 autowiringSpring 自动满足 bean 之间的依赖。 @Component @Named注解表明该类会作为组件类并告知 Spring 要为这个类创建 bean。 @ComponentSacn(basePackages={...}) &lt;context:component-scan base-package=&quot;...&quot; /&gt;注解启动了组件扫描。 @Configuration 注解表明这个类是一个配置类，该类应该包含如何在 Spring 应用上下文中创建 bean 的细节。 1234@Configuration@ComponentScan // 默认扫描同包类，可指定 public class MyConfig &#123;&#125; @Autowired(required=&quot;T/F&quot;) @Inject注解实现自动装配。（在 Spring 应用上下文中寻找匹配某个 bean 需求的其他 bean） 可以用在类的任何方法上，不管是构造器、Setter 方法还是其他的方法 Spring 都会尝试满足方法参数上所声明的依赖。假如有且只有一个 bean 匹配依赖需求的话那么这个 bean 将会被装配进来。如果没有匹配的 bean 那么在应用上下文创建的时候 Spring 会抛出一个异常。为了避免异常的出现你可以将@Autowired的required属性设置为false。如果有多个bean都能满足依赖关系的话Spring将会抛出一个异常表明没有明确指定要选择哪个bean进行自动装配。 @Bean注解会告诉 Spring 这个方法将会返回一个对象该对象要注册为 Spring 应用上下文中的 bean。方法体中包含了最终产生 bean 实例的逻辑。 XML 装配 bean 对强依赖使用构造器注入而对可选性的依赖使用属性注入。 12345678910111213141516171819202122232425262728293031&lt;bean id=\"\" class=\"\"&gt; &lt;!-- 构造器注入 --&gt; &lt;constructor-arg ref=\"...\" /&gt; &lt;!-- 构造器参数名 --&gt; &lt;c:[name]-ref=\"...\" /&gt; &lt;!-- 参数索引 --&gt; &lt;c:_0-ref=\"...\" /&gt; &lt;!-- 只有一个索引的省略形式 --&gt; &lt;c:_-ref=\"...\" /&gt; &lt;!-- 字面量装配 --&gt; &lt;constructor-arg value=\"...\" /&gt; &lt;c:_[name]=\"...\" /&gt; &lt;c:_0=\"...\" /&gt; &lt;c:_=\"...\" /&gt; &lt;!-- list / set --&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;ref bean=\"\" /&gt; &lt;value&gt;\"...\"&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;!-- 属性注入（用法同上） --&gt; &lt;property name=\"...\" ref=\"...\" /&gt; &lt;!-- 属性名 --&gt; &lt;p:[name]-ref=\"...\" /&gt;&lt;/bean&gt;&lt;!-- util 命名空间 --&gt;&lt;util:list id=\"...\"&gt; &lt;value&gt;\"...\"&lt;/value&gt;&lt;/util:list&gt; 配置的引用@ComponentScan &lt;context:component-scan&gt; @Import({MyConfig.class, ...}) &lt;bean class=&quot;MyConfig.class&quot; /&gt; @ImportResource(“classpath:my-config.xml”) &lt;import resource=&quot;my-config.xml&quot; /&gt; 处理歧义性将可选 bean 中的某一个设为首选 primary 的 bean 或者使用限定符 qualifier 来帮助 Spring 将可选的 bean 的范围缩小到只有一个 bean。 @Primary &lt;bean id=&quot;...&quot; class=&quot;...&quot; primary=&quot;true&quot; /&gt;注解声明为首选的 bean。 @Qualifier(&quot;[Name]&quot;)注解与@Autowired和@Inject协同使用在注入的时候指定想要注入进去的是哪个 bean。注解与@Component和@Bean协同使用创建自定义限定符。 bean 的 id 发生改变则装配失败？创建自定义限定符可以解决此问题。 可以创建自定义的限定符注解，解决 Java 不允许在同一个条目上重复出现相同类型的多个注解的规则。 1234@Target(&#123;ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifierpublic @interface Wingo &#123;&#125; bean 的作用域在默认情况下 Spring 应用上下文中所有 bean 都是作为以单例 singleton 的形式创建的。也就是说不管给定的一个 bean 被注入到其他 bean 多少次每次所注入的都是同一个实例。 单例 Singleton 在整个应用中只创建 bean 的一个实例； 原型 Prototype 每次注入或者通过 Spring 应用上下文获取的时候都会创建一个新的 bean 实例； 会话 Session 在 Web 应用中为每个会话创建一个 bean 实例； 请求 Rquest 在 Web 应用中为每个请求创建一个 bean 实例。 @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) &lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;prototype&quot; /&gt; 在典型的电子商务应用中可能会有一个 bean 代表用户的购物车。如果购物车是单例的话那么将会导致所有的用户都会向同一个购物车中添加商品。另一方面如果购物车是原型作用域的那么在应用中某一个地方往购物车中添加商品在应用的另外一个地方可能就不可用了因为在这里注入的是另外一个原型作用域的购物车。 就购物车 bean 来说会话作用域是最为合适的因为它与给定的用户关联性最大。要指定会话作用域我们可以使用@Scope注解它的使用方式与指定原型作用域是相同的。 12345@Component@Scope&#123;value=WebApplication.SCOPE_SESSION, proxyMode=ScopedProxyMode.INTERFACES&#125;public ShoppingCart cart()&#123; // ??? 这里有点不理解，咋就把一个方法申明成一个 Component 了，然后代理还是用的 INTERFACE&#125; 12345&lt;bean id=\"...\" class=\"...\" scope=\"prototype\" &gt; &lt;!-- 默认情况下它会使用 CGLib 创建目标类的代理 --&gt; &lt;!-- 设置为 false 则生成基于接口的代理 --&gt; &lt;aop:scope-proxy proxy-target-class=\"false \"/&gt;&lt;/bean&gt; 要注意的是@Scope同时还有一个proxyMode属性它被设置成了ScopedProxyMode.INTERFACES。这个属性解决了将会话或请求作用域的bean注入到单例bean中所遇到的问题。 因为StoreService是一个单例的bean会在Spring应用上下文加载的时候创建。当它创建的时候Spring会试图将ShoppingCart bean注入到setShoppingCart()方法中。但是ShoppingCartbean是会话作用域的此时并不存在。直到某个用户进入系统创建了会话之后才会出现ShoppingCart实例。 另外系统中将会有多个ShoppingCart实例每个用户一个。我们并不想让 Spring 注入某个固定的ShoppingCart实例到StoreService中。我们希望的是当StoreService处理购物车功能时它所使用的ShoppingCart实例恰好是当前会话所对应的那一个。 请求作用域的 bean 会面临相同的装配问题。因此请求作用域的 bean 应该也以作用域代理的方式进行注入。 Spring 并不会将实际的ShoppingCart bean 注入到 StoreService 中，Spring 会注入一个到ShoppingCart bean 的代理。这个代理会暴露与ShoppingCart相同的方法所以StoreService会认为它就是一个购物车。但是当StoreService调用ShoppingCart的方法时代理会对其进行懒解析并将调用委托给会话作用域内真正的ShoppingCart bean。 如果ShoppingCart是接口而不是类的话这是可以的也是最为理想的代理模式。但如果ShoppingCart是一个具体的类的话 Spring 就没有办法创建基于接口的代理了。此时它必须使用 CGLib 来生成基于类的代理。所以如果 bean 类型是具体类的话我们必须要将proxyMode属性设置为ScopedProxyMode.TARGET_CLASS以此来表明要以生成目标类扩展的方式创建代理。 运行时值注入 属性占位符 Property placeholder； Spring 表达式语言 SpEL。 @PropertySource(&quot;classpath:app.properties&quot;)注解声明属性源并通过 Spring 的Environment来检索属性。 @Value(&quot;${xxx.xxx}&quot;)注解配合解析属性占位符使用。 为了使用占位符我们必须要配置一个PropertySourcesPlaceholderConfigurer bean。 1234@Beanpublic static PropertysourcesPlaceholderConfigurer placeholderConfigurer() &#123; return new PropertySourcesPlaceholderConfigurer();&#125; 123&lt;beans&gt; &lt;context:property-placeholder /&gt;&lt;/beans&gt; 解析外部属性能够将值的处理推迟到运行时但是它的关注点在于根据名称解析来自于 Spring Environment和属性源的属性。而 Spring 表达式语言提供了一种更通用的方式在运行时计算所要注入的值。 Spring 3 引入了 Spring 表达式语言 Spring Expression Language，SpEL 它能够以一种强大和简洁的方式将值装配到 bean 属性和构造器参数中，在这个过程中所使用的表达式会在运行时计算得到值。 在 XML 配置中你可以将 SpEL 表达式传入&lt;property&gt;或```的 value 属性中或者将其作为 p- 命名空间或 c- 命名空间条目的值。 #{} @Value(&quot;#{ststemProperties[&#39;xxx.xxx&#39;]}&quot;) 使用 bean 的 ID 来引用 bean； 调用方法和访问对象的属性； 对值进行算术、关系和逻辑运算； 正则表达式匹配； 集合操作。 环境与 Profile配置@profile(&quot;dev&quot;) 12345678&lt;beans&gt; &lt;beans profile=\"dev\"&gt; &lt;/beans&gt; &lt;beans profile=\"prod\"&gt; &lt;/beans&gt;&lt;/beans&gt; 激活@ActiveProfiles 123456789101112131415161718&lt;!-- web.xml --&gt;&lt;!-- 作为 Web 应用的上下文参数 --&gt;&lt;context-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 作为 DispatcherServlet 的初始化参数 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 条件化Spring 4 引入了一个新的@Conditional注解它可以用到带有@Bean注解的方法上。如果给定的条件计算结果为true就会创建这个 bean 否则的话这个 bean 会被忽略。 123456789101112131415public class MyConfig &#123; @Bean @Conditional(MyCondition.class) // 条件化地创建 bean public ConditionBean conditionBean() &#123; return new ConditonBean(); &#125; &#125;public class Mycondition implements Condition &#123; public boolean matches (ConditionContext ctxt, AnnotatedTypeMetadata metadata)&#123; // 具体判断逻辑 &#125;&#125; 面向切面 面向切面编程的基本原理 通过 POJO 创建切面 使用@AspectJ注解 为 AspectJ 切面注入依赖 Spring 只支持方法级别的连接点，如果需要方法拦截之外的连接点拦截功能那么我们可以利用 Aspect 来补充 Spring AOP 的功能。 描述切面的常用术语有通知 advice、切点 pointcut 和连接点 join point。 引入 Introdution允许向现有的类添加新方法或属性。 织入 Weaving织入是把切面应用到目标对象并创建新的代理对象的过程。 编译期切面在目标类编译时被织入。这种方式需要特殊的编译器。AspectJ 的织入编译器就是以这种方式织入切面的。 类加载期切面在目标类加载到 JVM 时被织入。这种方式需要特殊的类加载器ClassLoader它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ 5 的加载时织入 load-time weavingLTW 就支持以这种方式织入切面。 运行期切面在应用运行的某个时刻被织入。一般情况下在织入切面时 AOP 容器会为目标对象动态地创建一个代理对象。Spring AOP 就是以这种方式织入切面的。 Spring 切面可以应用 5 种类型的通知： 前置通知 Before 在目标方法被调用之前调用通知功能； 后置通知 After 在目标方法完成之后调用通知此时不会关心方法的输出是什么； 返回通知 After-returning 在目标方法成功执行之后调用通知； 异常通知 After-throwing 在目标方法抛出异常后调用通知； 环绕通知Around通知包裹了被通知的方法在被通知的方法调用之前和调用之后执行自定义的行为。 代理类封装了目标类并拦截被通知方法的调用再把调用转发给真正的目标 bean。当代理拦截到方法调用时在调用目标 bean 方法之前会执行切面逻辑。 定义切面Spring 借助 AspectJ 的切点表达式语言来定义 Spring 切面。 AspectJ 指示器 描 述 arg() 限制连接点匹配参数为指定类型的执行方法 @args() 限制连接点匹配参数由指定注解标注的执行方法 execution() 用于匹配是连接点的执行方法 this() 限制连接点匹配 AOP 代理的 bean 引用为指定类型的类 target 限制连接点匹配目标对象为指定类型的类 @target() 限制连接点匹配特定的执行对象这些对象对应的类要具有指定类型的注解 within() 限制连接点匹配指定的类型 @within() 限制连接点匹配指定注解所标注的类型当使用 Spring AOP 时方法定义在由指定的注解所标注的类里 @annotation 限定匹配带有指定注解的连接点 XML 中的切面声明。 AOP配置元素 用 途 &lt;aop:advisor&gt; 定义 AOP 通知器 &lt;aop:after&gt; 定义 AOP 后置通知不管被通知的方法是否执行成功 &lt;aop:after-returning&gt; 定义 AOP 返回通知 &lt;aop:after-throwing&gt; 定义 AOP 异常通知 &lt;aop:around&gt; 定义 AOP 环绕通知 &lt;aop:aspect&gt; 定义一个切面 &lt;aop:aspectj-autoproxy&gt; 启用 @AspectJ注解驱动的切面 &lt;aop:before&gt; 定义一个 AOP 前置通知 &lt;aop:config&gt; 顶层的 AOP 配置元素。大多数的 &lt;aop:*&gt;元素必须包含在&lt;aop:config&gt;元素内 &lt;aop:declare-parents&gt; 以透明的方式为被通知的对象引入额外的接口 &lt;aop:pointcut&gt; 定义一个切点 为了阐述 Spring 中的切面我们需要有个主题来定义切面的切点。为此我们定义一个Performance接口。 1234package concert;public interface Performance &#123; public void perform();&#125; 通知Spring 使用 AspectJ 注解来声明通知方法。 注 解 通 知 @After 通知方法会在目标方法返回或抛出异常后调用 @AfterReturning 通知方法会在目标方法返回后调用 @AfterThrowing 通知方法会在目标方法抛出异常后调用 @Around 通知方法会将目标方法封装起来 @Before 通知方法会在目标方法调用之前执行 1234567891011@Aspectpublic class Audience &#123; @Pointcut(\"execution(* concert.Performance.perform(..))\") public void performance()&#123;&#125; @Before(\"performance()\") public void silenceCellPhones() &#123; System.out.println(\"Silencing cell phones\") &#125;&#125; 12345678910@Configuration@EnableAspectJAutoProxy // 启动 AspectJ 自动代理@ComponentScanpublic class ConcertConfig &#123; @Bean public Audience audience() &#123; // 声明 Audience bean return new Audience(); &#125;&#125; 123456789101112&lt;beans&gt; &lt;context:component-scan base-package=\"concert\" /&gt; &lt;aop:aspectj-autoproxy /&gt; &lt;bean id=\"audience\" class=\"concert.Audiance\" /&gt; &lt;aop:config&gt; &lt;aop:aspect ref=\"audience\"&gt; &lt;aop:pointcut id=\"performance\" expression=\"execution(* concert.Performance.perform(..))\" /&gt; &lt;aop:before pointcut-ref=\"performance\" method=\"silenceCellPhones\" /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 环绕通知环绕通知是最为强大的通知类型。它能够让你所编写的逻辑将被通知的目标方法完全包装起来。实际上就像在一个通知方法中同时编写前置通知和后置通知。 通知方法中可以做任何的事情，当要将控制权交给被通知的方法时它需要调用ProceedingJoinPoint的proceed()方法。 1234567891011@Around(\"performance()\")public void watchPerformance(ProceedingJoinPoint jp) try &#123; System.out.println(\"Silencing cell phones\"); System.out.println(\"Taking seats\"); jp.proceed(); // 不调这个方法的话那么你的通知实际上会阻塞对被通知方法的调用 System.out.println(\"CLAP CLAP CLAP!!!\"); &#125; catch (Throwable e) &#123; System.out.println(\"Demanding a refund\"); &#125;&#125; 123456&lt;aop:config&gt; &lt;aop:aspect ref=\"audience\"&gt; &lt;aop:pointcut id=\"performance\" expression=\"execution(* concert.Performance.perform(..))\" /&gt; &lt;aop:around pointcut-ref=\"performance\" method=\"watchPerformance\" /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 参数化通知，使用参数化的通知来记录磁道播放的次数。 引入如果切面能够为现有的方法增加额外的功能，为什么不能为一个对象增加新的方法呢？实际上利用被称为引入的 AOP 概念切面可以为 Spring bean 添加新方法。 Web 中的 SpringSpring 通常用来开发 Web 应用。 构建 Spring Web 映射请求到 Spring 控制器 透明地绑定表单参数 校验表单提交 使用 Spring MVC 所经历的所有站点。 请求旅程的第一站是 Spring 的DispatcherServlet。与大多数基于Java 的 Web 框架一样 Spring MVC 所有的请求都会通过一个前端控制器 front controllerServlet。前端控制器是常用的 Web 应用程序模式在这里一个单实例的 Servlet 将请求委托给应用程序的其他组件来执行实际的处理。在 Spring MVC 中DispatcherServlet就是前端控制器。 DispatcherServlet的任务是将请求发送给 Spring MVC 控制器 controller。控制器是一个用于处理请求的 Spring 组件。在典型的应用程序中可能会有多个控制器DispatcherServlet需要知道应该将请求发送给哪个控制器。所以DispatcherServlet会查询一个或多个处理器映射 handler mapping 来确定请求的下一站在哪里。处理器映射会根据请求所携带的 URL 信息来进行决策。 一旦选择了合适的控制器DispatcherServlet会将请求发送给选中的控制器。到了控制器请求会卸下其负载用户提交的信息并耐心等待控制器处理这些信息。实际上设计良好的控制器本身只处理很少甚至不处理工作而是将业务逻辑委托给一个或多个服务对象进行处理。 控制器在完成逻辑处理后通常会产生一些信息这些信息需要返回给用户并在浏览器上显示。这些信息被称为模型 model。不过仅仅给用户返回原始的信息是不够的——这些信息需要以用户友好的方式进行格式化一般会是 HTML。所以信息需要发送给一个视图 view 通常会是 JSP。 控制器所做的最后一件事就是将模型数据打包并且标示出用于渲染输出的视图名。它接下来会将请求连同模型和视图名发送回DispatcherServlet。 这样控制器就不会与特定的视图相耦合传递给DispatcherServlet的视图名并不直接表示某个特定的 JSP。实际上它甚至并不能确定视图就是JSP。相反它仅仅传递了一个逻辑名称，这个名字将会用来查找产生结果的真正视图。DispatcherServlet将会使用视图解析器 view resolver 来将逻辑视图名匹配为一个特定的视图实现，它可能是也可能不是 JSP。 既然DispatcherServlet已经知道由哪个视图渲染结果，那请求的任务基本上也就完成了。它的最后一站是视图的实现，可能是 JSP，在这里它交付模型数据。请求的任务就完成了。视图将使用模型数据渲染输出，这个输出会通过响应对象传递给客户端。 DispatcherServlet此 Servlet 为 Spring MVC 的核心类。 按照传统的方式像DispatcherServlet这样的 Servlet 会配置在 web.xml 文件中这个文件会放到应用的 WAR 包里面。当然这是配置DispatcherServlet的方法之一。但是借助于 Servlet 3 规范和 Spring 3.1 的功能增强可以使用 Java 将DispatcherServlet配置在 Servlet 容器中而不会再使用 web.xml 文件。 12345678910111213141516171819// 配置 DispatcherServletpackage spittr.config;import org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;public class SpittrWebAppInitializer extends AbstractAnnotationConfigDispactcherServletInitializer &#123; @Override protected String[] getServletMappings() &#123; // 将 DispatcherServlet 映射到 “/” return new String[] &#123;\"/\"&#125;; &#125; // 定义拦截器 ContextLoaderListener 应用上下文的 beans @Override protected class&lt;?&gt;[] getRootConfigClasses() &#123; return new class&lt;?&gt;[] &#123;RootConfig.class&#125;; &#125; // 定义 DispatcherServlet 应用上下文的 beans @Override protected class&lt;?&gt;[] getServletConfigClasses() &#123; // 指定配置类 return new class&lt;?&gt;[] &#123;WebConfig.class&#125;; &#125;&#125; 扩展AbstractAnnotation-ConfigDispatcherServletInitializer的任意类都会自动地配置Dispatcher-Servlet和 Spring 应用上下文，Spring 的应用上下文会位于应用程序的 Servlet 上下文之中。 最小但可用的 Spring MVC 配置。 12345678910111213141516171819@Configuration@EnableWebMvc // 启用 Spring MVC@ComponentScan(\"spittr.web\") // 开启组件扫描 @Controllerpublic class WebConfig extends WebMvcConfigurerAdapter &#123; @Bean public ViewResolver viewResolver() &#123; // 配置 JSP 视图解析器 InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); resolver.setExposeContextBeansAsAttributes(true); return resolver; &#125; @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); // 配置静态资源的处理 &#125;&#125; 12345678@Configuration@ComponentScan(basePackages=&#123;\"spitter\"&#125;, excludeFilter=&#123; @Filter&#123;type=FilterType.ANNOTATION, value=EnbaleWebMVC.class&#125; &#125;)public class RootConfig &#123;&#125; 添加其它组件除了DispatcherServlet以外，项目可能还需要额外的 Servlet 和 Filter ，并且可能还需要对DispatcherServlet本身做一些额外的配置。 在AbstractAnnotation-ConfigDispatcherServletInitializer将DispatcherServlet注册到Servlet容器中之后，就会调用customizeRegistration()并将Servlet注册后得到的Registration.Dynamic传递进来。通过重载customizeRegistration()方法我们可以对DispatcherServlet进行额外的配置。 如果我们想往 Web 容器中注册其他组件的话，只需创建一个新的初始化器就可以了。最简单的方式就是实现 Spring 的WebApplicationInitializer接口。 12345678910111213public class MyServletInitializer extends WebApplicationInitializer &#123; @Override public void onStartup(ServletContext servletContext) throws ServletException &#123; // 添加自定义 servlet Dynamic myServlet = servlectContext.addServlet(\"myServlet\", MyServlet.class); myServlect.addMapping(\"/custom/**\"); // 添加自定义 Filter javax.servlet.FilterRegistration.Dynamic filter = servletContext.addFilter(\"myFilter\", MyFilter.class); filter.addMappingForUrlPatterns(null, false, \"/custom/**\"); &#125;&#125; 源码片段： 1234// 寻找任何继承了 WebApplicationInitializer 接口的类并用其来配置 servlet 容器for (WebApplicationInitializer initializer : initializers) &#123; initializer.onStartup(servletContext);&#125; 编写基本控制器@Controller注解辅助实现组件扫描，与@Component注解所实现的效果是一样的，但是在表意性上可能会差一些。 @RequestMapping(value=&quot;/&quot;, method=GET)注解绑定请求：value属性指定了这个方法所要处理的请求路径，method属性细化了它所处理的 HTTP 方法。 控制器方法的 Model 参数实际上就是一个 Map，也就是 key-value 对的集合，它会传递给视图，这样数据就能渲染到客户端了。 12(Model model)(Map mode) 传参Spring MVC 允许以多种方式将客户端中的数据传送到控制器的处理器方法中。 查询参数 Query Parameter； 表单参数 Form Parameter； 路径变量 Path Variable。 @RequrearParam(&quot;ParamName&quot;) [URL]?ParamName=xxx注解用于参数。 @RequestMapping(value=&quot;/{ParamName}&quot;, method=GET) @PathVariable{&quot;ParamName&quot;} [URL]/xxx 表单 POST 请求的控制器方法参数可直接使用对象，Spring MVC 将会使用请求中同名的参数进行填充。 校验 从 Spring 3.0 开始在 Spring MVC 中提供了对 Java 校验 API 的支持。 Java 校验 API 所提供的校验注解。 注 解 描 述 @AssertFalse 所注解的元素必须是Boolean类型并且值为 false @AssertTrue 所注解的元素必须是Boolean类型并且值为 true @DecimalMax 所注解的元素必须是数字并且它的值要小于或等于给定的 BigDecimalString值 @DecimalMin 所注解的元素必须是数字并且它的值要大于或等于给定的 BigDecimalString值 @Digits 所注解的元素必须是数字并且它的值必须有指定的位数 @Future 所注解的元素的值必须是一个将来的日期 @Max 所注解的元素必须是数字并且它的值要小于或等于给定的值 @Min 所注解的元素必须是数字并且它的值要大于或等于给定的值 @NotNull 所注解元素的值必须不能为 null @Null 所注解元素的值必须为 null @Past 所注解的元素的值必须是一个已过去的日期 @Pattern 所注解的元素的值必须匹配给定的正则表达式 @Size 所注解的元素的值必须是 String、集合或数组并且它的长度要符合给定的范围 @valid注解用于参数，添加了@Valid注解这会告知Spring需要确保这个对象满足校验限制。 属性上添加校验限制并不能阻止表单提交，如果有校验出现错误的话那么这些错误可以通过Errors对象进行访问，Errors对象可作为控制器方法的一个参数。首次调用Errors.hasErrors()来检查是否有错误，之后再进行业务逻辑处理。 渲染 Web 视图 将模型数据渲染为 HTML 使用 JSP 视图 通过 tiles 定义视图布局 使用 Thymeleaf 视图 Spring 自带了 13 个视图解析器能够将逻辑视图名转换为物理实现。 视图解析器 描 述 BeanNameViewResolver 将视图解析为 Spring 应用上下文中的 bean 其中 bean 的 ID 与视图的名字相同 ContentNegotiatingViewResolver 通过考虑客户端需要的内容类型来解析视图委托给另外一个能够产生对应内容类 型的视图解析器 FreeMarkerViewResolver 将视图解析为 FreeMarker 模板 InternalResourceViewResolver 将视图解析为 Web 应用的内部资源一般为 JSP JasperReportsViewResolver 将视图解析为 JasperReports 定义 ResourceBundleViewResolver 将视图解析为资源 bundle 一般为属性文件 TilesViewResolver 将视图解析为 Apache Tile 定义其中 tile ID 与视图名称相同。注意有两个不同的 TilesViewResolver实现分别对应于 Tiles 2.0 和 Tiles 3.0 UrlBasedViewResolver 直接根据视图的名称解析视图视图的名称会匹配一个物理视图的定义 VelocityLayoutViewResolver 将视图解析为 Velocity 布局从不同的 Velocity 模板中组合页面 VelocityViewResolver 将视图解析为 Velocity 模板 XmlViewResolver 将视图解析为特定 XML 文件中的 bean 定义。类似于BeanName-ViewResolver XsltViewResolver 将视图解析为 XSLT 转换后的结果 配置适用于 JSP 的视图解析器InternalResourceViewResolver。它遵循一种约定会在视图名上添加前缀和后缀进而确定一个 Web 应用中视图资源的物理路径。 12345678@Beanpublic ViewResolver viewResolver() &#123; // 配置 JSP 视图解析器 InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); resolver.serViewClass(InternalResourceViewResolver) // 解析 JSTL 视图 return resolver;&#125; 1234&lt;bean id=\"viewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" p:prefix=\"/WEB-INF/views\" p:suffix=\".jsp\" p:viewClass=\"InternalResourceViewResolver\" /&gt; 解析 JSTL 视图，使用 Spring 的 JSP 库。 借助 Spring 表单绑定标签库中所包含的标签我们能够将模型对象绑定到渲染后的 HTML 表单中。 12&lt;!-- 为了使用表单绑定库需要在JSP页面中对其进行声明 --&gt;&lt;%@taglib prefix=\"sf\" uri=\"http://www.springframework.org/tags/form\" %&gt; JSP标签 描 述 &lt;sf:checkbox&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为checkbox &lt;sf:checkboxes&gt; 渲染成多个 HTML &lt;input&gt;标签其中type属性设置为checkbox &lt;sf:errors&gt; 在一个 HTML &lt;span&gt;中渲染输入域的错误 &lt;sf:form&gt; 渲染成一个 HTML &lt;form&gt;标签并为其内部标签暴露绑定路径用于数据绑定 &lt;sf:hidden&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为hidden &lt;sf:input&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为text &lt;sf:label&gt; 渲染成一个 HTML &lt;label&gt;标签 &lt;sf:option&gt; 渲染成一个 HTML &lt;option&gt;标签其selected属性根据所绑定的值进行设置 &lt;sf:options&gt; 按照绑定的集合、数组或 Map 渲染成一个HTML &lt;option&gt;标签的列表 &lt;sf:password&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为password &lt;sf:radiobutton&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为radio &lt;sf:radiobuttons&gt; 渲染成多个 HTML &lt;input&gt;标签其中type属性设置为radio &lt;sf:select&gt; 渲染为一个 HTML &lt;select&gt;标签 &lt;sf:textarea&gt; 渲染为一个 HTML &lt;textarea&gt;标签 ThymeLeafThymeleaf 模板是原生的,不依赖于标签库。它能在接受原始 HTML 的地方进行编辑和渲染。因为它没有与 Servlet 规范耦合，因此 Thymeleaf 模板能够进入 JSP 所无法涉足的领域。 ThymeleafViewResolver将逻辑视图名称解析为 Thymeleaf 模板视图； SpringTemplateEngine处理模板并渲染结果； TemplateResolver加载 Thymeleaf 模板。 12345678910111213141516171819202122232425// thymeleaf 视图解析器@Beanpublic ViewResolver viewResolver(TemplateEngine templateEngine)&#123; ThymeleafViewResolver viewResolver = new ThymeleafViewResolver(); viewResolver.setTemplateEngine(templateEngine); return viewResolver;&#125;// 模板引擎@Beanpublic TemplateEngine templateEngine(TemplateResolver templateResolver)&#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); return templateEngine;&#125;// 模板解析器@Beanpublic ITemplateResolver templateResolver()&#123; SpringResourceTemplateResolver templateResolver = new SpringResourceTemplateResolver(); templateResolver.setPrefix(\"/WEB-INF/templates/\"); templateResolver.setSuffix(\".html\"); templateResolver.setTemplateMode(\"HTML5\") return templateResolver;&#125; Thymeleaf 教程 文件上传一般表单提交所形成的请求结果是很简单的，就是以&amp;符分割的多个 name-value 对。 尽管这种编码形式很简单，并且对于典型的基于文本的表单提交也足够满足要求，但是对于传送二进制数据，如上传图片，就显得力不从心了。与之不同的是，multipart 格式的数据会将一个表单拆分为多个部分（part），每个部分对应一个输入域。在一般的表单输入域中，它所对应的部分中会放置文本型数据，但是如果上传文件的话，它所对应的部分可以是二进制。 在编写控制器方法处理文件上传之前我们必须要配置一个 multipart 解析器通过它来告诉DispatcherServlet该如何读取 multipart 请求。 1234@Beanpublic MultipartResolver multipartResolver() throws IOException &#123; return new StandardServletMultipartResolver();&#125; 在 Servlet registration 上调用setMultipartConfig()方法传入一个MultipartConfig-Element实例。 12345DispatcherServlet ds = new DispatchServlet();Dynamic registration = context.addServlet(\"appServlet\", ds);registration.addMapping(\"/\");// 将临时路径设置为 /tmp/spittr/uploadsregistration.setMultipartConfig(new MultipartConfigElement(\"/tmp/spittr/uploads\")); 如果我们配置DispatcherServlet的Servlet初始化类继承了Abstract AnnotationConfigDispatcherServletInitializer或AbstractDispatcher-ServletInitializer的话那么我们不会直接创建DispatcherServlet实例并将其注册到 Servlet 上下文中。这样的话将不会有对Dynamic Servlet registration 的引用供我们使用了。但是我们可以通过重载customizeRegistration()方法它会得到一个Dynamic作为参数来配置 multipart 的具体细节。 123456@Overrideprotected void customizeRegistration(Dynamic registration) &#123; registration.setMultipartConfig( new MultipartConfigElement(\"/tmp/spittr/uploads\", 2097152, 4194304, 0); );&#125; 123456789101112&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatchServlet &lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;multipart-config&gt; &lt;location&gt;/tmp/spittr/upload&lt;/location&gt; &lt;max-file-size&gt;2097152&lt;/max-file-size&gt; &lt;max-request-size&gt;4194304&lt;/max-request-size&gt; &lt;/multipart-config&gt;&lt;/servlet&gt; 对于非 Servlet 3.0 环境，Spring 内置了 CommonsMultipartResolver，可以作为 StandardServletMultipartResolver 的替代方案。 @RequestPart注解指定用于接收请求中对应 part 的数组。 1234&lt;form method=\"POST\" th:object=\"$&#123;spitter&#125;\" enctype=\"multipart/form-data\"&gt; &lt;label&gt;Profile Picture&lt;/label&gt;: &lt;input type=\"file\" name=\"profilePicture\" accept=\"image/jpeg,image/png,image/gif\" /&gt;&lt;br/&gt;&lt;/form&gt; 1234567@RequestMapping(value=\"/register\", method=POST)public String processRegistration( @RequestPart(\"profilePicture\") byte[] profilePicture, @Valid Spittr spittr, Errors errors) &#123; // 将文件保存到某个位置&#125; 使用上传文件的原始byte比较简单但是功能有限。因此 Spring 还提供了MultipartFile接口它为处理 multipart 数据提供了内容更为丰富的对象。 12345678910111213141516package org.springframework.web.multipart;import java.io.File;import java.io.IOException;import java.io.InputStream;public interface MultipartFile &#123; String getName(); String getOriginalFilename(); String getContentType(); boolean isEmpty(); long getSize(); byte[] getBytes() throws IOException; InputStream getInputStream() throws IOException; void transferTo(File dest) throws IOException;&#125; 12// 将上传的文件写入到文件系统中的便捷方法profilePicture.tranferTo(new File(\"/data/spittr/\" + profilePicture.getOriginalFilename())); 如将应用部署到 Servlet 3.0 的容器中，那么会有MultipartFile的一个替代方案。Spring MVC 也能接受javax.servlet.http.Part作为控制器方法的参数。 Part 接口与MultipartFile并没有太大的差别，Part 接口中的一些方法其实是与MultipartFile相对应的。 异常处理 特定的 Spring 异常将会自动映射为指定的 HTTP 状态码； 异常上可以添加@ResponseStatus注解从而将其映射为某一个 HTTP 状态码； 在方法上可以添加@ExceptionHandler注解使其用来处理异常。 Spring 的一些异常会默认映射为 HTTP 状态码。 Spring异常 HTTP状态码 BindException 400 - Bad Request ConversionNotSupportedException 500 - Internal Server Error HttpMediaTypeNotAcceptableException 406 - Not Acceptable HttpMediaTypeNotSupportedException 415 - Unsupported Media Type HttpMessageNotReadableException 400 - Bad Request HttpMessageNotWritableException 500 - Internal Server Error HttpRequestMethodNotSupportedException 405 - Method Not Allowed MethodArgumentNotValidException 400 - Bad Request MissingServletRequestParameterException 400 - Bad Request MissingServletRequestPartException 400 - Bad Request NoSuchRequestHandlingMethodException 404 - Not Found TypeMismatchException 400 - Bad Request 1234567891011@RequestMapping(value=\"/&#123;spittleId&#125;\", method=RequestMethod.GET)public String spittle( @PathVariable(\"spittleId\") long spittleId, Model model) &#123; Spittle spittle = spittleRepository.findOne(spittleId); if (spittle == null) &#123; // 若未找到此对象则抛出异常 throw new SpittleNotFoundException(); &#125; model.addAttribute(spittle); return \"spittle\";&#125; 12345678package spittr.web;import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.ResponseStatus;@ResponseStatus(value=HttpStatus.NOT_FOUND, reason=\"Spittle Not Found\")public class SpittleNotFoundException extends RuntimeException &#123;&#125; 在很多的场景下，将异常映射为状态码是很简单的方案，并且就功能来说也足够了。但是如果想在响应中不仅要包括状态码，还要包含所产生的错误，那该怎么办呢？此时的话，我们就不能将异常视为 HTTP 错误了，而是要按照处理请求的方式来处理异常了。 123456789101112// 在处理请求的方法中直接处理异常@RequestMapping(method=RequestMethod.POST)public String saveSpittle(SpittleForm form, Model model) &#123; try &#123; spittleRepository.save( new Spittle(null, form.getMessage(), new Date(), form.getLongitude(), form.getLatitude()) ); return \"redirect:/spittles\"; &#125; catch (DuplicateSpittleException e) &#123; return \"error/duplicate\"; &#125;&#125; 它运行起来没什么问题，但是这个方法有些复杂。该方法可以有两个路径，每个路径会有不同的输出。如果能让saveSpittle()方法只关注正确的路径，而让其他方法处理异常的话，那么它就能简单一些。 123456789101112@RequestMapping(method=RequestMethod.POST)public String saveSpittle(SpittleForm form, Model model) &#123; spittleRepository.save( new Spittle(null, form.getMessage(), new Date(), form.getLongitude(), form.getLatitude()) ); return \"redirect:/spittles\";&#125;@ExceptionHandler(DuplicateSpittleException.class)public String handleDuplicateSpittle() &#123; return \"error/duplicate\";&#125; 如果要在多个控制器中处理异常那@ExceptionHandler注解所标注的方法是很有用的。不过如果多个控制器类中都会抛出某个特定的异常那么你可能会发现要在所有的控制器方法中重复相同的@ExceptionHandler方法。或者为了避免重复我们会创建一个基础的控制器类所有控制器类要扩展这个类从而继承通用的@ExceptionHandler方法。 Spring 3.2 为这类问题引入了一个新的解决方案控制器通知。控制器通知 controller advice 是任意带有@ControllerAdvice注解的类这个类会包含一个或多个如下类型的方法 @ExceptionHandler注解标注的方法； @InitBinder注解标注的方法； @ModelAttribute注解标注的方法。 1234567@ControllerAdvicepublic class AppWideExceptionHandler &#123; @ExceptionHandler(DuplicateSpittleException.class) public String handleNotFound() &#123; return \"error/duplicate\"; &#125;&#125; 重定向在处理完 POST 请求后通常来讲一个最佳实践就是执行一下重定向。除了其他的一些因素外，这样做能够防止用户点击浏览器的刷新按钮或后退箭头时客户端重新执行危险的 POST 请求。 当控制器方法返回的String值以“redirect:”开头的话那么这个String不是用来查找视图的而是用来指导浏览器进行重定向的路径。 1return &quot;redirect:&#x2F;spitter&#x2F;&quot; + spitter.getUsername(); 模型的属性是以请求属性的形式存放在请求中的在重定向后无法存活。 使用 URL 模板以路径变量和 / 或查询参数的形式传递数据； 通过 flash 属性发送数据。 123456@RequestMapping(value=\"/register\", method=POST);public String processRegistration(Spitter spitter, Model model) &#123; spitterRepository.save(spitter); model.addAttribute(\"username\", spitter.getUsername()); return \"redirect:/spitter/&#123;username&#125;\";&#125; 除此之外，模型中所有其他的原始类型值都可以添加到 URL 中作为查询参数。 1234567@RequestMapping(value=\"/register\", method=POST)public String processRegistration(Spitter spitter, Model model) &#123; spitterRepository.save(spitter); model.addAttribute(\"username\", spitter.getUsername()); model.addAttribute(\"spitterId\", spitter.getId()); // 会以查询参数的形式进行重定向 return \"redirect:/spitter/&#123;username&#125;\";&#125; 如果 username 属性的值是 habuma 并且 spitterId 属性的值是 42，那么结果得到的重定向 URL 路径将会是 /spitter/habuma?spitterId=42。 在 URL 中，并没有办法发送更为复杂的值，但这正是flash属性能够提供帮助的领域。 Spring 提供了通过RedirectAttributes设置 flash 属性的方法，这是 Spring 3.1 引入的Model的一个子接口。RedirectAttributes提供了Model的所有功能除此之外还有几个方法是用来设置 flash 属性的。 1234567@RequestMapping(value=\"/register\", method=POST)public String processRegistration(Spitter spitter, RedirectAttribute model) &#123; spitterRespository.save(spitter); model.addAttribute(\"username\", spitter.getUsername()); model.addFlashAttribute(\"spitter\", spitter); // flash 属性 return \"redirect:/spitter/&#123;username&#125;\";&#125; 安全 Spring Security 介绍； 使用 Servlet 规范中的 Filter 保护Web应用； 基于数据库和 LDAP 进行认证。 Spring Security 是为基于 Spring 的应用程序提供声明式安全保护的安全性框架。Spring Security 提供了完整的安全性解决方案，能够在 Web 请求级别和方法调用级别处理身份认证和授权。因为基于 Spring 框架所以 Spring Security 充分利用了依赖注入 dependency injectionDI 和面向切面的技术。 Spring Security 从两个角度来解决安全性问题。它使用 Servlet 规范中的 Filter 保护 Web 请求并限制 URL 级别的访问。Spring Security 还能够使用 Spring AOP 保护方法调用——借助于对象代理和使用通知能够确保只有具备适当权限的用户才能访问安全保护的方法。 Spring Security 被分成了11个模块。 模 块 描 述 ACL 支持通过访问控制列表 access control list 为域对象提供安全性 Aspects 当使用 Spring Security 注解时，会使用基于 AspectJ 的切面，而不是使用标准的 Spring AOP CAS Client 提供与 Jasig 的中心认证服务 Central Authentication Service 进行集成的功能 Configuration 包含通过 XML 和 Java 配置 Spring Security 的功能支持 Core 提供 Spring Security 基本库 Cryptography 提供了加密和密码编码的功能 LDAP 支持基于 LDAP 进行认证 OpenID 支持使用 OpenID 进行集中式认证 Remoting 提供了对 Spring Remoting 的支持 Tag Library Spring Security 的 JSP 标签库 Web 提供了 Spring Security 基于 Filter 的 Web 安全性支持 Spring Security 借助一系列 Servlet Filter 来提供各种安全性功能。这是否意味着我们需要在 web.xml 或WebApplicationInitializer中配置多个 Filter 呢？实际上借助于 Spring 的小技巧我们只需配置一个 Filter 就可以了。 DelegatingFilterProxy是一个特殊的 Servlet Filter 它本身所做的工作并不多。只是将工作委托给一个javax.servlet.Filter实现类这个实现类作为一个&lt;bean&gt;注册在 Spring 应用的上下文。 不管我们通过 web.xml 还是通过AbstractSecurityWebApplicationInitializer的子类来配置DelegatingFilterProxy它都会拦截发往应用中的请求并将请求委托给ID为springSecurityFilterChain bean。 springSecurityFilterChain本身是另一个特殊的 Filter，它也被称为FilterChainProxy。它可以链接任意一个或多个其他的 Filter。Spring Security 依赖一系列 Servlet Filter 来提供不同的安全特性。不需要知道这些细节因为开发中不需要显式声明springSecurityFilterChain以及它所链接在一起的其他 Filter。当我们启用 Web 安全性的时候会自动创建这些Filter。 简单的安全性配置 方 法 描 述 configure(WebSecurity) 通过重载配置 Spring Security 的 Filter 链 configure(HttpSecurity) 通过重载配置如何通过拦截器保护请求 configure(AuthenticationManagerBuilder) 通过重载配置 user-detail 服务 @EnableWebSecurity注解将会启用 Web 安全功能。但它本身并没有什么用处 Spring Security 必须配置在一个实现了WebSecurityConfigurer的 bean 中或者简单起见扩展WebSecurityConfigurerAdapter。 @EnableWebSecurity可以启用任意 Web 应用的安全性功能，不过如果是使用 Spring MVC 开发的那么就应该考虑使用@EnableWebMvcSecurity`替代它。 12345678910111213@Configuration@EnableWebMvcSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(\"user\").password(\"password\").roles(\"USER\") .and .withUser(\"user\").password(\"password\").authorities(\"ROLE_USER\"); // 等价 &#125;&#125; AuthenticationManagerBuilder有多个方法用来配置 Spring Security 对认证的支持。通过inMemoryAuthentication()方法我们可以启用、配置并任意填充基于内存的用户存储。 withUser()方法返回的是UserDetailsManagerConfigurer.UserDetailsBuilder，这个对象提供了多个进一步配置用户的方法包括设置用户密码的password()方法以及为给定用户授予一个或多个角色权限的roles()方法。 配置用户详细信息的方法。 方 法 描 述 accountExpired(boolean) 定义账号是否已经过期 accountLocked(boolean) 定义账号是否已经锁定 and() 用来连接配置 authorities(GrantedAuthority...) 授予某个用户一项或多项权限 authorities(List) 授予某个用户一项或多项权限 authorities(String...) 授予某个用户一项或多项权限 credentialsExpired(boolean) 定义凭证是否已经过期 disabled(boolean) 定义账号是否已被禁用 password(String) 定义用户的密码 roles(String...) 授予某个用户一项或多项角色 基于数据库表认证用户数据通常会存储在关系型数据库中并通过 JDBC 进行访问。为了配置 Spring Security 使用以 JDBC 为支撑的用户存储，可以使用jdbcAuthentication()方法。 123456789101112@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .jdbcAuthentication() .dataSource(dataSource) .usersByUsernameQuery( \"select username, password, true \" + \"from Spitter where username=?\") .authoritiesByUsernameQuery( \"select username, 'ROLE_USER' from Spitter where username=?\") .passwordEncoder(new StandardPasswordEnconder(\"123456\"));&#125; passwordEncoder()方法可以接受 Spring Security 中PasswordEncoder接口的任意实现。Spring Security 的加密模块包括了三个这样的实现BCryptPasswordEncoder、NoOpPasswordEncoder和StandardPasswordEncoder。 基于 LDAP认证LDAP（Light Directory Access Portocol），它是基于 X.500 标准的轻量级目录访问协议。 拦截请求适量地应用安全性。 在任何应用中并不是所有的请求都需要同等程度地保护。有些请求需要认证而另一些可能并不需要。有些请求可能只有具备特定权限的用户才能访问没有这些权限的用户会无法访问。 对每个请求进行细粒度安全性控制的关键在于重载configure(HttpSecurity)方法。 123456789@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/spitters/me\").hasAuthority(\"ROLE_SPITTER\") // 路径可使用 Ant 风格的通配符 .antMatchers(HttpMethoed.POST, \"/spitters\").hasAuthority(\"ROLE_SPITTER\") // .antMatchers(HttpMethoed.POST, \"/spitters\").hasRole(\"SPITTER\") 同上 .anyRequest().permitAll();&#125; 用来定义如何保护路径的配置方法。 方 法 能够做什么 access(String) 如果给定的SpEL表达式计算结果为true就允许访问 anonymous() 允许匿名用户访问 authenticated() 允许认证过的用户访问 denyAll() 无条件拒绝所有访问 fullyAuthenticated() 如果用户是完整认证的话不是通过 Remember-me 功能认证的就允许访问 hasAnyAuthority(String...) 如果用户具备给定权限中的某一个的话就允许访问 hasAnyRole(String...) 如果用户具备给定角色中的某一个的话就允许访问 hasAuthority(String) 如果用户具备给定权限的话就允许访问 hasIpAddress(String) 如果请求来自给定 IP 地址的话就允许访问 hasRole(String) 如果用户具备给定角色的话就允许访问 not() 对其他访问方法的结果求反 permitAll() 无条件允许访问 rememberMe() 如果用户是通过Remember-me功能认证的就允许访问 上表中的大多数方法都是一维的，例如使用hasRole()限制某个特定的角色的同时不能在相同的路径上同时通过hasIpAddress()限制特定的 IP 地址。 借助access()方法我们也可以将 SpEL 作为声明访问限制的一种方式。 安全表达式 计 算 结 果 authentication 用户的认证对象 denyAll 结果始终为 false hasAnyRole(list of roles) 如果用户被授予了列表中任意的指定角色结果为 true hasRole(role) 如果用户被授予了指定的角色结果为 true hasIpAddress(IP Address) 如果请求来自指定IP的话结果为 true isAnonymous() 如果当前用户为匿名用户结果为 true isAuthenticated() 如果当前用户进行了认证的话结果为 true isFullyAuthenticated() 如果当前用户进行了完整认证的话，即非 Remember-me 功能进行的认证结果为 true isRememberMe() 如果当前用户是通过 Remember-me 自动认证的结果为 true permitAll 结果始终为 true principal 用户的 principal 对象 12.antMatchers(\"/spitter/me\").access(\"hasRole('ROLE_SPITTER') and hasIpAddress('192.168.1.2')\"); // 二维认证 强制通道的安全性对于敏感的信息，为了保证注册表单的数据通过 HTTPS 传送，可以在配置中添加requiresChannel()方法。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(\"/spitter/me\").hasRole(\"SPITTER\") .antMatchers(HttpMethod.POST, \"/spittles\").hasRole(\"SPITTER\") .anyRequest().permitAll() .and() .requeresChannel() .antMatchers(\"/spitter/form\").requiresSecure(); // 需要 HTTPS&#125; 不论何时只要是对 “/spitter/form” 的请求，Spring Security 都视为需要安全通道通过调用requiresChannel()确定的并自动将请求重定向到 HTTPS 上。 与之相反有些页面并不需要通过 HTTPS 传送，可以使用requiresInsecure()代替requiresSecure()方法将首页声明为始终通过 HTTP 传送。 防止跨站请求伪造如果一个站点欺骗用户提交请求到其他服务器的话就会发生 cross-site request forgery CSRF 攻击。 Spring Security 通过一个同步 token 的方式来实现 CSRF 防护的功能。它将会拦截状态变化的请求，例如非GET、HEAD、OPTIONS和TRACE的请求并检查 CSRF token。如果请求中不包含 CSRF token 的话或者 token 不能与服务器端的 token 相匹配请求将会失败并抛出CsrfException异常。 这意味着在你的应用中所有的表单必须在一个 “_csrf” 域中提交 token 而且这个 token 必须要与服务器端计算并存储的 token 一致这样的话当表单提交的时候才能进行匹配。 12345&lt;!-- Thymeleaf 只要 &lt;form&gt; 标签的 action 属性添加了 Thymeleaf 命名空间前缀则会自动生成 “_csrf” 隐藏域 --&gt;&lt;form method=\"POST\" th:action=\"@&#123;/spittles&#125;\"&gt;&lt;/form&gt;&lt;!-- JSP --&gt;&lt;input type=\"hidden\" name=\"$&#123;_csrf.parameterName&#125;\" value=\"$&#123;_csrf.token&#125;\" /&gt; 12345678// 手动禁用，不推荐@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http // ... .csrf() .disable();&#125; 认证用户页面实际上在重写configure(HttpSecurity)之前我们都能使用一个简单却功能完备的登录页。但是一旦重写了configure(HttpSecurity)方法就失去了这个简单的登录页面。 12345678910111213@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() // 指定自定义的登录页面的访问路径 .and() .authorizeRequests() .antMatchers(\"/spitter/me\").hasRole(\"SPITTER\") .antMatchers(HttpMethod.POST, \"/spittles\").hasRole(\"SPITTER\") .anyRequest().permitAll() .and() .requeresChannel() .antMatchers(\"/spitter/form\").requiresSecure();&#125; HTTP Basic 认证（HTTP Basic Authentication）会直接通过 HTTP 请求本身，对要访问应用程序的用户进行认证。你可能在以前见过 HTTP Basic 认证。当在 Web 浏览器中使用时，它将向用户弹出一个简单的模态对话框。 但这只是 Web 浏览器的显示方式。本质上，这是一个 HTTP 401 响应， 表明必须要在请求中包含一个用户名和密码。在 REST 客户端向它使用的服务进行认证的场景中，这种方式比较适合。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(\"/login\") .and() .httpBasic() .realmName(\"Spittr\") .and() // ...&#125; Spring Security 使得为应用添加 Remember-me 功能变得非常容易。为了启用这项功能只需在configure()方法所传入的HttpSecurity对象上调用rememberMe()即可。这个功能是通过在 cookie 中存储一个 token 完成的。 存储在 cookie 中的 token 包含用户名、密码、过期时间和一个私钥 —— 在写入 cookie 前都进行了 MD5 哈希。默认情况下，私钥的名为 SpringSecured，但在这里我们将其设置为 spitterKey，使它专门用于 Spittr应用。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(\"/login\") .and() .rememberMe() .tokenValiditySeconds(2419200) // token 四周内有效 .key(\"spittrKey\") // ...&#125; 退出功能是通过 Servlet 容器中的 Filter 实现的（默认情况下），这个 Filter 会拦截针对 “/logout” 的请求。 1&lt;a th:href=\"@&#123;/logout&#125;\"&gt;Logout&lt;/a&gt; 当用户点击这个链接的时候，会发起对 “/logout” 的请求，这个请求会被 Spring Security 的 LogoutFilter 所处理。用户会退出应用，所有的 Remember-me token 都会被清除掉。在退出完成后，用户浏览器将会 重定向到 “/login?logout”，从而允许用户进行再次登录。 如果你希望用户被重定向到其他的页面，如应用的首页，那么可以在 configure() 中进行配置。 1234567891011@overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(\"/login\") .and() .logout() .logoutSuccessUrl(\"/\") // 指定退出成功后的跳转页面 .logout(\"signout\") // 重写的默认的 LogoutFilter 拦截路径 // ...&#125; 保护视图使用 Spring Security 的 JSP 标签库。 JSP 标签 作 用 &lt;security:accesscontrollist&gt; 如果用户通过访问控制列表授予了指定的权限那么渲染该标签体中的内容 &lt;security:authentication&gt; 渲染当前用户认证对象的详细信息 &lt;security:authorize&gt; 如果用户被授予了特定的权限或者 SpEL 表达式的计算结果为 true 那么渲染该标签 体中的内容 使用&lt;security:authentication&gt; JSP 标签来访问用户的认证详情。 认 证 属 性 描 述 authorities 一组用于表示用户所授予权限的 GrantedAuthority 对象 Credentials 用于核实用户的凭证通常这会是用户的密码 details 认证的附加信息IP地址、证件序列号、会话 ID 等 principal 用户的基本信息对象 1234567891011121314&lt;sec:authorize access=\"hasRole('ROLE_SPITTER')\"&gt; &lt;s:url value=\"/spittles\" var=\"spittle_url\" /&gt; &lt;sf:form modelAttribute=\"spittle\" action=\"$&#123;spittle_url&#125;\"&gt; &lt;sf:label path=\"text\"&gt; &lt;s:message code=\"label.spittle\" text=\"Enter spittle:\" /&gt; &lt;/sf:label&gt; &lt;sf:textarea path=\"text\" row=\"2\" cols=\"40\" /&gt; &lt;sf:errors path=\"text\" /&gt; &lt;br/&gt; &lt;div class=\"spitItSubmit\"&gt; &lt;input type=\"submit\" value=\"Spit it !\" class=\"status-btn round-btn disabled\" /&gt; &lt;/div&gt; &lt;/sf:form&gt;&lt;/sec:authorize&gt; Thymeleaf 的安全方言提供了条件化渲染和显示认证细节的能力。 属 性 作 用 sec:authentication 渲染认证对象的属性 sec:authorize 基于表达式的计算结果条件性的渲染内容 sec:authorize-acl 基于表达式的计算结果条件性的渲染内容 sec:authorize-expr sec:authorize属性的别名 sec:authorize-url 基于给定URL路径相关的安全规则条件性的渲染内容 1234567@Beanpublic SpringTemplateEngine templateEngine(TemplateResolver templateResolver) &#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); templateEngine.addDialect(new SpringSecurityDialect()); // 注册安全方言 return templateEngine;&#125; 123&lt;div sec:authorize=\"isAuthenticated\"&gt; Hello &lt;span sec:authentication=\"name\"&gt;someone&lt;/span&gt;&lt;/div&gt; 后端中的 Spring关注 Spring 如何帮助我们在后端处理数据。 Spring 与 JDBC 定义 Spring 对数据访问的支持； 配置数据库资源； 使用 Spring 的 JDBC 模版。 为了避免持久化的逻辑分散到应用的各个组件中最好将数据访问的功能放到一个或多个专注于此项任务的组件中。这样的组件通常称为数据访问对象 data access object DAO 或 Repository。 为了避免应用与特定的数据访问策略耦合在一起编写良好的 Repository 应该以接口的方式暴露功能。 服务对象本身并不会处理数据访问而是将数据访问委托给 Repository，Repository 接口确保其与服务对象的松耦合。 Spring 将数据访问过程中固定的和可变的部分明确划分为两个不同的类模板 template 和回调 callback。模板管理过程中固定的部分而回调处理自定义的数据访问代码。 配置数据源 通过 JDBC 驱动程序定义的数据源； 通过 JNDI 查找的数据源； 连接池的数据源。 JNDI（Java Naming and Directory Interface ），类似于在一个中心注册一个东西，以后要用的时候，只需要根据名字去注册中心查找，注册中心返回你要的东西。在 web 程序中可以将一些东西（比如数据库相关的）交给服务器软件去配置和管理（有全局配置和单个 web 程序的配置），在程序代码中只要通过名称查找就能得到注册的东西，而且如果注册的东西有变，比如更换了数据库，我们只需要修改注册信息，名称不改，因此代码也不需要修改。 12345678@Beanpublic JndiObjectFactoryBean dataSource() &#123; JndiObjectFactoryBean jndiObjectFB = new JndiObjectFactoryBean(); jndiObjectFB.setJndiName(\"jdbc/SpittrDS\"); jndiObjectFB.setResourceRef(true); jndiObjectFB.setProxyInterface(javax.sql.DataSource.class); return jndiObjectFB;&#125; 使用数据源连接池。（推荐） 1234567891011@Beanpublic BasicDataSource dataSource() &#123; BasicDataSource ds = new BasicDataSource(); ds.setDriverClassName(\"org.h2.Driver\"); ds.setUrl(\"jdbc:h2:tcp://localhost/~/spitter\"); ds.setUsername(\"sa\"); ds.setPassword(\"\"); ds.setInitialSize(5); ds.setMaxActive(10); return ds;&#125; 在Spring中通过 JDBC 驱动定义数据源是最简单的配置方式。Spring 提供了三个这样的数据源类均位于org.springframework.jdbc.datasource包中供选择。 DriverManagerDataSource在每个连接请求时都会返回一个新建的连接。与 DBCP 的BasicDataSource不同由DriverManagerDataSource提供的连接并没有进行池化管理 SimpleDriverDataSource与DriverManagerDataSource的工作方式类似但是它直接使用 JDBC 驱动来解决在特定环境下的类加载问题这样的环境包括OSGi容器 SingleConnectionDataSource在每个连接请求时都会返回同一个的连接。尽管SingleConnectionDataSource不是严格意义上的连接池数据源但是你可以将其视为只有一个连接的池。 123456789@Beanpublic DataSource dataSource() &#123; DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(\"org.h2.Driver\"); ds.setUrl(\"jdbc:h2:tcp://localhost/~/spitter\"); ds.setUsername(\"sa\"); ds.setPassword(\"\"); return ds;&#125; 与具备池功能的数据源相比，唯一的区别在于这些数据源 bean 都没有提供连接池功能，所以没有可配置的池相关的属性。 Spring 的 JDBC 命名空间能够简化嵌入式数据库的配置，可以使用EmbeddedDatabaseBuilder来构建DataSource。 12345678@Beanpublic DataSource dataSource() &#123; return new EmbeddedDatabaseBuilder() .setType(EmbeddedDatabaseType.H2) .setScript(\"classpath:schema.sql\") .setScript(\"classpath:text-data.sql\") .build();&#125; 使用 @profile 选择数据源。 JDBC 模板 JdbcTemplate最基本的 Spring JDBC 模板，这个模板支持简单的 JDBC 数据库访问功能以及基于索引参数的查询； NamedParameterJdbcTemplate使用该模板类执行查询时可以将值以命名参数的形式绑定到 SQL 中而不是使用简单的索引参数； SimpleJdbcTemplate该模板类利用 Java 5 的一些特性如自动装箱、泛型以及可变参数列表来简化 JDBC 模板的使用。 为了让JdbcTemplate正常工作，只需要为其设置DataSource就可以了。之后将jdbcTemplate装配到 Repository 中并使用它来访问数据库就可以了。 1234@Beanpublic JdbcTemplate jdbcTemplate(DataSource dataSource) &#123; return new JdbcTemplate(dataSource);&#125; 1234567891011@Respositorypublic class JdbcSpitterRepository implements SpitterRepository &#123; // 通过注入非具体的 JdbcTemplate 达到松耦合 private JdbcOperations jdbcOperations; @Inject public JdbcSpitterRepository(JdbcOperations jdbcOperations) &#123; this.jdbcOperations= jdbcOperations; &#125; // ...&#125; ORM 持久化数据 使用 Spring 和 Hibernate； 借助上下文 Session 编写不依赖于 Spring 的 Repository； 通过 Spring 使用 JPA； 借助 Spring Data 实现自动化的 JPA Repository。 HIbernate使用 Hibernate 所需的主要接口是org.hibernate.Session。Session接口提供了基本的数据访问功能如保存、更新、删除以及从数据库加载对象的功能。通过 Hibernate 的Session接口应用程序的 Repository 能够满足所有的持久化需求。 获取 Hibernate Session 对象的标准方式是借助于 Hibernate SessionFactory接口的实现类。除了一些其他的任务SessionFactory主要负责 Hibernate Session的打开、关闭以及管理。 JPA在 Spring 中使用 JPA 的第一步是要在 Spring 应用上下文中将实体管理器工厂（entity manager factory）按照 bean 的形式来进行配置。 基于 JPA 的应用程序需要使用 EntityManagerFactory 的实现类来获取 EntityManager 实例。 容器管理的 JPA 采取了一个不同的方式。当运行在容器中时，可以使用容器（在我们的场景下是 Spring）提供的信息来生成 EntityManagerFactory。 12345678910111213141516171819@Beanpublic LocalContainerEntityManagerFactoryBean entityManagerFactory( DataSource dataSource, JpaVendorAdapter jpaVendorAdapter) &#123; LocalContainerEntityManagerFactoryBean emfb = new LocalContainerEntityManagerFactoryBean(); emfb.setDataSource(dataSource); emfb.setJpaVendorAdapter(jpaVendorAdapter); return emfb;&#125;@Beanpublic JpaVendorAdapter jpaVendorAdapter() &#123; // Hibernate 厂商提供的适配器 HibernateJpaVendorAdapter adapter = new HibernateJpaVendorAdapter(); adapter.setDatabase(\"HSQL\"); adapter.setShowSql(true); adapter.setGenerateDdl(false); adapter.setDatabasePlatform(\"org.hibernet.dialect.HSQLDialect\"); return adapter;&#125; 1234567public interface SpitterRepository extends JpaRepository&lt;Spitter, Long&gt; &#123;&#125;@Configuration@EnableJpaRepositories(basePackages=\"com.habuma.spittr.db\")public class JpaConfiguration &#123;&#125; Repository 方法的命名遵循一种模式，有助于 Spring Data 生成针对数据库的查询。（主题可以省略） 12// 方法签名List&lt;Spitter&gt; readByFirstnameIgnoringCaseOrLastnameIgnoringCase(String first, String last); 声明自定义查询。 12@Query(\"select s from Spitter s where s.email like '%gmail.com'\")List&lt;Spitter&gt; findAllGmailSpitters(); 混合自定义查询，即嵌套查询。 NoSQL 为 MongoDB 和 Neo4j 编写 Repository； 为多种数据存储形式持久化数据； 组合使用 Spring 和 Redis。 MongoDBMongoDB 是最为流行的开源文档数据库之一。Spring Data MongoDB 提供了三种方式在 Spring 应用中使用 MongoDB。 通过注解实现对象-文档映射； 使用MongoTemplate实现基于模板的数据库访问； 自动化的运行时 Repository 生成功能。 杂项基于LDAP进行认证使用 Apache Common Lang 包来实现equals()和hashCode()方法。 12345678@Overridepublic boolean equals(Object that) &#123; return EqualsBuilder.reflectionEquals(this, that, \"id\", \"time\");&#125;@Overridepublic int hashCode() &#123; return HashCodeBuilder.reflectionHashCode(this, \"id\", \"time\");&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}]},{"title":"Flume","slug":"大数据/Flume","date":"2020-05-08T06:27:33.000Z","updated":"2020-07-10T03:37:23.451Z","comments":true,"path":"2020/05/08/大数据/Flume/","link":"","permalink":"http://yoursite.com/2020/05/08/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flume/","excerpt":"Flume 基本介绍及简单使用。","text":"Flume 基本介绍及简单使用。 基本概念Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。 Flume 最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到 HDFS。 基础框架 Flume 支持将事件流向一个或者多个目的地，这种模式可以将相同数据复制到多个 channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。 Agent Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。 Agent 主要有 3 个部分组成：Source、Channel、Sink。 Source Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。 Sink Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。 Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。 Channel Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个 Sink 的读取操作。 Flume 自带两种 Channel：Memory Channel 和 File Channel。 Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适 用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。 File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。 Event 传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。 Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构， Body 用来存放该条数据，形式为字节数组。 快速入门12345tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/mv apache-flume-1.7.0-bin flumemv flume-env.sh.template flume-env.shvi flume-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144 使用 Flume 监听一个端口，收集该端口数据（使用 nectcat 工具向该端口发送数据），并打印到控制台。 1234567891011121314151617181920212223242526272829303132# 安装 netcat 工具sudo yum install -y nc# 判断 44444 端口是否被占用 sudo netstat -tunlp | grep 44444# 创建 Flume Agent 配置文件 flume-netcat-logger.conf # 在 flume 目录下创建 job 文件夹并进入 job 文件夹。mkdir jobcd job/# 在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf。# vim flume-netcat-logger.conf# 在 flume-netcat-logger.conf 文件中添加如下内容。（参考官方文档配置） # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat # 数据来源 netcat 工具 a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # 数据输出目的地为 logger 控制台类型 # Use a channel which buffers events in memory a1.channels.c1.type = memory # 通道的存储为内存型 a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # 100 条 even 后才提交事务 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1# 开启 flume 监听端口 控制台日志打印级别设置为 INFO 级别bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console# 使用 netcat 工具向本机的 44444 端口发送内容 nc localhost 44444 实际案例Exec 实时监控 Hive 日志，并上传到 HDFS 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243# Flume 要想将数据输出到 HDFS，须持有 Hadoop 相关 jar 包# 创建 flume-file-hdfs.conf 文件 vim flume-file-hdfs.conf # Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 # Describe/configure the source a2.sources.r2.type = exec a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log a2.sources.r2.shell = /bin/bash -c # Describe the sink a2.sinks.k2.type = hdfs a2.sinks.k2.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%H # 上传文件的前缀 a2.sinks.k2.hdfs.filePrefix = logs- # 是否按照时间滚动文件夹 a2.sinks.k2.hdfs.round = true # 多少时间单位创建一个新的文件夹 a2.sinks.k2.hdfs.roundValue = 1 # 重新定义时间单位 a2.sinks.k2.hdfs.roundUnit = hour # 是否使用本地时间戳 a2.sinks.k2.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k2.hdfs.batchSize = 1000 # 设置文件类型，可支持压缩 a2.sinks.k2.hdfs.fileType = DataStream # 多久生成一个新的文件 a2.sinks.k2.hdfs.rollInterval = 30 # 设置每个文件的滚动大小 a2.sinks.k2.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a2.sinks.k2.hdfs.rollCount = 0 # Use a channel which buffers events in memory a2.channels.c2.type = memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf# 开启 Hadoop 和 Hive 并操作 Hive 产生日志 Spooldir 使用 Flume 监听整个目录的文件，并上传至 HDFS。 123456789101112131415161718192021222324252627282930313233343536373839404142vim flume-dir-hdfs.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source a3.sources.r3.type = spooldir a3.sources.r3.spoolDir = /opt/module/flume/upload a3.sources.r3.fileSuffix = .COMPLETED # 定义文件上传完的后缀 a3.sources.r3.fileHeader = true # 忽略所有以.tmp 结尾的文件，不上传 a3.sources.r3.ignorePattern = ([^ ]*\\.tmp) # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H # 上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- # 是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true # 多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 # 重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour # 是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 # 设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream # 多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 # 设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf Exec source 适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir Source 能够保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而 Taildir Source 既能够实现断点续传，又可以保证数据不丢失，还能够进行实时监控。 Taildir 使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS。 12345678910111213141516171819202122232425262728293031323334353637383940414243vim flume-taildir-hdfs.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source a3.sources.r3.type = TAILDIR # 维护了一个 json 格式的 position File # 其会定期的往 position File 中更新每个文件读取到的最新的位置，因此能够实现断点续传 a3.sources.r3.positionFile = /opt/module/flume/tail_dir.json a3.sources.r3.filegroups = f1 a3.sources.r3.filegroups.f1 = /opt/module/flume/files/file.* # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H # 上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- # 是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true # 多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 # 重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour # 是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 # 设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream # 多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 # 设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf 进阶特性事务 内部原理 ChannelSelector ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 Replicating（复制）和 Multiplexing（多路复用）。 ReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。 SinkProcessor SinkProcessor 共有三种类型，分别是 DefaultSinkProcessor、LoadBalancingSinkProcessor 和 FailoverSinkProcessor。 DefaultSinkProcessor 对应的是单个的 Sink ， LoadBalancingSinkProcessor 和 FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以实现故障转移的功能。 拓扑结构简单串联 将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量，flume 数量过多不仅会影响传输速率， 而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。 复制和多路复用 Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个 channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。 负载均衡和故障转移 Flume支持使用将多个 sink 逻辑上分到一个 sink 组，sink 组配合不同的 SinkProcessor 可以实现负载均衡和错误恢复的功能。 聚合 这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志的 flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析。 企业开发复制和复用 使用 Flume-1 监控文件变动，Flume-1 将变动内容传递给 Flume-2，Flume-2 负责存储到 HDFS。同时 Flume-1 将变动内容传递给 Flume-3，Flume-3 负责输出到 Local FileSystem。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899cd group1/mkdir flume3# 监控文件变动vim flume-file-flume.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 # 将数据流复制给所有 channel a1.sources.r1.selector.type = replicating # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log a1.sources.r1.shell = /bin/bash -c # Describe the sink # sink 端的 avro 是一个数据发送者 a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop102 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = hadoop102 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2# 负责存储到 HDFSvim flume-flume-hdfs.conf # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source # source 端的 avro 是一个数据接收服务 a2.sources.r1.type = avro a2.sources.r1.bind = hadoop102 a2.sources.r1.port = 4141 # Describe the sink a2.sinks.k1.type = hdfs a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H # 上传文件的前缀 a2.sinks.k1.hdfs.filePrefix = flume2- # 是否按照时间滚动文件夹 a2.sinks.k1.hdfs.round = true # 多少时间单位创建一个新的文件夹 a2.sinks.k1.hdfs.roundValue = 1 # 重新定义时间单位 a2.sinks.k1.hdfs.roundUnit = hour # 是否使用本地时间戳 a2.sinks.k1.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k1.hdfs.batchSize = 100 # 设置文件类型，可支持压缩 a2.sinks.k1.hdfs.fileType = DataStream # 多久生成一个新的文件 a2.sinks.k1.hdfs.rollInterval = 600 # 设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a2.sinks.k1.hdfs.rollCount = 0 # Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1# 保存到本地目录vim flume-flume-dir.conf # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop102 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = file_roll # 输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。 a3.sinks.k1.sink.directory = /opt/module/data/flume3 # Describe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.confbin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.confbin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf# 启动 Hadoop 和 Hive，查看结果 均衡和转移 使用 Flume1 监控一个端口，其 sink 组中的 sink 分别对接 Flume2 和 Flume3，采用 FailoverSinkProcessor，实现故障转移的功能。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980cd group2/# 监听 nectcat 发送的数据vim flume-netcat-flume.conf # Name the components on this agent a1.sources = r1 a1.channels = c1 a1.sinkgroups = g1 a1.sinks = k1 k2 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # 失败重试 a1.sinkgroups.g1.processor.type = failover a1.sinkgroups.g1.processor.priority.k1 = 5 a1.sinkgroups.g1.processor.priority.k2 = 10 a1.sinkgroups.g1.processor.maxpenalty = 10000 # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop102 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = hadoop102 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 # 成组负载平衡或故障转移策略 a1.sinkgroups.g1.sinks = k1 k2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c1# 控制台输出 1 号vim flume-flume-console1.conf # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = avro a2.sources.r1.bind = hadoop102 a2.sources.r1.port = 4141 # Describe the sink a2.sinks.k1.type = logger # Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1# 控制台输出 2 号vim flume-flume-console2.conf # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop102 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = logger # Describe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf# 使用 netcat 工具向本机的 44444 端口发送内容nc localhost 44444# 查看 Flume2 及 Flume3 的控制台打印日志 # 将 Flume2 kill，观察 Flume3 的控制台打印情况# 使用 jps -ml 查看 Flume 进程。 聚合 hadoop102 上的 Flume-1 监控文件 /opt/module/data/group.log，hadoop103 上的 Flume-2 监控某一个端口的数据流， Flume-1 与 Flume-2 将数据发送给 hadoop104 上的 Flume-3，Flume-3 将最终数据打印到控制台。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 分发程序xsync flume# 各节点上创建配置文件目录mkdir group3# 配置 Source 用于监控 hive.log 文件，配置 Sink 输出数据到下一级 Flume。# 在 hadoop102 上编辑配置文件vim flume1-logger-flume.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /opt/module/group.log a1.sources.r1.shell = /bin/bash -c # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop104 a1.sinks.k1.port = 4141 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1# 配置 Source 监控端口 44444 数据流，配置 Sink 数据到下一级 Flume：# 在 hadoop103 上编辑配置文件vim flume2-netcat-flume.conf # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = netcat a2.sources.r1.bind = hadoop103 a2.sources.r1.port = 44444 # Describe the sink a2.sinks.k1.type = avro a2.sinks.k1.hostname = hadoop104 a2.sinks.k1.port = 4141 # Use a channel which buffers events in memory a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1# 配置 source 用于接收 flume1 与 flume2 发送过来的数据流，最终合并后 sink 到控制台。# 在 hadoop104 上编辑配置文件touch flume3-flume-logger.confvim flume3-flume-logger.conf # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c1 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop104 a3.sources.r1.port = 4141 # Describe the sink a3.sinks.k1.type = logger # Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 a3.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c1 a3.sinks.k1.channel = c1bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume1-logger-flume.confbin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume2-netcat-flume.conf# 在 hadoop103 上向/opt/module 目录下的 group.log 追加内容 echo 'hello' &gt; group.log# 在 hadoop102 上向 44444 端口发送数据 telnet hadoop102 44444# 检查 hadoop104 上数据 Interceptor在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing 结构。 Multiplexing 的原理是根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel 中，所以我们需要自定义一个 Interceptor，为不同类型的 event 的 Header 中的 key 赋予不同的值。 在该案例中，我们以端口数据模拟日志，以数字（单个）和字母（单个）模拟不同类型的日志，我们需要自定义 interceptor 区分数字和字母，将其分别发往不同的分析系统 （Channel）。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536// 打成 jar 包放在 lib 目录下public class CustomInterceptor implements Interceptor &#123; @Override public void initialize() &#123; &#125; @Override public Event intercept(Event event) &#123; byte[] body = event.getBody(); if (body[0] &lt; 'z' &amp;&amp; body[0] &gt; 'a') &#123; // 头部添加 K-V 用于识别分发 event.getHeaders().put(\"type\", \"letter\"); &#125; else if (body[0] &gt; '0' &amp;&amp; body[0] &lt; '9') &#123; event.getHeaders().put(\"type\", \"number\"); &#125; return event; &#125; @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; for (Event event : events) &#123; intercept(event); &#125; return events; &#125; @Override public void close() &#123; &#125; public static class Builder implements Interceptor.Builder &#123; @Override public Interceptor build() &#123; return new CustomInterceptor(); &#125; @Override public void configure(Context context) &#123; &#125; &#125;&#125; 为 hadoop102 上的 Flume1 配置 1 个 netcat source，1 个 sink group（2 个 avro sink）， 并配置相应的 ChannelSelector 和 interceptor。 123456789101112131415161718192021222324252627282930313233343536373839# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.interceptors = i1# 全路径，并且用”$“符号分割加上自定义 Builder 的类名a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.CustomInterceptor$Buildera1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = type# 根据头部 value 进行通道的选择a1.sources.r1.selector.mapping.letter = c1a1.sources.r1.selector.mapping.number = c2# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop103a1.sinks.k1.port = 4141a1.sinks.k2.type=avroa1.sinks.k2.hostname = hadoop104a1.sinks.k2.port = 4242# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Use a channel which buffers events in memorya1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2# 为 hadoop103/104 上的 Flume3 配置一个 avro source 和一个 logger sink# 分别在 hadoop102，hadoop103，hadoop104 上启动 flume 进程，注意先后顺序# 在 hadoop102 使用 netcat 向 localhost:44444 发送字母和数字# 观察 hadoop103 和 hadoop104 打印的日志 Source 使用 flume 接收数据，并给每条数据添加前缀，输出到控制台。前缀可从 flume 配置文件中配置。 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243// 打成 jar 包放在 lib 目录下public class MySource extends AbstractSource implements Configurable, PollableSource &#123; // 定义配置文件将来要读取的字段 private Long delay; private String field; // 初始化配置信息 @Override public void configure(Context context) &#123; delay = context.getLong(\"delay\"); field = context.getString(\"field\", \"Hello!\"); &#125; @Override public Status process() throws EventDeliveryException &#123; try &#123; // 创建事件头信息 HashMap&lt;String, String&gt; hearderMap = new HashMap&lt;&gt;(); // 创建事件 SimpleEvent event = new SimpleEvent(); // 循环封装事件 for (int i = 0; i &lt; 5; i++) &#123; // 给事件设置头信息 event.setHeaders(hearderMap); // 给事件设置内容 event.setBody((field + i).getBytes()); // 将事件写入 channel getChannelProcessor().processEvent(event); Thread.sleep(delay); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; return Status.READY; &#125; @Override public long getBackOffSleepIncrement() &#123; return 0; &#125; @Override public long getMaxBackOffSleepInterval() &#123; return 0; &#125;&#125; 123456789101112131415161718# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = com.wingo.MySource# 自定义配置a1.sources.r1.delay = 1000#a1.sources.r1.field = wingo# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 SinkSink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。 Sink 是完全事务性的。在从 Channel 批量删除数据之前，每个 Sink 用 Channel 启动一个事务。批量事件一旦成功写出到存储系统或下一个 Flume Agent，Sink 就利用 Channel 提交事务。事务一旦被提交，该 Channel 从自己的内部缓冲区删除事件。 使用 flume 接收数据，并在 Sink 端给每条数据添加前缀和后缀，输出到控制台。前后缀可在 flume 任务配置文件中配置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 打成 jar 包放在 lib 目录下public class MySink extends AbstractSink implements Configurable &#123; // 创建 Logger 对象 private static final Logger LOG = LoggerFactory.getLogger(AbstractSink.class); private String prefix; private String suffix; @Override public Status process() throws EventDeliveryException &#123; // 声明返回值状态信息 Status status; // 获取当前 Sink 绑定的 Channel Channel ch = getChannel(); // 获取事务 Transaction txn = ch.getTransaction(); // 声明事件 Event event; // 开启事务 txn.begin(); // 读取 Channel 中的事件，直到读取到事件结束循环 while (true) &#123; event = ch.take(); if (event != null) &#123; break; &#125; &#125; try &#123; // 处理事件（打印） LOG.info(prefix + new String(event.getBody()) + suffix); // 事务提交 txn.commit(); status = Status.READY; &#125; catch (Exception e) &#123; // 遇到异常，事务回滚 txn.rollback(); status = Status.BACKOFF; &#125; finally &#123; // 关闭事务 txn.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; // 读取配置文件内容，有默认值 prefix = context.getString(\"prefix\", \"hello:\"); // 读取配置文件内容，无默认值 suffix = context.getString(\"suffix\"); &#125;&#125; 123456789101112131415161718# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = com.wingo.MySinka1.sinks.k1.suffix = :wingo# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 Ganglia 监控 大公司一般用平台组开发的监控服务，中小型公司用的比较多的是这个，了解即可。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://yoursite.com/tags/Flume/"}]},{"title":"Kafka","slug":"大数据/Kafka","date":"2020-05-06T04:05:51.000Z","updated":"2020-07-10T03:41:01.609Z","comments":true,"path":"2020/05/06/大数据/Kafka/","link":"","permalink":"http://yoursite.com/2020/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/Kafka/","excerpt":"Kafka 基本介绍及简单使用。","text":"Kafka 基本介绍及简单使用。 基本概念Kafka 是一个分布式的基于发布 / 订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 发布 / 订阅模式（一对多，消费者消费数据之后不会清除消息） 基础架构 Producer：消息生产者，就是向 Kafka Broker 发消息的客户端。 Consumer：消息消费者，向 Kafka Broker 取消息的客户端。 Consumer Group：消费者组，由多个 Consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 Broker：一台 Kafka 服务器就是一个 Broker。一个集群由多个 Broker 组成。一个 Broker 可以容纳多个 Topic。 Topic：可以理解为一个队列，生产者和消费者面向的都是一个 Topic。 Partition：为了实现扩展性，一个非常大的 Topic 可以分布到多个 Broker（即服务器）上，一个 Topic 可以分为多个 Partition，每个 Partition 是一个有序的队列。 Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 Partition 数据不丢失，且 Kafka 仍然能够继续工作，Kafka 提供了副本机制，一个 Topic 的每个分区都有若干个副本，一个 Leader 和若干个 Follower。 Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。 Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和 Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Follower。 安装部署123456789101112131415161718192021222324252627282930313233343536tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/mv kafka_2.11-0.11.0.0/ kafkamkdir logscd config/vi server.properties #broker 的全局唯一编号，不能重复 broker.id=0 #删除 topic 功能使能 delete.topic.enable=true #处理网络请求的线程数量 num.network.threads=3 #用来处理磁盘 IO 的现成数量 num.io.threads=8 #发送套接字的缓冲区大小 socket.send.buffer.bytes=102400 #接收套接字的缓冲区大小 socket.receive.buffer.bytes=102400 #请求套接字的缓冲区大小 socket.request.max.bytes=104857600 #kafka 运行日志存放的路径 log.dirs=/opt/module/kafka/logs #topic 在当前 broker 上的分区个数 num.partitions=1 #用来恢复和清理 data 下数据的线程数量 num.recovery.threads.per.data.dir=1 #segment 文件保留的最长时间，超时将被删除 log.retention.hours=168 #配置连接 Zookeeper 集群地址 zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181sudo vi /etc/profile #KAFKA_HOME export KAFKA_HOME=/opt/module/kafka export PATH=$PATH:$KAFKA_HOME/binsource /etc/profile# 将配置分发到其它 Broker，并且记得修改其它 Broker 的环境变量# 其它 Broker 的 broker.id 记得修改，不得重复 123456# Kafka 群起脚本for i in hadoop102 hadoop103 hadoop104 do echo \"========== $i ==========\" ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties' done Shell 操作12345678910111213141516171819# 集群规划：# hadoop102 位 kafka # 查看当前服务器中的所有 topicbin/kafka-topics.sh --zookeeper hadoop102:2181 --list# 创建 topicbin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first# 删除 topicbin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first# 需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除。# 发送消息bin/kafka-console-producer.sh --brokerlist hadoop102:9092 --topic first# 消费消息bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first# --from-beginning：会把主题中以往所有的数据都读取出来bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first# 查看某个 Topic 的详情bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first# 修改分区数bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6 架构深入 Kafka 中消息是以 topic 进行分类的，生产者生产消息，消费者消费消息，都是面向 topic 的。 topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文 件，该 log 文件中存储的就是 producer 生产的数据。Producer 生产的数据会被不断追加到该 log 文件末端，且每条数据都有自己的 offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。 由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位 效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。每个 segment 对应两个文件：“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。 例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。 index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log 文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。 生产者分区策略 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发，因为可以以 Partition 为单位读写了。 将 producer 发送的数据封装成一个 ProducerRecord 对象。 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 可靠性为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果 producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 ISR采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据， 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， 直到它完成同步，才能发送 ack。这个问题怎么解决呢？ Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 合。当 ISR 中的 follower 完成数据的同步之后，follower 就会给 fleader 发送 ack。如果 follower 长时间未向 leader 同步数据 ， 则该 follower 将被踢出 ISR ， 该时间阈值由 replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 应答机制对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失， 所以没必要等 ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡， 选择以下的配置。 acks 为 0： producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； acks 为 1：producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据； acks 为 -1（all）：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才 返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会造成数据重复。 故障处理 HW：指的是消费者能见到的最大的 offset，ISR 队列中最小的 LEO。 follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。 等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 leader 故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 将服务器的 ACK 级别设置为 -1，可以保证 Producer 到 Server 之间不会丢失数据，但不能保证数据不重复，即 At Least Once 语义。 将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，但不能保证数据不丢失，即 At Most Once 语义。 Exactly Once0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论 向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语 义，就构成了 Kafka 的 Exactly Once 语义。 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。 消费者consumer 采用 pull（拉）模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。 它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。 pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费，consumer 会等待一段时间之后再消费，这段时长即为 timeout。 分区分配策略一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 Kafka 有两种分配策略，一是 RoundRobin，一是 Range。 offset 维护由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始， consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为 __consumer_offsets。 事务Kafka 事务 Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 Producer 事务 为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的 PID 和 Transaction ID 绑定。这样当 Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。 为了管理 Transaction，Kafka 引入了一个新的组件 Transaction Coordinator。Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于 事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 Producer API消息发送流程Kafka 的 Producer 发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程：main 线程和 Sender 线程，以及一个线程共享变量：RecordAccumulator。 main 线程将消息发送给 RecordAccumulator，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka broker。 相关参数 batch.size：只有数据积累到 batch.size 之后，sender 才会发送数据。linger.ms：如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。 异步发送12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt; 常用类 KafkaProducer：需要创建一个生产者对象，用来发送数据；ProducerConfig：获取所需的一系列配置参数；ProducerRecord：每条数据都要封装成一个 ProducerRecord 对象。 1234567891011121314151617181920212223242526272829// 不带回调函数的 APIpublic class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // kafka 集群，broker-list props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); // 重试次数 props.put(\"retries\", 1); // 批次大小 props.put(\"batch.size\", 16384); // 等待时间 props.put(\"linger.ms\", 1); // RecordAccumulator 缓冲区大小 props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; producer.send( new ProducerRecord&lt;String, String&gt;( \"first\", Integer.toString(i), Integer.toString(i) // 组 key value ) ); &#125; producer.close(); &#125;&#125; 回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是 RecordMetadata 和 Exception，如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。 消息发送失败会自动重试，不需要我们在回调函数中手动重试。 1234567891011121314151617181920212223242526// 带回调函数的 APIpublic class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // 常规配置 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; producer.send( new ProducerRecord&lt;String, String&gt;( \"first\", Integer.toString(i), Integer.toString(i) ), new Callback() &#123; // 回调函数，该方法会在 Producer 收到 ack 时调用，为异步调用 @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception == null) &#123; System.out.println(\"success-&gt;\" + metadata.offset()); &#125; else &#123; exception.printStackTrace(); &#125; &#125; &#125; ); &#125; producer.close(); &#125;&#125; 同步发送同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回 ack。 由于 send 方法返回的是一个 Future 对象，根据 Futrue 对象的特点也可以实现同步发送的效果，只需在调用 Future 对象的 get 方发即可。 123456789101112131415public class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // 常规配置 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; producer.send( new ProducerRecord&lt;String, String&gt;( \"first\", Integer.toString(i), Integer.toString(i) ) ).get(); // 阻塞等待获取返回值 &#125; producer.close(); &#125;&#125; Consumer APIconsumer 消费数据时的可靠性是很容易保证的，因为数据在 Kafka 中是持久化的，故不用担心数据丢失问题。 由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 所以 offset 的维护是 consumer 消费数据是必须考虑的问题。 为了使我们能够专注于自己的业务逻辑，Kafka 提供了自动提交 offset 的功能。 自动提交 offset 的相关参数： enable.auto.commit：是否开启自动提交 offset 功能；auto.commit.interval.ms：自动提交 offset 的时间间隔。 自动提交12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122public class CustomConsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"true\"); props.put(\"auto.commit.interval.ms\", \"1000\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( \"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value() ); &#125; &#125; &#125;&#125; 手动提交 自动提交是基于时间提交的，开发人员难以把握 offset 提交的时机。因此 Kafka 还提供了手动提交 offset 的 API。 123456789101112131415161718192021222324252627// 同步提交public class CustomComsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // Kafka 集群 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"false\"); // 关闭自动提交 offset props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\")); // 消费者订阅主题 while (true) &#123; // 消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( \"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value() ); &#125; // 同步提交，当前线程会阻塞直到 offset 提交成功 consumer.commitSync(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728// 异步提交public class CustomComsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // ... 常规配置 consumer.subscribe(Arrays.asList(\"first\")); // 消费者订阅主题 while (true) &#123; // 消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( \"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value() ); &#125; //异步提交 consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete( Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.err.println(\"Commit failed for\" + offsets); &#125; &#125; &#125;); &#125; &#125;&#125; 自定义存储Kafka 0.9 版本之前，offset 存储在 zookeeper，0.9 版本及之后，默认将 offset 存储在 Kafka 的一个内置的 topic 中。除此之外，Kafka 还可以选择自定义存储 offset。 offset 的维护是相当繁琐的，因为需要考虑到消费者的 Rebalance。 当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做 Rebalance。 消费者发生 Rebalance 之后，每个消费者消费的分区就会发生变化。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的 offset 位置继续消费。 要实现自定义存储 offset，需要借助 ConsumerRebalanceListener，以下为示例代码，其中提交和获取 offset 的方法，需要根据所选的 offset 存储系统自行实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class CustomConsumer &#123; // 自定义存储 offset 的容器 private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;(); public static void main(String[] args) &#123; // 创建配置信息 Properties props = new Properties(); // Kafka 集群 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(\"group.id\", \"test\"); // 关闭自动提交 offset props.put(\"enable.auto.commit\", \"false\"); // Key 和 Value 的反序列化类 props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); // 创建一个消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); //消费者订阅主题 consumer.subscribe( Arrays.asList(\"first\"), new ConsumerRebalanceListener() &#123; // 该方法会在 Rebalance 之前调用 @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; commitOffset(currentOffset); &#125; // 该方法会在 Rebalance 之后调用 @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; currentOffset.clear(); for (TopicPartition partition : partitions) &#123; // 定位到最近提交的 offset 位置继续消费 consumer.seek(partition, getOffset(partition)); &#125; &#125; &#125;); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); //消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( \"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value() ); currentOffset.put( new TopicPartition(record.topic(), record.partition()), record.offset() ); &#125; commitOffset(currentOffset); // 异步提交 &#125; &#125; // 获取某分区的最新 offset private static long getOffset(TopicPartition partition) &#123; return 0; &#125; // 提交该消费者所有分区的 offset private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) &#123; &#125;&#125; 拦截器producer 拦截器（interceptor）是在 Kafka 0.10 版本被引入的，主要用于实现 clients 端的定制化控制逻辑。 对于 producer 而言，interceptor 使得用户在消息发送前以及 producer 回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。 同时，producer 允许用户指定多个 interceptor 按序作用于同一条消息从而形成一个拦截链（interceptor chain）。 123456789101112131415161718192021// 增加时间戳拦截器public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; // 获取配置信息和初始化数据时调用 @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; // 对消息进行自定义操作 @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; // 创建一个新的 record，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + \",\" + record.value().toString()); &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; &#125; @Override public void close() &#123; &#125;&#125; 12345678910111213141516171819202122232425262728// 统计发送消息成功和发送失败消息数，并在 producer 关闭时打印这两个计数器public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123; private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; return record; &#125; // 消息发送成功或者失败后都会调用此方法 @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; // 统计成功和失败的次数 if (exception == null) &#123; successCounter++; &#125; else &#123; errorCounter++; &#125; &#125; @Override public void close() &#123; // 保存结果 System.out.println(\"Successful sent: \" + successCounter); System.out.println(\"Failed sent: \" + errorCounter); &#125;&#125; 1234567891011121314151617181920212223242526272829// producer public class InterceptorProducer &#123; public static void main(String[] args) throws Exception &#123; // 设置配置信息 Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 3); props.put(\"batch.size\", 16384); props.put(\"linger.ms\", 1); props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 构建拦截链 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(\"com.wingo.kafka.interceptor.TimeInterceptor\"); interceptors.add(\"com.wingo.kafka.interceptor.CounterInterceptor\"); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = \"first\"; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 发送消息 for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, \"message\" + i); producer.send(record); &#125; // 一定要关闭 producer，这样才会调用 interceptor 的 close 方法 producer.close(); &#125;&#125; Kafka Eagle 基于 Web 的 Kafka 监控。 杂项高效读写数据 Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端， 为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这 磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 零拷贝。 Zookeeper 的作用 Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。 Controller 的管理工作都是依赖于 Zookeeper 的。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"Hbase","slug":"大数据/HBase","date":"2020-05-03T00:41:00.000Z","updated":"2020-07-10T03:39:13.940Z","comments":true,"path":"2020/05/03/大数据/HBase/","link":"","permalink":"http://yoursite.com/2020/05/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/","excerpt":"HBase 的简本介绍及简使用。","text":"HBase 的简本介绍及简使用。 简介HBase 是一种分布式、可扩展、支持海量数据存储的 NoSQL 数据库。 逻辑上，HBase 的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。 但从 HBase 的底层物理存储结构（K-V）来看，HBase 更像是一个 multi-dimensional map。 数据模型Name Space 命名空间，类似于关系型数据库的 DatabBase 概念，每个命名空间下有多个表。HBase 有两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表， default 表是用户默认使用的命名空间。 Region 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需 要声明具体的列。这意味着，往 HBase 写入数据时，字段可以动态、按需指定。因此，和关 系型数据库相比，HBase 能够轻松应对字段变更的场景。 Row HBase 表中的每行数据都由一个 RowKey 和多个 Column（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重 要。 Column HBase 中的每个列都由 (Column Family) 列族 和 (Column Qualifier )列限定符进行限 定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。 Time Stamp 用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入 HBase 的时间。 Cell 由 {RowKey, Column Family：Column Qualifier, Time Stamp} 唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 基本架构 Region Server Region Server 为 Region 的管理者，其实现类为 HRegionServer，主要作用如下：对于数据的操作：get, put, delete； 对于 Region 的操作：splitRegion、compactRegion。 Master Master 是所有 Region Server 的管理者，其实现类为 HMaster，主要作用如下： 对于表的操作：create, delete, alter；对于 RegionServer的操作：分配 regions 到每个RegionServer，监控每个 RegionServer 的状态，负载均衡和故障转移。 Zookeeper HBase 通过 Zookeeper 来做 Master 的高可用、RegionServer 的监控、元数据的入口以及集群配置的维护等工作。 HDFS HDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高可用的支持。 安装部署 首先要保证 Zookeeper 集群和 Hadoop 集群正常部署并启动。 1234tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/modulevi hbase-env.sh export JAVA_HOME=/opt/module/jdk1.6.0_144 export HBASE_MANAGES_ZK=false # 关闭自带的 Zookeeper 12345678910111213141516171819202122232425&lt;!-- 修改 hbase-site.xml 配置文件 --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98 后的新变动，之前版本没有.port,默认端口为 60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- region servers --&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678# 软连接 hadoop 配置文件到 HBaseln -s /opt/module/hadoop2.7.2/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xmlln -s /opt/module/hadoop2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml# 利用脚本发送到其它集群 xsync hbase/ # 时间同步，否则会导致 regionserver 无法启动，抛出 ClockOutOfSyncException 异常 bin/start-hbase.sh # 可通过 16010 端口访问 HBase 管理页面 Shell基本操作123456# 进入 HBase 客户端命令行bin/hbase shell# 查看帮助命令hbase(main):001:0&gt; help# 查看当前数据库中有哪些表 hbase(main):002:0&gt; list 表操作1234567891011121314151617181920212223242526272829303132333435# 创建表，只需要初始化一个列族信息即可hbase(main):002:0&gt; create 'student','info'# 插入数据到表 hbase(main):003:0&gt; put 'student','1001','info:sex','male'hbase(main):004:0&gt; put 'student','1001','info:age','18'hbase(main):005:0&gt; put 'student','1002','info:name','Janna'hbase(main):006:0&gt; put 'student','1002','info:sex','female'hbase(main):007:0&gt; put 'student','1002','info:age','20'# 扫描查看表数据hbase(main):008:0&gt; scan 'student'hbase(main):009:0&gt; scan 'student',&#123;STARTROW=&gt;'1001', STOPROW=&gt;'1001'&#125;hbase(main):010:0&gt; scan 'student',&#123;STARTROW=&gt;'1001'&#125;# 查看表结构hbase(main):011:0&gt; describe ‘student’# 更新指定字段的数据hbase(main):012:0&gt; put 'student','1001','info:name','Nick'hbase(main):013:0&gt; put 'student','1001','info:age','100'hbase(main):014:0&gt; get 'student','1001'hbase(main):015:0&gt; get 'student','1001','info:name'# 统计表数据行数hbase(main):021:0&gt; count 'student'# 删除数据# 删除某 rowkey 的全部数据hbase(main):016:0&gt; deleteall 'student','1001'# 删除某 rowkey 的某一列数据hbase(main):017:0&gt; delete 'student','1002','info:sex'# 清空表数据hbase(main):018:0&gt; truncate 'student'# 删除表 需要先让该表为 disable 状态hbase(main):019:0&gt; disable 'student'# 然后才能 drop 这个表hbase(main):020:0&gt; drop 'student'# 变更表信息 将 info 列族中的数据存放 3 个版本hbase(main):022:0&gt; alter 'student',&#123;NAME=&gt;'info',VERSIONS=&gt;3&#125;hbase(main):022:0&gt; get 'student','1001',&#123;COLUMN=&gt;'info:name',VERSIONS=&gt;3&#125; 进阶结构原理 StoreFile 保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有 一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 MemStore 写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile。 WAL Write-Ahead Logging 由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件 中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 写流程 Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server； 访问对应的 Region Server，获取 hbase:meta 表，根据写请求的 namespace:table/rowkey， 查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以 及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问； 与目标 Region Server 进行通讯； 将数据顺序写入（追加）到 WAL； 将数据写入对应的 MemStore，数据会在 MemStore 进行排序； 向客户端发送 ack； 等达到 MemStore 的刷写时机后，将数据刷写到 HFile。 MemStore Flush 刷写时机 当某个 memstroe 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M）， 其所在 region 的所有 memstore 都会刷写。 当 memstore 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M） * hbase.hregion.memstore.block.multiplier（默认值 4） 时，会阻止继续往该 memstore 写数据。 当 region server 中 memstore 的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值 0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值 0.95）， region 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server 中所有 memstore 的总大小减小到上述值以下。 当 region server 中 memstore 的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值 0.4） 时，会阻止继续往所有的 memstore 写数据。 到达自动刷写的时间，也会触发 memstore flush。自动刷新的时间间隔由该属性进行配置，官方建议关闭此配置，hbase.regionserver.optionalcacheflushinterval（默认 1 小时）。 当 WAL 文件的数量超过 hbase.regionserver.max.logs，region 会按照时间顺序依次进行刷写，直到 WAL 文件数量减小到 hbase.regionserver.max.log 以下（该属性名已经废弃， 现无需手动设置，最大值为 32）。 读流程 Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server； 访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey， 查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以 及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问； 与目标 Region Server 进行通讯； 分别在 Block Cache（读缓存），MemStore 和 Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put / Delete）； 将从文件中查询到的数据块（Block，HFile 数据存储单元，默认大小为 64KB）缓存到 Block Cache； 将合并后的最终结果返回给客户端。 StoreFile Compaction由于 memstore 每次刷写都会生成一个新的 HFile，且同一个字段的不同版本（timestamp） 和不同类型（Put / Delete）有可能会分布在不同的 HFile 中，因此查询时需要遍历所有的 HFile。为了减少 HFile 的个数，以及清理掉过期和删除的数据，会进行 StoreFile Compaction。 Compaction 分为两种，分别是 Minor Compaction 和 Major Compaction。 Minor Compaction 会将临近的若干个较小的 HFile 合并成一个较大的 HFile，但不会清理过期和删除的数据。Major Compaction 会将一个 Store 下的所有的 HFile 合并成一个大 HFile，并且会清理掉过期 和删除的数据。 Region Split默认情况下，每个 Table 起初只有一个 Region，随着数据的不断写入，Region 会自动进行拆分。刚拆分时，两个子 Region 都位于当前的 Region Server，但处于负载均衡的考虑， HMaster 有可能会将某个 Region 转移给其他的 Region Server。 Region Split 时机： .当1个region中的某个 Store 下所有 StoreFile 的总大小超过 hbase.hregion.max.filesize， 该 Region 就会进行拆分（0.94 版本之前）。 当 1 个 region 中 的 某 个 Store 下所有 StoreFile 的 总 大 小 超 过Min(R^2 * “hbase.hregion.memstore.flush.size”,hbase.hregion.max.filesize”)，该 Region 就会进行拆分，其 中 R 为当前 Region Server 中属于该 Table 的个数（0.94 版本之后）。 API12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 123456789// 获取 Configuration 对象public static Configuration conf;static&#123; // 使用 HBaseConfiguration 的单例方法实例化 conf = HBaseConfiguration.create(); // 只需要配置 Zookeeper 的信息，因为 mate 都存在 Zookeeper 中 conf.set(\"hbase.zookeeper.quorum\", \"192.166.9.102\"); conf.set(\"hbase.zookeeper.property.clientPort\", \"2181\");&#125; 表操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119// 判断表石是否存在public static boolean isTableExist(String tableName) throws MasterNotRunningException,ZooKeeperConnectionException, IOException&#123; // 在 HBase 中管理、访问表需要先创建 HBaseAdmin 对象 ConnectionFactory.createConnection(conf); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName);&#125;// 创建表public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); // 判断表是否存在 if(isTableExist(tableName))&#123; System.out.println(\"表\" + tableName + \"已存在\"); &#125;else&#123; // 创建表属性对象，表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); // 创建多个列族 for(String cf : columnFamily)&#123; descriptor.addFamily(new HColumnDescriptor(cf)); &#125; // 根据对表的配置，创建表 admin.createTable(descriptor); System.out.println(\"表\" + tableName + \"创建成功！\"); &#125;&#125;// 删除表public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName))&#123; admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(\"表\" + tableName + \"删除成功！\"); &#125;else&#123; System.out.println(\"表\" + tableName + \"不存在！\"); &#125;&#125;// 向表中插入数据public static void addRowData( String tableName, String rowKey, String columnFamily, String column, String value) throws IOException&#123; // 创建 HTable 对象 HTable hTable = new HTable(conf, tableName); // 向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); // 向 Put 对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println(\"插入数据成功\");&#125;// 删除多行数据public static void deleteMultiRow(String tableName, String... rows) throws IOException&#123; HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows)&#123; Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); &#125; hTable.delete(deleteList); hTable.close();&#125;// 获取所有数据public static void getAllRows(String tableName) throws IOException&#123; HTable hTable = new HTable(conf, tableName); // 得到用于扫描 region 的对象 Scan scan = new Scan(); // 使用 HTable 得到 resultcanner 实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner)&#123; // 获取单元 Cell[] cells = result.rawCells(); for(Cell cell : cells)&#123; //得到 rowkey System.out.println(\" 行 键 :\" + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println(\" 列 族 \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\" 列 :\" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\" 值 :\" + Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125;&#125;// 获取某一行数据public static void getRow(String tableName, String rowKey) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(\" 行 键 :\" + Bytes.toString(result.getRow())); System.out.println(\" 列 族 \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\" 列 :\" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\" 值 :\" + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println(\"时间戳:\" + cell.getTimestamp()); &#125;&#125;// 获取某一行指定(列族:列)的数据public static void getRowQualifier( String tableName, String rowKey, String family, String qualifier) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family),Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(\" 行 键 :\" + Bytes.toString(result.getRow())); System.out.println(\" 列 族 \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\" 列 :\" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\" 值 :\" + Bytes.toString(CellUtil.cloneValue(cell))); &#125;&#125; MapReduce通过 HBase 的相关 Java API，我们可以实现伴随 HBase 操作的 MapReduce 过程，比如使用 MapReduce 将数据从本地文件系统导入到 HBase 的表中，比如我们从 HBase 中读取一些原 始数据后使用 MapReduce 做数据分析。 官方案例12345678910111213141516171819202122vi /etc/profile # export HBASE_HOME=/opt/module/hbase # export HADOOP_HOME=/opt/module/hadoop-2.7.2vi hadoop-env.sh # 注意：在 for 循环之后配 # export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*# 使用 MapReduce 将本地数据导入到 HBasevi fruit.tsv # 1001 Apple Red # 1002 Pear Yellow # 1003 Pineapple Yellow# 创建 Hbase 表Hbase(main):001:0&gt; create 'fruit','info'# 在 HDFS 中创建 input_fruit 文件夹并上传 fruit.tsv 文件/opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit//opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/# 执行 MapReduce 到 HBase 的 fruit 表中/opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jarimporttsv \\-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\hdfs://hadoop102:9000/input_fruit# 使用 scan 命令查看导入后的结果Hbase(main):001:0&gt; scan ‘fruit’ 自定义 将 fruit 表中的一部分数据，通过 MR 迁入到 fruit_mr 表中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// 继承的都是 HBase 提供的 Mapper 和 Reducerpublic class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; // 将 fruit 的 name 和 color 提取出来，相当于将每一行数据读取出来放入到 Put 对象中 Put put = new Put(key.get()); // 遍历添加 column 行 for(Cell cell: value.rawCells())&#123; // 添加/克隆列族:info if(\"info\".equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123; // 添加/克隆列：name if(\"name\".equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; // 将该列 cell 加入到 put 对象中 put.add(cell); // 添加/克隆列:color &#125;else if(\"color\".equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; // 向该列 cell 加入到 put 对象中 put.add(cell); &#125; &#125; &#125; // 将从 fruit 读取到的每行数据写入到 context 中作为 map 的输出 context.write(key, put); &#125;&#125;public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; // 读出来的每一行数据写入到 fruit_mr 表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125;public class Fruit2FruitMRRunner extends Configured implements Tool&#123; // 组装 Job public int run(String[] args) throws Exception &#123; // 得到 Configuration Configuration conf = this.getConf(); // 创建 Job 任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); // 配置 Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); // 设置 Mapper，注意导入的是 mapreduce 包下的，不是 mapred 包下的，后者是老版本 TableMapReduceUtil.initTableMapperJob( \"fruit\", // 数据源的表名 scan, // scan 扫描控制器 ReadFruitMapper.class, // 设置 Mapper 类 ImmutableBytesWritable.class, // 设置 Mapper 输出 key 类型 Put.class, // 设置 Mapper 输出 value 值类型 job // 设置给哪个 JOB ); // 设置 Reducer TableMapReduceUtil.initTableReducerJob(\"fruit_mr\", WriteFruitMRReducer.class, job); // 设置 Reduce 数量，最少 1 个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(\"Job running with error\"); &#125; return isSuccess ? 0 : 1; &#125;&#125;public static void main( String[] args ) throws Exception&#123; Configuration conf = HbaseConfiguration.create(); int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args); System.exit(status);&#125; 实现将 HDFS 中的数据写入到 Hbase 表中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 从 HDFS 中读取的数据 String lineValue = value.toString(); // 读取出来的每行数据使用 \\t 进行分割，存于 String 数组 String[] values = lineValue.split(\"\\t\"); // 根据数据中值的含义取值 String rowKey = values[0]; String name = values[1]; String color = values[2]; // 初始化 rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); // 初始化 put 对象 Put put = new Put(Bytes.toBytes(rowKey)); // 参数分别 列族、列、值 put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"name\"), Bytes.toBytes(name)); put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"color\"),Bytes.toBytes(color)); context.write(rowKeyWritable, put); &#125;&#125;public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException&#123; // 读出来的每一行数据写入到 fruit_hdfs 表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125;public class Txt2FruitRunner extends Configured implements Tool&#123; public int run(String[] args) throws Exception &#123; // 得到 Configuration Configuration conf = this.getConf(); // 创建 Job 任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Txt2FruitRunner.class); Path inPath = new Path(\"hdfs://hadoop102:9000/input_fruit/fruit.tsv\"); FileInputFormat.addInputPath(job, inPath); // 设置 Mapper job.setMapperClass(ReadFruitFromHDFSMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); // 设置 Reducer TableMapReduceUtil.initTableReducerJob(\"fruit_mr\", WriteFruitMRFromTxtReducer.class, job); // 设置 Reduce 数量，最少 1 个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(\"Job running with error\"); &#125; return isSuccess ? 0 : 1; &#125;&#125; 集成 Hive 操作 Hive 的同时对 HBase 也会产生影响，所以 Hive 需要持有操作 HBase 的 Jar。 1234567891011# 这里使用软链接的方式引包export HBASE_HOME=/opt/module/hbaseexport HIVE_HOME=/opt/module/hiveln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar $HIVE_HOME/lib/hbase-common-1.3.1.jarln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbaseserver-1.3.1.jarln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jarln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jarln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it1.3.1.jarln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jarln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jarln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar 1234567891011121314&lt;!-- 修改 hive-site.xml --&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt; 1234567891011121314151617181920212223242526272829303132333435# 在 Hive 中创建表同时关联 HBaseCREATE TABLE hive_hbase_emp_table( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key, info:ename, info:job, info:mgr,i nfo:hiredate,i nfo:sal, info:comm, info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\");# 完成之后，可以分别进入 Hive 和 HBase 查看，都生成了对应的表# 不能将数据直接 load 进 Hive 所关联 HBase 的那张表中CREATE TABLE emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)row format delimited fields terminated by '\\t';# 向 Hive 中间表中 load 数据load data local inpath '/home/admin/softwares/data/emp.txt' into table emp;# 通过 insert 命令将中间表中的数据导入到 Hive 关联 Hbase 的那张表中insert into table hive_hbase_emp_table select * from emp;# 查看 Hive 以及关联的 HBase 表中是否已经成功的同步插入了数据select * from hive_hbase_emp_table;scan ‘hbase_emp_table’ 在 HBase 中已经存储了某一张表 hbase_emp_table，然后在 Hive 中创建一个外部表来关联 HBase 中的 hbase_emp_table 这张表，使之可以借助 Hive 来分析 HBase 这张表中的数据。 12345678910111213141516CREATE EXTERNAL TABLE relevance_hbase_emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" =\":key, info:ename, info:job, info:mgr, info:hiredate, info:sal, info:comm, info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\");# 关联后就可以使用 Hive 函数进行一些分析操作了 select * from relevance_hbase_emp; 优化高可用在 HBase 中 HMaster 负责监控 HRegionServer 的生命周期，均衡 RegionServer 的负载， 如果 HMaster 挂掉了，那么整个 HBase 集群将陷入不健康的状态，并且此时的工作状态并 不会维持太久。所以 HBase 支持对 HMaster 的高可用配置。 123456789# 关闭 HBase 集群 bin/stop-hbase.sh# 在 conf 目录下创建 backup-masters 文件touch conf/backup-masters# 在 backup-masters 文件中配置高可用 HMaster 节点echo hadoop103 &gt; conf/backup-master# 将整个 conf 目录 scp 到其他节点scp -r conf/ hadoop103:/opt/module/hbase/scp -r conf/ hadoop104:/opt/module/hbase/ 预分区每一个 region 维护着 StartRow 与 EndRow，如果加入的数据符合某个 Region 维护的 RowKey 范围，则该数据交给这个 Region 维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高 HBase 性能。 123456# 手动设定预分区create 'staff1','info','partition1',SPLITS =&gt; ['1000','2000','3000','4000']# 生成 16 进制序列预分区create 'staff2','info','partition2',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125;# 按照文件中设置的规则预分区create 'staff3','partition3',SPLITS_FILE =&gt; 'splits.txt' 123456789// Java API// 自定义算法，产生一系列 hash 散列值存储在二维数组中byte[][] splitKeys = 某个散列值函数// 创建 HbaseAdmin 实例HBaseAdmin hAdmin = new HBaseAdmin(HbaseConfiguration.create());// 创建 HTableDescriptor 实例HTableDescriptor tableDesc = new HTableDescriptor(tableName);// 通过 HTableDescriptor 实例和散列值二维数组创建带有预分区的 Hbase 表hAdmin.createTable(tableDesc, splitKeys); RowKey一条数据的唯一标识就是 RowKey，那么这条数据存储于哪个分区，取决于 RowKey 处于哪个一个预分区的区间内，设计 RowKey 的主要目的 ，就是让数据均匀的分布于所有的 region 中，在一定程度上防止数据倾斜。 HASH，在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的 rowKey 来 Hash 后作为每个分区的临界值。 内存优化HBase 操作过程中需要大量的内存开销，毕竟 Table 是可以缓存在内存中的，一般会分配整个可用内存的 70%给 HBase 的 Java 堆。但是不建议分配非常大的堆内存，因为 GC 程持续太久会导致 RegionServer 处于长期不可用状态，一般 16~48G 内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 属性：dfs.support.append解释：开启 HDFS 追加同步，可以优秀的配合 HBase 的数据同步和持久化。默认值为 true。 属性：dfs.datanode.max.transfer.threads解释：HBase 一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作， 设置为 4096 或者更高。默认值：4096 属性：dfs.image.transfer.timeout解释：如果对于某一次数据操作来讲，延迟非常高，socket 需要等待更长的时间，建议把该值设置为更大的值（默认 60000 毫秒），以确保 socket 不会被 timeout 掉。 属性：mapreduce.map.output.compress mapreduce.map.output.compress.codec解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为 true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec 或者其他压缩方式。 属性：Hbase.regionserver.handler.count解释：默认值为 30，用于指定 RPC 监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 属性：hbase.hregion.max.filesize解释：默认值 10737418240（10GB），如果需要运行 HBase 的 MR 任务，可以减小此值， 因为一个 region 对应一个 map 任务，如果单个 region 过大，会导致 map 任务执行时间过长。该值的意思就是，如果 HFile 的大小达到这个数值，则这个 region 会被切分为两个 Hfile。 属性：hbase.client.write.buffer解释：用于指定 Hbase 客户端缓存，增大该值可以减少 RPC 调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少 RPC 次数的目的。 属性：hbase.client.scanner.caching解释：用于指定 scan.next 方法获取的默认行数，值越大，消耗内存越大。 flush、compact、split 机制 当 MemStore 达到阈值，将 Memstore 中的数据 Flush 进 Storefile；compact 机制则是把 flush 出来的小文件合并成大的 Storefile 文件。split 则是当 Region 达到阈值，会把过大的 Region 一分为二。 hbase.hregion.memstore.flush.size：134217728 这个参数的作用是当单个 HRegion 内所有的 Memstore 大小总和超过指定值时，flush 该 HRegion 的所有 memstore。RegionServer 的 flush 是通过将请求添加一个队列，模拟生 产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求 时，可能会导致内存陡增，最坏的情况是触发 OOM hbase.regionserver.global.memstore.upperLimit：0.4hbase.regionserver.global.memstore.lowerLimit：0.38当 MemStore 使用内存总量达到hbase.regionserver.global.memstore.upperLimit 指定值时，将会有多个 MemStores flush 到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到 MemStore 使用内存略小于 lowerLimit 项目实战 微博内容的浏览，数据库表设计；用户社交体现：关注用户，取关用户；拉取关注的人的微博内容。 表的创建 123456789101112131415161718192021222324252627282930313233343536373839// 创建命名空间以及表名的定义// 获取配置 confprivate Configuration conf = HbaseConfiguration.create();// 微博内容表的表名private static final byte[] TABLE_CONTENT = Bytes.toBytes(\"weibo:content\");// 用户关系表的表名private static final byte[] TABLE_RELATIONS = Bytes.toBytes(\"weibo:relations\");// 微博收件箱表的表名private static final byte[] TABLE_RECEIVE_CONTENT_EMAIL = Bytes.toBytes(\"weibo:receive_content_email\");public void initNamespace()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); // 命名空间类似于关系型数据库中的 schema，可以想象成文件夹 NamespaceDescriptor weibo = NamespaceDescriptor .create(\"weibo\") .addConfiguration(\"creator\", \"wingo\") .addConfiguration(\"create_time\", System.currentTimeMillis() + \"\") .build(); admin.createNamespace(weibo); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建微博内容表 12345678910111213141516171819202122232425262728293031323334353637383940414243/** 表结构Table Name weibo:contentRowKey 用户 ID_时间戳ColumnFamily infoColumnLabel 内容Version 1 个版本*/public void createTableContent()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); // 创建表表述 HTableDescriptor content = new HTableDescriptor(TableName.valueOf(TABLE_CONTENT)); // 创建列族描述 HColumnDescriptor info = new HColumnDescriptor(Bytes.toBytes(\"info\")); // 设置块缓存 info.setBlockCacheEnabled(true); // 设置块缓存大小 info.setBlocksize(2097152); // 设置压缩方式 // info.setCompressionType(Algorithm.SNAPPY); // 设置版本数量 info.setMaxVersions(1); info.setMinVersions(1); content.addFamily(info); admin.createTable(content); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建用户关系表 12345678910111213141516171819202122232425262728293031323334353637383940414243/** 表结构Table Name weibo:relationsRowKey 用户 IDColumnFamily attends、fansColumnLabel 关注用户 ID，粉丝用户 IDColumnValue 用户 IDVersion 1 个版本*/public void createTableRelations()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); HTableDescriptor relations = new HTableDescriptor(TableName.valueOf(TABLE_RELATIONS)); HColumnDescriptor attends = new HColumnDescriptor(Bytes.toBytes(\"attends\")); attends.setBlockCacheEnabled(true); attends.setBlocksize(2097152); attends.setMaxVersions(1); attends.setMinVersions(1); HColumnDescriptor fans = new HColumnDescriptor(Bytes.toBytes(\"fans\")); fans.setBlockCacheEnabled(true); fans.setBlocksize(2097152); fans.setMaxVersions(1); fans.setMinVersions(1); relations.addFamily(attends); relations.addFamily(fans); admin.createTable(relations); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建微博收件箱表 1234567891011121314151617181920212223242526272829303132333435363738/** 表结构Table Name weibo:receive_content_emailRowKey 用户 IDColumnFamily infoColumnLabel 用户 IDColumnValue 取微博内容的 RowKeyVersion 1000*/public void createTableReceiveContentEmail()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); HTableDescriptor receive_content_email = new HTableDescriptor(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); HColumnDescriptor info = new HColumnDescriptor(Bytes.toBytes(\"info\")); info.setBlockCacheEnabled(true); info.setBlocksize(2097152); info.setMaxVersions(1000); info.setMinVersions(1000); receive_content_email.addFamily(info);; admin.createTable(receive_content_email); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 功能实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/**微博内容表中添加 1 条数据微博收件箱表对所有粉丝用户添加数据：微博内容的 RowKey*/public class Message &#123; private String uid; private String timestamp; private String content; public String getUid() &#123; return uid; &#125; public void setUid(String uid) &#123; this.uid = uid; &#125; public String getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(String timestamp) &#123; this.timestamp = timestamp; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; @Override public String toString() &#123; return \"Message [uid=\" + uid + \", timestamp=\" + timestamp + \", content=\" + content + \"]\"; &#125;&#125;public void publishContent(String uid, String content)&#123; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); // 微博内容表中添加 1 条数据，首先获取微博内容表描述 HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT)); // 组装 Rowkey long timestamp = System.currentTimeMillis(); String rowKey = uid + \"_\" + timestamp; Put put = new Put(Bytes.toBytes(rowKey)); put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"content\"), timestamp, Bytes.toBytes(content)); contentTBL.put(put); // 向微博收件箱表中加入发布的 Rowkey // 查询用户关系表，得到当前用户有哪些粉丝 HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS)); // 取出目标数据 Get get = new Get(Bytes.toBytes(uid)); // 指定取出的目标数据的值 get.addFamily(Bytes.toBytes(\"fans\")); Result result = relationsTBL.get(get); // 初始化一个二进制数组用于存放数据 List&lt;byte[]&gt; fans = new ArrayList&lt;byte[]&gt;(); // 遍历取出当前发布微博的用户的所有粉丝数据 for(Cell cell : result.rawCells())&#123; fans.add(CellUtil.cloneQualifier(cell)); &#125; // 如果该用户没有粉丝，则直接 return if(fans.size() &lt;= 0) return; // 开始操作收件箱表 HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); for(byte[] fan : fans)&#123; Put fanPut = new Put(fan); fanPut.add(Bytes.toBytes(\"info\"), Bytes.toBytes(uid), timestamp, Bytes.toBytes(rowKey)); puts.add(fanPut); &#125; recTBL.put(puts); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != connection)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 添加关注用户 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/**在微博用户关系表中，对当前主动操作的用户添加新关注的好友在微博用户关系表中，对被关注的用户添加新的粉丝微博收件箱表中添加所关注的用户发布的微博*/public void addAttends(String uid, String... attends)&#123; // 参数过滤 if(attends == null || attends.length &lt;= 0 || uid == null || uid.length() &lt;= 0)&#123; return; &#125; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); // 用户关系表操作对象（连接到用户关系表） HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS)); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); // 在微博用户关系表中，添加新关注的好友 Put attendPut = new Put(Bytes.toBytes(uid)); for(String attend : attends)&#123; // 为当前用户添加关注的人 attendPut.add(Bytes.toBytes(\"attends\"), Bytes.toBytes(attend), Bytes.toBytes(attend)); // 为被关注的人，添加粉丝 Put fansPut = new Put(Bytes.toBytes(attend)); fansPut.add(Bytes.toBytes(\"fans\"), Bytes.toBytes(uid), Bytes.toBytes(uid)); // 将所有关注的人一个一个的添加到 puts（List）集合中 puts.add(fansPut); &#125; puts.add(attendPut); relationsTBL.put(puts); // 微博收件箱添加关注的用户发布的微博内容（content）的 rowkey HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT)); Scan scan = new Scan(); // 用于存放取出来的关注的人所发布的微博的 rowkey List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;(); for(String attend : attends)&#123; // 过滤扫描 rowkey，即：前置位匹配被关注的人的 uid_ RowFilter filter = new RowFilter (CompareFilter.CompareOp.EQUAL, new SubstringComparator(attend + \"_\")); // 为扫描对象指定过滤规则 scan.setFilter(filter); // 通过扫描对象得到 scanner ResultScanner result = contentTBL.getScanner(scan); // 迭代器遍历扫描出来的结果集 Iterator&lt;Result&gt; iterator = result.iterator(); while(iterator.hasNext())&#123; // 取出每一个符合扫描结果的那一行数据 Result r = iterator.next(); for(Cell cell : r.rawCells())&#123; // 将得到的 rowkey 放置于集合容器中 rowkeys.add(CellUtil.cloneRow(cell)); &#125; &#125; &#125; // 将取出的微博 rowkey 放置于当前操作用户的收件箱中 if(rowkeys.size() &lt;= 0) return; // 得到微博收件箱表的操作对象 HTableInterface recTBL =connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); // 用于存放多个关注的用户的发布的多条微博 rowkey 信息 List&lt;Put&gt; recPuts = new ArrayList&lt;Put&gt;(); for(byte[] rk : rowkeys)&#123; Put put = new Put(Bytes.toBytes(uid)); // uid_timestamp String rowKey = Bytes.toString(rk); // 截取 uid String attendUID = rowKey.substring(0, rowKey.indexOf(\"_\")); long timestamp = Long.parseLong(rowKey.substring(rowKey.indexOf(\"_\") + 1)); // 将微博 rowkey 添加到指定单元格中 put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(attendUID), timestamp, rk); recPuts.add(put); &#125; recTBL.put(recPuts); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != connection)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 取关用户 12345678910111213141516171819202122232425262728293031323334// 即关注用户的反向操作public void removeAttends(String uid, String... attends)&#123; // 过滤数据 if(uid == null || uid.length() &lt;= 0 || attends == null || attends.length &lt;= 0) return; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); // 在微博用户关系表中，删除已关注的好友 HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS)); // 待删除的用户关系表中的所有数据 List&lt;Delete&gt; deletes = new ArrayList&lt;Delete&gt;(); // 当前取关操作者的 uid 对应的 Delete 对象 Delete attendDelete = new Delete(Bytes.toBytes(uid)); // 遍历取关，同时每次取关都要将被取关的人的粉丝 -1 for(String attend : attends)&#123; attendDelete.deleteColumn(Bytes.toBytes(\"attends\"), Bytes.toBytes(attend)); Delete fansDelete = new Delete(Bytes.toBytes(attend)); fansDelete.deleteColumn(Bytes.toBytes(\"fans\"), Bytes.toBytes(uid)); deletes.add(fansDelete); &#125; deletes.add(attendDelete); relationsTBL.delete(deletes); // 收件箱表中删除取关的人的微博 rowkey HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); Delete recDelete = new Delete(Bytes.toBytes(uid)); for(String attend : attends)&#123; recDelete.deleteColumn(Bytes.toBytes(\"info\"), Bytes.toBytes(attend)); &#125; recTBL.delete(recDelete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 获取关注的人的微博内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 即通过微博收件箱中的 RowKey 获取对应的微博内容public List&lt;Message&gt; getAttendsContent(String uid)&#123; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); // 从收件箱中取得微博 rowKey Get get = new Get(Bytes.toBytes(uid)); // 设置最大版本号 get.setMaxVersions(5); List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;(); Result result = recTBL.get(get); for(Cell cell : result.rawCells())&#123; rowkeys.add(CellUtil.cloneValue(cell)); &#125; // 根据取出的所有 rowkey 去微博内容表中检索数据 HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT)); List&lt;Get&gt; gets = new ArrayList&lt;Get&gt;(); // 根据 rowkey 取出对应微博的具体内容 for(byte[] rk : rowkeys)&#123; Get g = new Get(rk); gets.add(g); &#125; // 得到所有的微博内容的 result 对象 Result[] results = contentTBL.get(gets); List&lt;Message&gt; messages = new ArrayList&lt;Message&gt;(); for(Result res : results)&#123; for(Cell cell : res.rawCells())&#123; Message message = new Message(); String rowKey = Bytes.toString(CellUtil.cloneRow(cell)); String userid = rowKey.substring(0, rowKey.indexOf(\"_\")); String timestamp = rowKey.substring(rowKey.indexOf(\"_\") + 1); String content = Bytes.toString(CellUtil.cloneValue(cell)); message.setContent(content); message.setTimestamp(timestamp); message.setUid(userid); messages.add(message); &#125; &#125; return messages; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"算法整理","slug":"技术原理/十大排序算法（内部排序）","date":"2020-04-30T14:22:10.000Z","updated":"2020-07-10T02:56:24.973Z","comments":true,"path":"2020/04/30/技术原理/十大排序算法（内部排序）/","link":"","permalink":"http://yoursite.com/2020/04/30/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E5%86%85%E9%83%A8%E6%8E%92%E5%BA%8F%EF%BC%89/","excerpt":"一些常用的算法整理。","text":"一些常用的算法整理。 十大排序算法插入排序 - 直接插入排序基本思想 将一个记录插入到已排序好的有序表中，从而得到一个新，记录数增1的有序表。即：先将序列的第1个记录看成是一个有序的子序列，然后从第2个记录逐个进行插入，直至整个序列有序为止 如果碰见一个和插入元素相等的，那么插入元素把想插入的元素放在相等元素的后面。所以，相等元素的前后顺序没有改变，从原无序序列出去的顺序就是排好序后的顺序，所以插入排序是稳定的 时间复杂度：O(n^2) 排序示例 初始关键字，此时有序表只有一个数字 49 (49) 38 65 97 76 12 27 49 将 38 插入有序表中进行排，此时有序表只有两个数字 (38 49) 65 97 76 12 27 49 循环此过程 (38 49 65) 97 76 12 27 49 (38 49 65 97) 76 12 27 49 (38 49 65 76 97) 12 27 49 (12 38 49 65 76 97) 27 49 (12 27 38 49 65 76 97) 49 (12 27 38 49 49 65 76 97) 算法实现12345678910111213141516171819202122232425262728293031/** * 插入排序 * @param data 要排序的数组 * @param reverse 从大到小(false)还是从小到大(ture) */public static void sort(int[] data, boolean reverse) &#123; if (data.length == 1) &#123; return; //长度为 1 的数组已有序 &#125; int tmp = 0; //临时存储变量 for (int i = 1; i &lt; data.length; i**) &#123; tmp = data[i]; int n = i - 1; for (; n &gt;= 0; n--) &#123; if (reverse) &#123; //从小到大排序 if (data[n] &gt;= tmp) &#123; data[n + 1] = data[n]; //将大于当前值的数后移一个位置 &#125; else &#123; break; //已有序，无需继续比较 &#125; &#125; else &#123; //从大到小排序 if (data[n] &lt;= tmp) &#123; data[n + 1] = data[n]; //将小于当前值的数后移一个位置 &#125; else &#123; break; &#125; &#125; &#125; data[n+1] = tmp; &#125;&#125; 插入排序 - 希尔排序基本思想 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序 希尔排序时效分析很难，关键码的比较次数与记录移动次数依赖于增量因子序列 d 的选取，特定情况下可以准确估算出关键码的比较次数和记录的移动次数 时间复杂度：O(n^2) 操作思想 选择一个增量序列 n/2, n/4, …, 1 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量，将待排序列分割成若干长度的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度 排序示例 初始关键字（增量为 10/2 = 5） 49 38 65 97 76 13 27 49 55 04 根据增量划分子序列，并对子序列进行排序 49 13 -&gt; 13 49 38 27 -&gt; 27 38 65 49 -&gt; 49 65 97 55 -&gt; 55 97 76 04 -&gt; 04 76 第一躺排序结果 13 27 49 55 04 49 38 65 97 76 根据增量划分子序列（增量为 10/4 ≈ 3） 13 55 38 76 -&gt; 13 38 55 76 27 04 65 -&gt; 04 27 65 49 49 97 -&gt; 49 49 97 第二躺排序结果 13 04 49 38 27 49 55 65 97 76 第三躺排序结果 04 13 27 38 49 49 55 65 76 97 算法实现12345678910111213141516171819202122232425262728293031323334/** * 希尔排序 * @param data 要排序的数组 * @param reverse 从大到小(false)还是从小到大(ture) */public static void sort(int[] data, boolean reverse) &#123; if (data.length == 1) &#123; return; &#125; for (int d = data.length / 2; d &gt;= 1; d = d / 2) &#123; //组大小 for (int k = 0; k &lt; d; k**) &#123; //多少组 for (int n = d + k; n &lt; data.length; n = n + d) &#123; //同一组 int cur = n; while (cur - d &gt;= 0) &#123; //插入排序 int tmp = 0; if (reverse) &#123; //小到大(ture) if (data[cur] &lt;= data[cur - d]) &#123; tmp = data[cur]; data[cur] = data[cur - d]; data[cur - d] = tmp; &#125; &#125; else &#123; //从大到小(false) if (data[cur] &gt;= data[cur - d]) &#123; tmp = data[cur]; data[cur] = data[cur - d]; data[cur - d] = tmp; &#125; &#125; cur = cur - d; &#125; &#125; &#125; &#125;&#125; 选择排序 - 直接选择排序基本思想 在要排序的一组数中，选出最小（或者最大）的一个数与第1个位置的数交换；然后在剩下的数当中再找最小（或者最大）的与第2个位置的数交换，依次类推，直到第n-1个元素（倒数第二个数）和第n个元素（最后一个数）比较为止 时间复杂度：O(n^2) 不稳定 排序示例 初始值 ()3 1 5 7 2 4 9 6 无序序列中找出最小的值放入有序序列 (1) 3 5 7 2 4 9 6 重复上一步骤 (1 2) 3 5 7 4 9 6 (1 2 3) 5 7 4 9 6 (1 2 3 4) 5 7 9 6 (1 2 3 4 5) 7 9 6 (1 2 3 4 5 6) 7 9 (1 2 3 4 5 6 7) 9 (1 2 3 4 5 6 7 9) 算法实现123456789101112131415public static void selectSort(int array[])&#123; for(int i = 0; i &lt; array.length-1; i**)&#123; int minIndex = i; //取无序序列的第一个值为最小值（标识位） for(int j = i+1; j &lt; array.length-1; j**)&#123; //比较选择最小值 if(array[j] &lt; array[minIndex])&#123; minIndex = j; &#125; &#125; if(minIndex != i)&#123; //只有当无序序列的第一个数不是最小值时才进行交换 int temp = array[i]; array[i] = array[minIndex]; array[minIndex] = temp; &#125; &#125;&#125; 选择排序 - 堆排序基本思想 堆是一种特殊的树形数据结构，其每个节点都是一个值，通常提到的堆是指一颗完全二叉树，根节点的值小于（或大于）两个子节点的值，且根节点的两个字数也分别是一个堆 逻辑结构：树形结构 存储结构：顺序存储（数组） 时间复杂度：O(nlog2n) 不稳定 两个关键问题 将一个无序序列构成一个堆 输出顶堆元素后，调整剩余元素成为一个新堆 排序示例 1 6 7 2 3 4 5 左子树和右子树都符合最大堆 根元素所在的数并不符合，且 7 &gt; 6 &gt; 1 将 7 与 1 交换位置 7 6 1 2 3 4 5 此时右子树不符合最大堆，且 5 &gt; 4 &gt; 1 将 5 与 1 交换位置 7 6 5 2 3 4 1 此时建堆操作完成，此序列的最大值就在堆的根节点上 随后将堆顶最大值和数组最后的元素进行替换，我们就完成了一趟排序了 1 6 5 2 3 4 (7) 重复上述过程直到序列排序完成 算法实现12345678910111213141516171819202122232425262728293031323334353637383940/** * 建堆 * * @param arrays 看作是完全二叉树 * @param currentRootNode 当前父节点位置 * @param size 节点总数 */public static void heapify(int[] arrays, int currentRootNode, int size) &#123; if (currentRootNode &lt; size) &#123; //左子树和右字数的位置 int left = 2 * currentRootNode + 1; int right = 2 * currentRootNode + 2; //把当前父节点位置看成是最大的 int max = currentRootNode; if (left &lt; size) &#123; //如果比当前根元素要大，记录它的位置 if (arrays[max] &lt; arrays[left]) &#123; max = left; &#125; &#125; if (right &lt; size) &#123; //如果比当前根元素要大，记录它的位置 if (arrays[max] &lt; arrays[right]) &#123; max = right; &#125; &#125; //如果最大的不是根元素位置，那么就交换 if (max != currentRootNode) &#123; int temp = arrays[max]; arrays[max] = arrays[currentRootNode]; arrays[currentRootNode] = temp; //继续比较，直到完成一次建堆 heapify(arrays, max, size); &#125; &#125;&#125; 显然，一个普通的数组并不能有这种条件(父&gt;子)，因此，我们往往是从数组最后一个元素来进行建堆 1234567891011/** * 完成一次建堆，最大值在堆的顶部(根节点) */public static void maxHeapify(int[] arrays, int size) &#123; // 从数组的尾部开始，直到第一个元素(角标为0) for (int i = size - 1; i &gt;= 0; i--) &#123; heapify(arrays, i, size); &#125;&#125; 完成第一次建堆之后，我们会发现最大值会在数组的首位 接下来不断建堆，然后让数组最后一位与当前堆顶(数组第一位)进行交换即可排序 1234567891011for (int i = 0; i &lt; arrays.length; i**) &#123; //每次建堆就可以排除一个元素了（其实可以不用每次都从最后一个元素进行建堆） maxHeapify(arrays, arrays.length - i); //交换 int temp = arrays[0]; arrays[0] = arrays[(arrays.length - 1) - i]; arrays[(arrays.length - 1) - i] = temp;&#125; 交换排序 - 冒泡排序基本思想 两个数比较大小，较大的数下沉，较小的数冒起来 时间复杂度：O(n^2) 排序过程 比较相邻的两个数据，如果第二个数小，就交换位置 从后向前两两比较，一直到比较最前两个数据。最终最小数被交换到起始的位置，这样第一个最小数的位置就排好了 继续重复上述过程，依次将第2.3…n-1个最小数排好位置 算法实现1234567891011121314public static void BubbleSort(int [] arr)&#123; int temp;//临时变量 for(int i=0; i&lt;arr.length-1; i**)&#123; //表示趟数，一共arr.length-1次。 for(int j=arr.length-1; j&gt;i; j--)&#123; if(arr[j] &lt; arr[j-1])&#123; temp = arr[j]; arr[j] = arr[j-1]; arr[j-1] = temp; &#125; &#125; &#125;&#125; 交换排序 - 快速排序基本思想 先从数列中取出一个数作为 key 值 将比这个数小的数全部放在它的左边，大于或等于它的数全部放在它的右边 对左右两个小数列重复第二步，直至各区间只有 1 个数 时间复杂度：O(n^2) 不稳定 排序示例 ()：low指针位置 []：high指针位置 初始无序序列 (23) 46 0 8 11 [18] 用一个临时变量存储基准数据 key = 23 首先从后半部分开始 如果扫描到的值大于基准数据就让high减1 如果发现有元素比该基准数据的值小，就将high位置的值赋值给low位置 第 1 次扫描后的序列（high 指针指向的值 18 &lt; key） (18) 46 0 8 11 [18] 然后开始从前往后扫描 如果扫描到的值小于基准数据就让low加1 如果发现有元素大于基准数据的值，就再将low位置的值赋值给high位置的值,指针移动并且数据交换 第 2 次扫描后的序列（18 &lt; key） 18 (46) 0 8 11 [18] 第 3 次扫描后的序列（46 &gt; key） 18 (46) 0 8 11 [46] 第 4 次扫描后的序列（46 &gt; key） 18 (46) 0 8 [11] 46 第 5 次扫描后的序列（11 &lt; key） 18 (11) 0 8 [11] 46 第 6 次扫描后的序列（11 &lt; key） 18 11 (0) 8 [11] 46 第 7 次扫描后的序列（0 &lt; key） 18 11 0 (8) [11] 46 第 8 次扫描后的序列（8 &lt; key） 18 11 0 8 ([11]) 46 赋 key 值 18 11 0 8 ([23]) 46 算法实现123456789101112131415161718192021222324252627282930313233343536public static int getIndex(int[] arr, int low, int high) &#123; // 基准数据 int tmp = arr[low]; while (low &lt; high) &#123; // 当队尾的元素大于等于基准数据时,向前挪动high指针 while (low &lt; high &amp;&amp; arr[high] &gt;= tmp) &#123; high--; &#125; // 如果队尾元素小于tmp了,需要将其赋值给low arr[low] = arr[high]; // 当队首元素小于等于tmp时,向前挪动low指针 while (low &lt; high &amp;&amp; arr[low] &lt;= tmp) &#123; low**; &#125; // 当队首元素大于tmp时,需要将其赋值给high arr[high] = arr[low]; &#125; // 跳出循环时low和high相等,此时的low或high就是tmp的正确索引位置 // 由原理部分可以很清楚的知道low位置的值并不是tmp,所以需要将tmp赋值给arr[low] arr[low] = tmp; return low; // 返回tmp的正确位置&#125;private static void quickSort(int[] arr, int low, int high) &#123; if (low &lt; high) &#123; // 找寻基准数据的正确索引 int index = getIndex(arr, low, high); // 进行迭代对index之前和之后的数组进行相同的操作使整个数组变成有序 quickSort(arr, low, index - 1); quickSort(arr, index + 1, high); &#125;&#125; 归并排序基本思想 是利用递归与分治的技术将数据序列划分为越来越小的半子表，再对半子表排序，最后再用递归方法将排好序的半子表合并成越来越大的有序序列 时间复杂度：O(nlog2n) 算法实现12345678910111213141516171819202122232425262728293031323334public static void mergeSort(int[] arr) &#123; sort(arr, 0, arr.length - 1);&#125;public static void sort(int[] arr, int L, int R) &#123; if(L == R) &#123; return; &#125; int mid = L + ((R - L) &gt;&gt; 1); sort(arr, L, mid); sort(arr, mid + 1, R); merge(arr, L, mid, R);&#125;public static void merge(int[] arr, int L, int mid, int R) &#123; int[] temp = new int[R - L + 1]; int i = 0; int p1 = L; int p2 = mid + 1; // 比较左右两部分的元素，哪个小，把那个元素填入temp中 while(p1 &lt;= mid &amp;&amp; p2 &lt;= R) &#123; temp[i**] = arr[p1] &lt; arr[p2] ? arr[p1**] : arr[p2**]; &#125; // 上面的循环退出后，把剩余的元素依次填入到temp中 // 以下两个while只有一个会执行 while(p1 &lt;= mid) &#123; temp[i**] = arr[p1**]; &#125; while(p2 &lt;= R) &#123; temp[i**] = arr[p2**]; &#125; // 把最终的排序的结果复制给原数组 for(i = 0; i &lt; temp.length; i**) &#123; arr[L + i] = temp[i]; &#125;&#125; 基数排序基本思想 BinSort想法非常简单，首先创建数组A[MaxValue]；然后将每个数放到相应的位置上（例如17放在下标17的数组位置）；最后遍历数组，即为排序后的结果 基数排序是在BinSort的基础上，通过基数的限制来减少空间的开销 排序示例 初始序列（基数为 10 ） 27 91 01 97 17 23 84 28 72 05 67 25 第 1 次排序：对 (元素 % 10) 进行排序 91 01 72 23 84 05 25 27 97 17 67 28 第 2 次排序：对 (元素 / 10) 进行排序 01 05 17 23 25 27 28 67 72 84 91 97 算法实现12345678910111213141516171819202122232425262728293031public static void radixSort(int[] array,int d)&#123; int n=1;//代表位数对应的数：1,10,100... int k=0;//保存每一位排序后的结果用于下一位的排序输入 int length=array.length; int[][] bucket=new int[10][length];//排序桶用于保存每次排序后的结果，这一位上排序结果相同的数字放在同一个桶里 int[] order=new int[length];//用于保存每个桶里有多少个数字 while(n&lt;d) &#123; for(int num:array) //将数组array里的每个数字放在相应的桶里 &#123; int digit=(num/n)%10; bucket[digit][order[digit]]=num; order[digit]**; &#125; for(int i=0;i&lt;length;i**)//将前一个循环生成的桶里的数据覆盖到原数组中用于保存这一位的排序结果 &#123; if(order[i]!=0)//这个桶里有数据，从上到下遍历这个桶并将数据保存到原数组中 &#123; for(int j=0;j&lt;order[i];j**) &#123; array[k]=bucket[i][j]; k**; &#125; &#125; order[i]=0;//将桶里计数器置0，用于下一次位排序 &#125; n*=10; k=0;//将k置0，用于下一轮保存位排序结果 &#125;&#125;","categories":[{"name":"技术原理","slug":"技术原理","permalink":"http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"算法整理","slug":"算法整理","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/"}]},{"title":"MySQL 常用命令","slug":"开发杂项/MySQL 常用命令","date":"2020-04-30T05:33:19.000Z","updated":"2020-07-10T04:15:45.074Z","comments":true,"path":"2020/04/30/开发杂项/MySQL 常用命令/","link":"","permalink":"http://yoursite.com/2020/04/30/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/MySQL%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"MySQL 常用命令汇总。","text":"MySQL 常用命令汇总。 DDL（Data Definition Language）DML（Data Manipulation Language）DCL（Data Control Language）TCL（Transaction Control Language） Shell 操作12345678910111213&#x2F;* 启动MySQL *&#x2F;net start mysql&#x2F;* 连接与断开服务器 *&#x2F;mysql -h 地址 -P 端口 -u 用户名 -p 密码&#x2F;* 跳过权限验证登录MySQL *&#x2F;mysqld --skip-grant-tables-- 修改 root 密码 密码加密函数 password()update mysql.user set password&#x3D;password(&#39;root&#39;);SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示所有配置参数，配合 like 以及 % 可进行查询某一参数 导入导出12345678910111213141516171819202122232425262728293031323334353637383940414243&#x2F;* 备份与还原 *&#x2F;-- 备份，将数据的结构与表内数据保存起来。-- 利用 mysqldump 指令完成。-- 导出-- 导出一张表mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:&#x2F;a.sql)-- 导出多张表mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:&#x2F;a.sql)-- 导出所有表mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:&#x2F;a.sql)-- 导出一个库 mysqldump -u用户名 -p密码 -B 库名 &gt; 文件名(D:&#x2F;a.sql)-- 可以 -w 携带备份条件-- 导入-- 在登录 mysql 的情况下：source 备份文件-- 在不登录的情况下mysql -u用户名 -p密码 库名 &lt; 备份文件&#x2F;* 导入导出 *&#x2F;select * into outfile 文件地址 [控制格式] from 表名; -- 导出表数据load data [local] infile 文件地址 [replace|ignore] into table 表名 [控制格式]; -- 导入数据 生成的数据默认的分隔符是制表符 local未指定，则数据文件必须在服务器上 replace 和 ignore 关键词控制对现有的唯一键记录的重复的处理 -- 控制格式fields 控制字段格式默认：fields terminated by &#39;\\t&#39; enclosed by &#39;&#39; escaped by &#39;\\\\&#39; terminated by &#39;string&#39; -- 终止 enclosed by &#39;char&#39; -- 包裹 escaped by &#39;char&#39; -- 转义 -- 示例： SELECT a,b,a+b INTO OUTFILE &#39;&#x2F;tmp&#x2F;result.text&#39; FIELDS TERMINATED BY &#39;,&#39; OPTIONALLY ENCLOSED BY &#39;&quot;&#39; LINES TERMINATED BY &#39;\\n&#39; FROM test_table;lines 控制行格式默认：lines terminated by &#39;\\n&#39; terminated by &#39;string&#39; -- 终止 权限操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#x2F;* 用户和权限管理 *&#x2F; ------------------用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &#39;user_name&#39;@&#39;192.168.1.1&#39; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD &#x3D; PASSWORD(&#39;密码&#39;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 &#x3D; PASSWORD(&#39;密码&#39;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限&#x2F;添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &#39;password&#39;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 数据库操作1234567891011121314151617-- 查看当前数据库select database();-- 显示当前时间、用户名、数据库版本select now(), user(), version();-- 创建库create database [if not exists] 数据库名 数据库选项-- 数据库选项： -- CHARACTER SET charset_name 指定数据库采用的字符集 -- COLLATE collation_name 指定数据库字符集的比较方式（默认 utf8_general_ci）-- 查看已有库show databases [like &#39;pattern&#39;]-- 查看当前库信息show create database 数据库名-- 修改库的选项信息alter database 库名 选项信息-- 删除库，同时删除该数据库相关的目录及其目录内容drop database [if exists] 数据库名 表操作CREATE12345678910111213141516171819202122232425262728293031323334-- 创建表create [temporary] table [if not exists] [库名.]表名 (表的结构定义) [表选项] -- 每个字段必须有数据类型 -- 最后一个字段后不能有逗号 -- temporary 临时表，会话结束时表自动消失 -- 对于字段的定义： -- 字段名 数据类型 -- [NOT NULL | NULL] -- [DEFAULT default_value] -- [AUTO_INCREMENT] -- [UNIQUE [KEY] | [PRIMARY] KEY] -- [COMMENT &#39;string&#39;] -- 表选项 -- 字符集，如果表没有设定，则使用数据库字符集 CHARSET &#x3D; charset_name -- 存储引擎 ENGINE &#x3D; engine_name -- 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 -- 常见的引擎：InnoDB MyISAM Memory&#x2F;Heap BDB Merge Example CSV MaxDB Archive -- 不同的引擎在保存表的结构和数据时采用不同的方式 -- MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 -- InnoDB表文件含义：.frm表定义，表空间数据和日志文件 &#x2F;* 列属性（列约束） *&#x2F;primary key -- 能唯一标识记录的字段，可以作为主键。 -- 一个表只能有一个主键。 -- 主键具有唯一性。 -- 声明字段时，用 primary key 标识。 -- 也可以在字段列表之后声明 -- 例：create table tab ( id int, stu varchar(10), primary key (id)); -- 主键字段的值不能为null。 -- 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 -- 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age)); 列属性1234567891011121314151617181920212223242526272829303132333435363738unique -- 唯一索引（唯一约束）-- 使得某字段的值不能重复。null -- 约束-- null 不是数据类型，是列的一个属性。-- 表示当前列是否可以为 null，表示什么都没有。 -- null, 允许为空。默认。 -- not null, 不允许为空。default -- 默认值属性，当前字段的默认值。insert into tab values (default, &#39;val&#39;); -- 此时表示强制使用默认值。create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。-- 常用默认值 current_date, current_timeauto_increment -- 自动增长约束-- 自动增长必须为索引（主键或unique），只能存在一个字段为自动增长。-- 默认为 1 开始自动增长。可以通过表属性 auto_increment &#x3D; x 进行设置。-- 或 alter table tbl auto_increment &#x3D; x。comment -- 注释-- 例：create table tab (id int) comment &#39;注释内容&#39;;foreign key -- 外键约束，用于限制主表与从表数据完整性。alter table t1 add constraint &#96;t1_t2_fk&#96; foreign key (t1_id) references t2(id);-- 将表 t1 的 t1_id 外键关联到表 t2 的 id 字段。-- 每个外键都有一个名字，可以通过 constraint 指定-- 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。-- 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。-- MySQL中，可以对 InnoDB 引擎使用外键约束：foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作]-- 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为 null，前提是该外键列，没有 not null。-- 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。-- 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： -- cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 -- set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 -- restrict，拒绝父表删除和更新。-- 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范12345678910111213-- Normal Format, NF -- 每个表保存一个实体信息 -- 每个具有一个 ID 字段作为主键 -- ID 主键 + 原子表-- 1NF, 第一范式 -- 字段不能再分，就满足第一范式。-- 2NF, 第二范式 -- 满足第一范式的前提下，不能出现部分依赖。 -- 消除符合主键就可以避免部分依赖。增加单列关键字。-- 3NF, 第三范式 -- 满足第二范式的前提下，不能出现传递依赖。 -- 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 -- 将一个实体信息的数据放在一个表内实现。 SHOW12345678910111213141516SHOW ENGINES -- 显示存储引擎的状态信息SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 数据文件目录 DATA DIRECTORY &#x3D; &#39;目录&#39; -- 索引文件目录 INDEX DIRECTORY &#x3D; &#39;目录&#39; -- 表注释 COMMENT &#x3D; &#39;string&#39; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表SHOW TABLES [LIKE &#39;pattern&#39;]SHOW TABLES FROM 表名-- 查看表结构SHOW CREATE TABLE 表名 &#x2F; DESC 表名 &#x2F; DESCRIBE 表名 &#x2F; EXPLAIN 表名 &#x2F; SHOW COLUMNS FROM 表名 [LIKE &#39;PATTERN&#39;]SHOW TABLE STATUS [FROM db_name] [LIKE &#39;pattern&#39;] ALTER12345678910111213141516171819-- 修改表-- 修改表本身的选项 ALTER TABLE 表名 表的选项 -- EG: ALTER TABLE 表名 ENGINE&#x3D;MYISAM;-- 修改表的字段结构ALTER TABLE 表名 操作名 -- 操作名 ADD [COLUMN] 字段名 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名) -- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP [COLUMN] 字段名 -- 删除字段 MODIFY [COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名 CHANGE [COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其 AUTO_INCREMENT 属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键 SELECT1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556select [all|distinct] select_expr from -&gt; where -&gt; group by [合计函数] -&gt; having -&gt; order by -&gt; limit-- select_expr 可以用 * 表示所有字段。select * from tb;-- 可以使用表达式（计算公式、函数调用、字段也是个表达式）select stu, 29+25, now() from tb;-- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。-- 使用 as 关键字，也可省略 as.select stu+10 as add10 from tb;from -- 子句 用于标识查询来源。-- 可以为表起别名。使用as关键字。select * from tb1 as tt, tb2 as bb;-- from子句后，可以同时出现多个表。-- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。select * from tb1, tb2;where -- 子句 -- 从 from 获得的数据源中进行筛选。 -- 整型 1 表示真，0 表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符：&#x3D;, &lt;&#x3D;&gt;, &lt;&gt;, !&#x3D;, &lt;&#x3D;, &lt;, &gt;&#x3D;, &gt;, !, &amp;&amp;, ||, -- in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor -- is&#x2F;is not 加上 ture &#x2F; false &#x2F; unknown 检验某个值的真假 -- &lt;&#x3D;&gt; 与 &lt;&gt; 功能相同，&lt;&#x3D;&gt; 可用于null比较group by -- 子句, 分组子句group by 字段&#x2F;别名 [排序方式]-- 分组后会进行排序。升序：ASC，降序：DESC-- 以下 [合计函数] 需配合 group by 使用：group by 相当于给合计函数划定合计范围，否则统计全局数据 -- count 返回不同的非NULL值数目 count(*)、count(字段) -- sum 求和 -- max 求最大值 -- min 求最小值 -- avg 求平均值 -- group_concat 返回带有来自一个组的连接的非 NULL 值的字符串结果。组内字符串连接。having -- 子句，条件子句，与 where 功能、用法相同，执行时机不同。 -- where 在开始时执行检测数据，对原数据进行过滤。 -- having 对筛选出的结果再次进行过滤。 -- having 字段必须是查询出来的，where 字段必须是数据表存在的。 -- where 不可以使用字段的别名，having 可以。因为执行 WHERE 代码时，可能尚未确定列值。 -- where 不可以使用合计函数。一般需用合计函数才会用 having -- SQL 标准要求 HAVING 必须引用 GROUP BY 子句中的列或用于合计函数中的列。order by -- 子句，排序子句order by 排序字段&#x2F;别名 排序方式 [,排序字段&#x2F;别名 排序方式]...-- 升序：ASC，降序：DESC 支持多个字段的排序。limit -- 子句，限制结果数量子句-- 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从 0 开始。limit 起始位置, 获取条数-- 省略第一个参数，表示从索引 0 开始。limit 获取条数distinct -- distinct 去除重复记录，默认为 all, 全部记录 UNION12345678&#x2F;* UNION *&#x2F;-- 将多个 select 查询的结果组合成一个结果集合。SELECT ... UNION [ALL|DISTINCT] SELECT ...-- 默认 DISTINCT 方式，即所有返回的行都是唯一的。-- 建议，对每个SELECT查询加上小括号包裹。-- ORDER BY 排序时，需加上 LIMIT 进行结合。-- 需要各 select 查询的字段数量一样。-- 每个 select 查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条 select 语句为准。 子查询1234567891011121314151617181920212223242526272829&#x2F;* 子查询 *&#x2F;-- 子查询需用括号包裹。-- from型 -- from 后要求是一个表，必须给子查询结果取个别名。 -- 简化每个查询内的条件。 -- from 型需将结果生成一个临时表格，可用以原表的锁定的释放。 -- 子查询返回一个表，表型子查询。select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 -- 子查询返回一个值，标量子查询。 -- 不需要给子查询取别名。 -- where 子查询内的表，不能直接用以更新。select * from tb where money &#x3D; (select max(money) from tb);-- 列子查询 -- 如果子查询结果返回的是一列。 -- 使用 in 或 not in 完成查询 -- exists 和 not exists 条件 -- 如果子查询返回数据，则返回 1 或 0。常用于判断条件。select column1 from t1 where exists (select * from t2);-- 行子查询 查询条件是一个行。select * from t1 where (id, gender) in (select id, gender from t2);-- 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...)-- 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。-- 特殊运算符!&#x3D; all() -- 相当于 not in&#x3D; some() -- 相当于 in，any 是 some 的别名!&#x3D; some() -- 不等同于 not in，不等于其中某一个。all, some -- 可以配合其他运算符一起使用。 连接查询1234567891011121314151617181920212223242526&#x2F;* 连接查询(join) *&#x2F; -- 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) -- 默认就是内连接，可省略inner。 -- 只有数据存在时才能发送连接。即连接结果不能出现空行。 -- on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） -- 也可用 where 表示连接条件。 -- 还有 using, 但需字段名相同。 using (字段名)-- 交叉连接 cross join 即，没有条件的内连接。select * from tb1 cross join tb2;-- 外连接(outer join) 如果数据不存在，也会出现在连接结果中。-- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充-- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 -- 相当于省略了using，会自动查找相同字段名。natural joinnatural left joinnatural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num &#x3D; extra_info.stu_id; INSERT1234567891011121314151617181920&#x2F;* insert *&#x2F;select语句获得的数据可以用insert插入。可以省略对列的指定，要求 values () 括号内，提供给了按照列顺序出现的所有字段的值。 或者使用set语法。 insert into tbl_name set field&#x3D;value,...；可以一次性使用多个值，采用(), (), ();的形式。 insert into tbl_name values (), (), ();可以在列值指定时，使用表达式。 insert into tbl_name values (field_value, 10+10, now());可以使用一个特殊值 default，表示该列使用默认值。 insert into tbl_name values (field_value, default);可以通过一个查询的结果，作为需要插入的值。 insert into tbl_name select ...;可以指定在插入的值出现主键（或唯一索引）冲突时，更新其他非主键列的信息。 insert into tbl_name values&#x2F;set&#x2F;select on duplicate key update 字段&#x3D;值, …; DELETE123456789101112131415161718&#x2F;* delete *&#x2F;DELETE FROM tbl_name [WHERE where_definition] [ORDER BY ...] [LIMIT row_count]-- 按照条件删除-- 指定删除的最多记录数 Limit-- 可以通过排序条件删除 order by + limit-- 支持多表删除，使用类似连接语法。delete from 需要删除数据多表1，表2 using 表连接操作 条件。&#x2F;* truncate *&#x2F;TRUNCATE [TABLE] tbl_name-- 清空数据-- 删除重建表-- 区别： 1，truncate 是删除表再创建，delete 是逐条删除 2，truncate 重置 auto_increment 的值。而 delete 不会 3，truncate 不知道删除了几条，而 delete 知道。 4，当被用于带分区的表时，truncate 会保留分区 Other12345678910111213141516171819202122232425262728-- 对表进行重命名RENAME TABLE 原表名 TO 新表名RENAME TABLE 原表名 TO 库名.表名 -- 可将表移动到另一个数据库-- 删除表 DROP TABLE [IF EXISTS] 表名-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...&#x2F;* 表维护 *&#x2F;-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option &#x3D; &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作123456789101112131415-- 增INSERT [INTO] 表名 [(字段列表)] VALUES [(值列表)] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录 -- REPLACE 与 INSERT 完全一样，可互换。INSERT [INTO] 表名 SET 字段名&#x3D;值[, 字段名&#x3D;值, ...]-- 查SELECT 字段列表 FROM 表名 [其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用 * 代替，表示所有字段-- 删 DELETE FROM 表名 [删除条件子句]-- 改 UPDATE 表名 SET 字段名&#x3D;新值[, 字段名&#x3D;新值] [更新条件] 字符集编码123456789101112131415-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &#39;character_set_%&#39; -- 查看所有字符集编码项 -- character_set_client 客户端向服务器发送数据时使用的编码 -- character_set_results 服务器端将结果返回给客户端所使用的编码 -- character_set_connection 连接层编码-- SET 变量名 &#x3D; 变量值set character_set_client &#x3D; gbk;set character_set_results &#x3D; gbk;set character_set_connection &#x3D; gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序SHOW CHARACTER SET [LIKE &#39;pattern&#39;] &#x2F; SHOW CHARSET [LIKE &#39;pattern&#39;] -- 查看所有字符集SHOW COLLATION [LIKE &#39;pattern&#39;] -- 查看所有校对集-- charset 字符集编码 设置字符集编码-- collate 校对集编码 设置校对集编码 数值类型整型12345678910111213类型 字节 范围（有符号位）tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255smallint 2字节 -32768 ~ 32767mediumint 3字节 -8388608 ~ 8388607int 4字节bigint 8字节int(M) -- M表示总位数-- 默认存在符号位，unsigned 属性修改-- 显示宽度，如果某个数不够定义字段时设置的位数，则前面以 0 补填，zerofill 属性修改int(5) -- 插入一个数&#39;123&#39;，补填后为&#39;00123&#39;-- 在满足要求的情况下，越小越好。-- 1 表示 bool 值真，0 表示 bool 值假。MySQL 没有布尔类型，通过整型 0 和 1 表示。常用 tinyint(1) 表示布尔型。 浮点型1234567891011121314类型 字节float(单精度) 4字节double(双精度) 8字节-- 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。-- 不同于整型，前后均会补填0.-- 定义浮点型时，需指定总位数和小数位数。float(M, D) double(M, D)-- M 表示总位数，D 表示小数位数。-- M 和 D 的大小会决定浮点数的范围。不同于整型的固定范围。-- M 既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。-- 支持科学计数法表示。-- 浮点数表示近似值。 定点数1234decimal -- 可变长度decimal(M, D) -- M也表示总位数，D表示小数位数。-- 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。-- 将浮点数转换为字符串来保存，每 9 位数字保存为 4 个字节。 字符串1234567891011121314151617181920212223242526-- a. char, varchar ----------char -- 定长字符串，速度快，但浪费空间varchar -- 变长字符串，速度慢，但节省空间-- M 表示能存储的最大长度，此长度是字符数，非字节数。-- 不同的编码，所占用的空间不同。 -- char 最多 255 个字符，与编码无关。 -- varchar 最多 65535 字符，与编码有关。-- 一条有效记录最大不能超过65535个字节。-- utf8 最大为 21844 个字符，gbk 最大为 32766 个字符，latin1 最大为 65532 个字符-- varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于 255 位，则采用一个字节来保存长度，反之需要两个字节来保存。-- varchar 的最大有效长度由最大行大小和使用的字符集确定。-- 最大有效长度是 65532 字节，因为在 varchar 存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是 64432-1-2&#x3D;65532 字节。-- 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset&#x3D;utf8; 问 N 的最大值是多少？ -- 答：MySQL定义行的长度不能超过65535，(65535-1-2-4-30*3)&#x2F;3-- b. blob, text ----------blob -- 二进制字符串（字节字符串）tinyblob, blob, mediumblob, longblobtext -- 非二进制字符串（字符字符串）tinytext, text, mediumtext, longtexttext -- 在定义时，不需要定义长度，也不会计算总长度。text -- 类型在定义时，不可给 default 值-- c. binary, varbinary ------------ 类似于 char 和 varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。-- char, varchar, text 对应 binary, varbinary, blob. 日期类型123456789101112-- 一般用整型保存时间戳，可以很方便的将时间戳进行格式化。datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59date 3字节 日期 1000-01-01 到 9999-12-31timestamp 4字节 时间戳 1970-01-01 00:00:00 到 2038-01-19 03:14:07time 3字节 时间 -838:59:59 到 838:59:59year 1字节 年份 1901 - 2155datetime “YYYY-MM-DD hh:mm:ss”timestamp “YYYY-MM-DD hh:mm:ss”date “YYYY-MM-DD”time “hh:mm:ss”year “YYYY” 枚举和集合1234567891011121314-- 枚举(enum) ----------enum(val1, val2, val3...)-- 在已知的值中进行单选。最大数量为 65535。-- 枚举值在保存时，以 2 个字节的整型 (smallint) 保存。每个枚举值，按保存的位置顺序，从 1 开始逐一递增。-- 表现为字符串类型，存储却是整型。-- NULL 值的索引是 NULL。-- 空字符串错误值的索引值是 0。-- 集合（set） ----------set(val1, val2, val3...)create table tab ( gender set(&#39;男&#39;, &#39;女&#39;, &#39;无&#39;) );insert into tab values (&#39;男, 女&#39;);-- 最多可以有 64 个不同的成员。以 bigint 存储，共 8 个字节。采取位运算的形式。-- 当创建表时，SET 成员值的尾部空格将自动被删除。 内置函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-- 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) &#x3D; 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) &#x3D; 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) &#x3D; 11floor(x) -- 向下取整 floor (10.1) &#x3D; 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3&#x3D;1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&#39;yyyy-mm-dd hh:ii:ss&#39;); -- 获取日期部分time(&#39;yyyy-mm-dd hh:ii:ss&#39;); -- 获取时间部分date_format(&#39;yyyy-mm-dd hh:ii:ss&#39;, &#39;%d %y %a %d %m %b %j&#39;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 字符连接函数concat(str1[, str2,...])-- 其他常用函数md5();default(); 视图123456789101112131415161718192021222324252627282930313233343536&#x2F;* 视图 *&#x2F;什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的 sql 语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM &#x3D; &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用 select 语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过 ALGORITHM 指定。 - column_list 如果存在，则数目必须等于 SELECT 语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name -- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#x2F;* 事务(transaction) *&#x2F; ------------------事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续 SQL 的集体成功或集体撤销。 - 事务是数据库在数据完整性方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB 被称为事务安全型引擎。-- 事务开启 开启事务后，所有被执行的 SQL 语句均被认作当前事务内的 SQL 语句。 START TRANSACTION; &#x2F; BEGIN;-- 事务提交 COMMIT;-- 事务回滚 如果部分操作发生问题，映射到事务开启前。 ROLLBACK;-- 事务的特性 ACID 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用 InnoDB 的自动提交 (autocommit) 特性完成。 普通的 MySQL 执行语句后，当前的数据提交操作均可被其他客户端可见；而事务是暂时关闭“自动提交”机制，需要 commit 提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB 自动提交特性设置 SET autocommit &#x3D; 0|1; 0 表示关闭自动提交，1 表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要 commit 提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与 START TRANSACTION 不同的是， - SET autocommit 是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) - 而 START TRANSACTION 记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 触发器12345678910111213141516171819202122&#x2F;* 触发器 *&#x2F;-- 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt tbl_name：监听的表，必须是永久性的表，不能将触发程序与 TEMPORARY 表或视图关联起来。 trigger_time 是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event 指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用 BEGIN...END 复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name-- 触发器可以使用 old 和 new 代替旧的和新的数据。 更新操作，更新前是 old，更新后是 new。 删除操作，只有 old。 增加操作，只有 new。-- 注意：对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。 编程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115&#x2F;* SQL编程 *&#x2F;-- 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个 default 子句。值可以被指定为一个表达式，不需要为一个常数。如果没有 default 子句，初始值为 null。 -- 赋值 使用 set 和 select into 语句为变量赋值。 注意：在函数内是可以使用全局变量（用户自定义的变量）-- 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var &#x3D; value;也可以使用 select into 语句为变量初始化并赋值。这样要求 select 语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过 select 执行完成。此时为了避免 &#x3D; 被当作关系运算符看待，使用 :&#x3D; 代替。（set 语句可以使用 &#x3D; 和 :&#x3D;）。select @var:&#x3D;20;select @v1:&#x3D;id, @v2&#x3D;name from t1 limit 1;select * from tbl_name where @var:&#x3D;30;select into -- 可以将表中查询获得的数据赋给变量。select max(height) into @max_height from tb;-- 自定义变量名为了避免 select 语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var&#x3D;10;-- 变量被定义后，在整个会话周期都有效（登录到退出）-- 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];-- 如果需要在循环内提前终止 while 循环，则需要使用标签；标签需要成对出现。-- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环 -- 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用 db_name.funciton_name 的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的 mysql 语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &#39;partten&#39; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--&#x2F;&#x2F; 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。&#x2F;* 存储过程 *&#x2F;存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意 - 没有返回值。 - 只能单独调用，不可夹杂在其他语句中。-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 杂项12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert/* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \\'5. CMD命令行内的语句结束符可以为 \";\", \"\\G\", \"\\g\"，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\\c","categories":[{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"Hive","slug":"大数据/Hive","date":"2020-04-28T03:27:12.000Z","updated":"2020-07-10T03:39:21.927Z","comments":true,"path":"2020/04/28/大数据/Hive/","link":"","permalink":"http://yoursite.com/2020/04/28/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/","excerpt":"Hive 基本介绍及简单使用。","text":"Hive 基本介绍及简单使用。 基本概念Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。 本质上是将 HQL 转化为 MapReduce 程序。 Hive 处理的数据存储在 HDFS；Hive 分析数据底层的实现是 MapReduce；执行程序运行在 Yarn 上 HQL VS SQL Hive RDBMS 查询语言 HQL SQL 数据存储 HDFS LOCAL FS 执行 MapReduce Executor 执行延迟 高 低 处理数据规模 大 小 索引 位图索引 复杂索引 用户接口 Client：CLI（hive shell）、JDBC / ODBC（java 访问 hive）、WEBUI（浏览器访问 hive）。 元数据 Metastore：元数据包括表名、表所属的数据（默认是 default）、表的拥有者、列 / 分区字段、表的类型（是否是外部表）、表的数据所在目录等；默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore。 Hadoop使用 HDFS 进行存储，使用 MapReduce 进行计算。 驱动器 Driver解析器 SQL Parser：将 SQL 字符串转换成抽象语法树 AST；编译器 Physical Plan： 将 AST 编译生成逻辑执行计划；优化器 Query Optimizer：对逻辑执行计划进行优化；执行器 Execution：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来 说，就是 MR / Spark。 Hive 通过给用户提供的一系列交互接口，接收到用户的指令（SQL），使用自己的 Driver， 结合元数据（MetaStore），将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。 安装配置1234567891011121314# hive 安装tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/mv apache-hive-1.2.1-bin/ hivemv hive-env.sh.template hive-env.shvi hive-env.sh # export HADOOP_HOME=/opt/module/hadoop-2.7.2 # export HIVE_CONF_DIR=/opt/module/hive/conf# 启动 hive 前必须启动集群sbin/start-dfs.shsbin/start-yarn.sh# 创建目录bin/hadoop fs -mkdir -p /user/hive/warehousebin/hadoop fs -chmod g+w /user/hive/warehouse# 或者直接在配置文件中关闭权限检查 123456&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 基本操作1234567891011bin/hive# 进入 hive shellhive&gt; show databases;hive&gt; use default;hive&gt; show tables;hive&gt; create table student(id int, name string);hive&gt; show tables;hive&gt; desc student;hive&gt; insert into student values(1000,\"ss\");hive&gt; select * from student;hive&gt; quit; 文件导入1234# 将 /opt/module/datas/student.txt 文件导入 hive 的 student(id int, name string) 表中hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';hive&gt; load data local inpath '/opt/module/datas/student.txt' into table student; 再打开一个客户端窗口启动 hive，会产生 java.sql.SQLException 异常。原因是，Metastore 默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore。 MySQL 安装1234567891011121314151617181920212223242526272829303132# 检查环境rpm -qa|grep mysqlrpm -e --nodepsunzip mysql-libs.zip # MySQL-client-5.6.24-1.el6.x86_64.rpm # mysql-connector-java-5.1.27.tar.gz 驱动包 # MySQL-server-5.6.24-1.el6.x86_64.rpm# 安装服务rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm# 获取随机密码cat /root/.mysql_secret# 检查服务状态service mysql statusservice mysql start# 安装客户端rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm# 利用好随机密码进行登录mysql -uroot -pOEXaQuS8IWkG19Xs# 修改密码mysql&gt;SET PASSWORD=PASSWORD('000000');# 修改 user 表中的主机配置mysql&gt;show databases;mysql&gt;use mysql;mysql&gt;show tables;mysql&gt;desc user;mysql&gt;select User, Host, Password from user;mysql&gt;update user set host='%' where host='localhost';mysql&gt;delete from user where Host='127.0.0.1';mysql&gt;delete from user where Host='::1';# 配置生效mysql&gt;flush privileges;mysql&gt;quit; 元数据配置12# 拷贝所需驱动cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/ 123456789101112131415161718192021222324252627&lt;!-- 在 /opt/module/hive/conf 目录下创建一个 hive-site.xml --&gt;&lt;!-- 拷贝官方文档的配置参数 --&gt;&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://[ip_address]:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 查看 MySQL 数据库，显示增加了 metastore 数据库。 交互命令1234567891011121314# 查看帮助bin/hive -help# 不进入 hive 的交互窗口执行 sql 语句bin/hive -e \"select id from student;\"# 执行脚本中 sql 语句bin/hive -f /opt/module/datas/hivef.sql# 执行脚本，并写出结果bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt# 在 hive cli 命令窗口中查看 hdfs 文件系统hive(default)&gt;dfs -ls /;# 在 hive cli 命令窗口中查看本地文件系统hive(default)&gt;! ls /opt/module/datas;# 查看在 hive 中输入的所有历史命令，用户家目录下cat .hivehistory 属性配置仓库路径Default 数据仓库的最原始位置是在 hdfs 上的：/user/hive/warehouse 路径下； 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。 改 default 数据仓库原始位置（将 hive-default.xml.template 如下配置信息拷贝到 hive-site.xml 文件中） 12345678910111213141516&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;!-- 显示数据表头信息 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 显示当前数据库名称信息 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 12# 配置同组用户有执行权限bin/hdfs dfs -chmod g+w /user/hive/warehouse 日志信息12345678910# 方式一mv hive-log4j.properties.template hive-log4j.propertiesvi hive-log4j.properties # hive.log.dir=/opt/module/hive/logs# 方式二，仅对本次 hive 启动有效bin/hive -hiveconf hive.log.dir=/opt/module/hive/logs# 方式三，仅对本次 hive 启动有效hive (default)&gt; set hive.log.dir=/opt/module/hive/logs;# 查看所有参数配置hive (default)&gt; set [某一参数] 数据类型简单类型 类型 描述 示例 boolean true / false TRUE tinyint 1 字节的有符号整数 -128~127 1Y smallint 2 个字节的有符号整数，-32768~32767 1S int 4 个字节的带符号整数 1 bigint 8 字节带符号整数 1L float 4 字节单精度浮点数 1.0 double 8 字节双精度浮点数 1.0 deicimal 任意精度的带符号小数 1.0 String 字符串，变长 “a”,’b’ varchar 变长字符串 “a”,’b’ char 固定长度字符串 “a”,’b’ binary 字节数组 无法表示 timestamp 时间戳，纳秒精度 122327493795 date 日期 ‘2018-04-07’ 复杂类型 类型 描述 示例 array 有序的的同类型的集合 array(1,2) map key-value，key 必须为原始类型，value 可以任意类型 map(‘a’,1,’b’,2) struct 字段集合,类型可以不同 struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) 实例操作123456789101112&#123; \"name\": \"songsong\", \"friends\": [\"bingbing\" , \"lili\"] , // 列表 Array \"children\": &#123; // 键值 Map \"xiao song\": 18 , \"xiaoxiao song\": 19 &#125; \"address\": &#123; // 结构 Struct, \"street\": \"hui long guan\" , \"city\": \"beijing\"&#125;&#125; 基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing &#x2F;&#x2F; 第一条 JSONyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing &#x2F;&#x2F; 第二条 JSON MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用 “_”。 1234567891011create table test( name string, friends array&lt;string&gt;, children map&lt;string, int&gt;, address struct&lt;street:string, city:string&gt;)row format delimitedfields terminated by ','collection items terminated by '_'map keys terminated by ':'lines terminated by '\\n'; 12345678# 导入数据hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’ into table test# 访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式select friends[1],children['xiao song'],address.city from test where name=\"songsong\";OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s) 类型转化Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式 使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如， 某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST 操作。 隐式类型转换规则： 任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT；所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE；TINYINT、SMALLINT、INT 都可以转换为 FLOAT；BOOLEAN 类型不可以转换为任何其它的类型。 可以使用 CAST 操作显示进行数据类型转换 例如 CAST(‘1’ AS INT) 将把字符串 ‘1’ 转换成整数 1；如果强制类型转换失败，如执行 CAST(‘X’ AS INT)，表达式返回空值 NULL。 DDL Data Definition Language 数据定义。 库操作创建 创建一个数据库，数据库在 HDFS 上的默认存储路径是 /user/hive/warehouse/*.db 123create database if not exists db_hive;# 指定库存储路径create database db_hive location '/db_hive.db'; 显示123456# 显示数据库信息desc database db_hive;# 显示数据库详细信息 extendeddesc database extended db_hive;# 切换当前数据库use db_hive; 修改用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。 数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。修改当前正在使用的数据库，要先退出使用. 1alter database db_hive set dbproperties(&#39;createtime&#39;&#x3D;&#39;20170830&#39;); 删除1234# 采用 if exists 判断数据库是否存在drop database if exists [db_name];# 如果数据库不为空，可以采用 cascade 命令强制删除drop database [db_name] cascade; 表操作表类型123456789101112CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path]# 重命名ALTER TABLE table_name RENAME TO new_table_name EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）。 Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 PARTITIONED 表示根据某一个 key （不在 create table 里面）对数据进行分区，体现在 HDFS 上就是 table 目录下有 n 个不同的分区文件夹（country=China,country=USA）。 CLUSTERED 对于每一个表（table）或者分区， Hive 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive 也是针对某一列进行桶的组织。Hive 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。使取样（sampling）更高效。 ROW FORMAT 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。SerDe 是 Serialize / Deserilize 的简称，目的是用于序列化和反序列化。 STORED AS 指定存储文件类型。常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、 RCFILE（列式存储格式文件） 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩， 使用 STORED AS SEQUENCEFILE。 LOCATION 指定表在 HDFS 上的存储位置。 LIKE 允许用户复制现有的表结构，但是不复制数据。 内部表 默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项 hive.metastore.warehouse.dir（例如，/user/hive/warehouse）所定义的目录的子目录下。当删除一个管理表时，Hive 也会删除这个表中数据。内部表不适合和其他工具共享数据。 12345678910111213# 创建内部表create table if not exists student(id int, name string)row format delimited fields terminated by '\\t'stored as textfilelocation '/user/hive/warehouse/student';# 根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student01 as select id, name from student;# 根据已经存在的表结构创建表create table if not exists student02 like student;# 查询表的类型desc formatted student01 外部表 因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这 份数据，不过描述表的元数据信息会被删除掉。 每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础 上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT 进入内部表。 12345678910111213141516171819202122232425262728293031323334# 创建外部表部门表create external table if not exists default.dept( deptno int, dname string, loc int)row format delimited fields terminated by '\\t';# 创建外部表员工表create external table if not exists default.emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)row format delimited fields terminated by '\\t';# 查询表结构hive (default)&gt; show tables;OKtab_namedeptemp# 向外部表中导入数据hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept;hive (default)&gt; load data local inpath '/opt/module/datas/emp.txt' into table default.emp;# 查询结果hive (default)&gt; select * from emp;hive (default)&gt; select * from dept;# 查看表的类型hive (default)&gt; desc formatted dept;Table Type: EXTERNAL_TABLE 表转换 只能用单引号，严格区分大小写，如果不是完全符合，那么只会添加 K V 而不生效。 1234567891011# 查询表的类型hive (default)&gt; desc formatted student;Table Type: MANAGED_TABLE# 修改内部表 student 为外部表alter table student set tblproperties('EXTERNAL'='TRUE');hive (default)&gt; desc formatted student;Table Type: EXTERNAL_TABLE# 修改外部表 student 为内部表alter table student set tblproperties('EXTERNAL'='FALSE');hive (default)&gt; desc formatted student;Table Type: MANAGED_TABLE 分区表 分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 一级分区 12345678910111213141516171819202122232425262728293031323334353637383940# 引入分区表（需要根据日期对日志进行管理）/user/hive/warehouse/log_partition/20170702/20170702.log/user/hive/warehouse/log_partition/20170703/20170703.log/user/hive/warehouse/log_partition/20170704/20170704.log# 创建分区表语法hive (default)&gt; create table dept_partition( deptno int, dname string, loc string)partitioned by (month string)row format delimited fields terminated by '\\t';# 分区域导入数据hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into tabledefault.dept_partition partition(month='201709');hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into tabledefault.dept_partition partition(month='201708');hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into tabledefault.dept_partition partition(month='201707’);# 单分区查询hive (default)&gt; select * from dept_partition where month='201709';# 多分区联合查询 union（排序） or in 三种方式hive (default)&gt; select * from dept_partition where month='201709'unionselect * from dept_partition where month='201708'unionselect * from dept_partition where month='201707';# 增加分区hive (default)&gt; alter table dept_partition add partition(month='201706') ;# 同时创建多个分区 用空格分开hive (default)&gt; alter table dept_partition add partition(month='201705') partition(month='201704');# 删除单个分区hive (default)&gt; alter table dept_partition drop partition (month='201704');# 同时删除多个分区 用逗号分开hive (default)&gt; alter table dept_partition drop partition (month='201705'), partition (month='201706');# 查看分区表有多少分区hive&gt; show partitions dept_partition;# 查看分区表结构hive&gt; desc formatted dept_partition; # Partition Information # col_name data_type # comment month string 二级分区 1234567891011121314151617181920212223242526272829303132333435# 创建二级分区表hive (default)&gt; create table dept_partition2( deptno int, dname string, loc string)partitioned by (month string, day string)row format delimited fields terminated by '\\t';# 加载数据到二级分区表中hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition2 partition(month='201709', day='13');hive (default)&gt; select * from dept_partition2 where month='201709' and day='13';# 把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式# 方式一：上传数据后修复# 上传数据hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; # 查询数据（查询不到刚上传的数据）hive (default)&gt; select * from dept_partition2 where month='201709' and day='12'; # 执行修复命令hive&gt; msck repair table dept_partition2; # 再次查询数据hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';# 方式二：上传数据后添加分区# 上传数据hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11;hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11; # 执行添加分区hive (default)&gt; alter table dept_partition2 add partition(month='201709',day='11'); # 查询数据hive (default)&gt; select * from dept_partition2 where month='201709' and day='11';# 方式三：上传数据后 load 数据到分区# 创建目录hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; # 上传数据hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table dept_partition2 partition(month='201709',day='10');# 查询数据hive (default)&gt; select * from dept_partition2 where month='201709' and day='10'; 列信息1234# 更新列ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)# ADD 是代表新增一字段，字段位置在所有列后面（partition 列前），REPLACE 则是表示替换表中所有字段 删除表1hive (default)&gt; drop table [table_name]; DML数据导入123456789# 创建一张表hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by '\\t';# 加载本地文件到 hivehive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table default.student;# 加载 HDFS 文件到 hive 中# 上传文件到 HDFShive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;# 加载 HDFS 上的数据hive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' into table default.student; 加载数据覆盖表中已有的数据。 1234# 上传文件到 HDFShive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;# 加载数据覆盖表中已有的数据hive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' overwrite into table default.student; 通过查询语句向表中插入数据（Insert）。 12345678910111213# 创建一张分区表hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\\t';# 基本插入数据hive (default)&gt; insert into table student partition(month='201709') values(1,'wangwu');# 基本模式插入（根据单张表查询结果）hive (default)&gt; insert overwrite table student partition(month='201708') select id, name from student where month='201709';# 多插入模式（根据多张表查询结果）hive (default)&gt; from student insert overwrite table student partition(month='201707') select id, name where month='201709' insert overwrite table student partition(month='201706') select id, name where month='201709'; 查询语句中创建表并加载数据（As Select）。 12# 根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student_new as select id, name from student; 创建表时通过 Location 指定加载数据路径. 12345678910# 创建表，并指定在 hdfs 上的位置hive (default)&gt; create table if not exists student( id int, name string)row format delimited fields terminated by '\\t'location '/user/hive/warehouse/student';# 上传数据到 hdfs 上hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student;# 查询数据hive (default)&gt; select * from student; Import 数据（export 到处的数据）到指定 Hive 表中。 1hive (default)&gt; import table student partition(month='201709') from '/user/hive/warehouse/export/student'; 数据导出 Insert 导出。 123456# 将查询的结果导出到本地hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student' select * from student;# 将查询的结果格式化导出到本地hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student;# 将查询的结果导出到 HDFS 上（没有 local）hive (default)&gt; insert overwrite directory '/user/wingo/student' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student; Hadoop 命令导出到本地。 1hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student.txt; Hive Shell 命令导出。 1bin/hive -e 'select * from default.student;' &gt; /opt/module/datas/export/student.txt; Export 导出到 HDFS 上。 1hive (default) &gt;export table default.student to '/user/hive/warehouse/export/student'; 清除表 Truncate 只能删除管理表，不能删除外部表中数据。 1hive (default)&gt; truncate table student; 查询 基本查询 123456# 全表查询hive (default)&gt; select * from emp;# 选择特定列查询hive (default)&gt; select empno, ename from emp;# 列别名hive (default)&gt; select ename AS name, deptno dn from emp; 运算符查询 12# 算术运算符：查询出所有员工的薪水后加 1 显示hive (default)&gt; select sal+1 from emp; 函数查询 12345678910# 求总行数（count）hive (default)&gt; select count(*) cnt from emp;# 求工资的最大值（max）hive (default)&gt; select max(sal) max_sal from emp;# 求工资的最小值（min）hive (default)&gt; select min(sal) min_sal from emp;# 求工资的总和（sum）hive (default)&gt; select sum(sal) sum_sal from emp;# 求工资的平均值（avg）hive (default)&gt; select avg(sal) avg_sal from emp; Limit 语句 典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。 1hive (default)&gt; select * from emp limit 5; Where12# 查询出薪水大于 1000 的所有员工hive (default)&gt; select * from emp where sal &gt;1000; 比较运算符（Between/In/ Is Null），这些操作符同样可以用于 JOIN…ON 和 HAVING 语句中。 12345678# 查询出薪水等于 5000 的所有员工hive (default)&gt; select * from emp where sal =5000;# 查询工资在 500 到 1000 的员工信息hive (default)&gt; select * from emp where sal between 500 and 1000;# 查询 comm 为空的所有员工信息hive (default)&gt; select * from emp where comm is null;# 查询工资是 1500 和 5000 的员工信息hive (default)&gt; select * from emp where sal IN (1500, 5000); Like 和 RLike % 代表零个或多个字符（任意个字符）；_ 代表一个字符RLIKE 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件。 123456# 查找以 2 开头薪水的员工信息hive (default)&gt; select * from emp where sal LIKE '2%';# 查找第二个数值为 2 的薪水的员工信息hive (default)&gt; select * from emp where sal LIKE '_2%';# 查找薪水中含有 2 的员工信息hive (default)&gt; select * from emp where sal RLIKE '[2]'; 逻辑运算符（And/Or/Not） 123456# 查询薪水大于 1000，部门是 30hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;# 查询薪水大于 1000，或者部门是 30hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;# 查询除了 20 部门和 30 部门以外的员工信息hive (default)&gt; select * from emp where deptno not IN(30, 20); 分组 GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。 1234# 计算 emp 表每个部门的平均工资hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;# 计算 emp 每个部门中每个岗位的最高薪水hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job; Having 语句 where 针对表中的列发挥作用，查询数据；Having 针对查询结果中的列发挥作用， 筛选数据。 12# 求每个部门的平均薪水大于 2000 的部门hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000; Join 是只支持等值连接，不支持非等值连接 123456789101112# 根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门编号；hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno;# 合并员工表和部门表hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;# 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;# 左外连接：JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;# 右外连接：JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;# 满外连接：将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno; 多表连接，大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。Hive 总是按照从左到右的 顺序执行的。 排序全局排序 Order By：全局排序，一个 MapReduce。 ASC（ascend）: 升序（默认）；DESC（descend）: 降序。 12345678# 查询员工信息按工资升序排列hive (default)&gt; select * from emp order by sal;# 查询员工信息按工资降序排列hive (default)&gt; select * from emp order by sal desc;# 按照别名排序，按照员工薪水的 2 倍排序hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;# 按照部门和工资升序排序hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ; 内部排序 每个 MapReduce 内部排序（Sort By）。 Sort By：每个 MapReduce 内部进行排序，对全局结果集来说不是排序。 1234567# 设置 reduce 个数hive (default)&gt; set mapreduce.job.reduces=3;# 查看设置 reduce 个数hive (default)&gt; set mapreduce.job.reduces;# 根据部门编号降序查看员工信息hive (default)&gt; select * from emp sort by empno desc;hive (default)&gt; insert overwrite local directory '/opt/module/datas/sortby-result' select * from emp sort by deptno desc; 分区排序 Distribute By：类似 MR 中 partition，进行分区，结合 sort by 使用。 Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前 1234# 先按照部门编号分区，再按照员工编号降序排序。hive (default)&gt; set mapreduce.job.reduces=3;hive (default)&gt; insert overwrite local directory '/opt/module/datas/distribute-result' select * from emp distribute by deptno sort by empno desc; Cluster By 除了具有 Distribute By 的功能外还兼具 Sort By 的功能。但是排序只能是倒序排序，不能指定排序规则为 ASC 或者 DESC。 123hive (default)&gt; select * from emp cluster by deptno;# 等价于hive (default)&gt; select * from emp distribute by deptno sort by deptno; 桶 分区针对的是数据的存储路径；分桶针对的是数据文件。 123456789101112131415161718192021222324252627282930# 第一次尝试# 创建分桶表，按 id 取模create table stu_buck(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by '\\t';# 查看表结构hive (default)&gt; desc formatted stu_buck; # Num Buckets: 4# 导入数据到分桶表中，直接 load 不会进行分桶，还是一整个文件，那么通过 MR 导入呢？hive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table stu_buck;# 第二次尝试，创建分桶表时，数据通过子查询的方式导入# 创建用于子查询的表create table stu(id int, name string) row format delimited fields terminated by '\\t';# 向普通的 stu 表中导入数据load data local inpath '/opt/module/datas/student.txt' into table stu;# 清空 stu_buck 表中数据truncate table stu_buck;select * from stu_buck;# 导入数据到分桶表，通过子查询的方式insert into table stu_buck select id, name from stu;# 发现还是只有一个分桶# 需要设置一个属性hive (default)&gt; set hive.enforce.bucketing=true;hive (default)&gt; set mapreduce.job.reduces=-1;hive (default)&gt; insert into table stu_buck select id, name from stu;# 查询分桶的数据，分桶成功hive (default)&gt; select * from stu_buck; 分桶抽样查询 对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结 果。Hive 可以通过对表进行抽样来满足这个需求。 12# 查询表 stu_buck 中的数据hive (default)&gt; select * from stu_buck tablesample (bucket 1 out of 4 on id); TABLESAMPLE(BUCKET x OUT OF y) y 必须是 table 总 bucket 数的倍数或者因子。hive 根据 y 的大小，决定抽样的比例。table 总 bucket 数为 4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2 个 bucket 的数据，抽取第 1(x) 个和第 4(x+y) 个 bucket 的数据。 其它空字段赋值12# 如果员工的 comm 为 NULL，则用-1 代替hive (default)&gt; select nvl(comm,-1) from emp; CASE WHEN 12345678910111213141516171819202122232425262728# 求出不同部门男女各多少人vi emp_sex.txt # 悟空 A 男 # 大海 A 男 # 宋宋 B 男 # 凤姐 A 女 # 婷姐 B 女 # 婷婷 B 女# 创建 hive 表并导入数据create table emp_sex( name string, dept_id int, sex string)row format delimited fields terminated by \"\\t\";load data local inpath '/opt/module/datas/emp_sex.txt' into table emp_sex;# 按需求查询数据select dept_id, sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id;# 结果 # A 2 1 # B 1 2 行转列 CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入 字符串； CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数为参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL， 返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间； COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 array 类型字段。 1234567891011121314151617181920212223242526272829vi constellation.txt # 孙悟空 白羊座 A # 大海 射手座 A # 宋宋 白羊座 B # 猪八戒 白羊座 A # 凤姐 射手座 A# 按数据格式创建表create table person_info( name string, constellation string, blood_type string)row format delimited fields terminated by \"\\t\";load data local inpath “/opt/module/datas/person_info.txt” into table person_info;# 按需求查询数据select t1.base, concat_ws('|', collect_set(t1.name)) namefrom (select name,concat(constellation, \",\", blood_type) base # 星座,血型 from person_info ) t1group by t1.base;# 结果 # 射手座,A 大海|凤姐 # 白羊座,A 孙悟空|猪八戒 # 白羊座,B 宋宋 列转行 EXPLODE(col)：将 hive 一列中复杂的 array 或者 map 结构拆分成多行。 LATERAL VIEW 用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias解释：与 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。 1234567891011121314151617181920212223242526272829303132# 创建本地 movie.txt，导入数据vi movie.txt # 《疑犯追踪》 悬疑,动作,科幻,剧情 # 《Lie to me》 悬疑,警匪,动作,心理,剧情 # 《战狼 2》 战争,动作,灾难create table movie_info( movie string, category array&lt;string&gt;)row format delimited fields terminated by \"\\t\"collection items terminated by \",\";load data local inpath \"/opt/module/datas/movie.txt\" into table movie_info;# 按需求查询数据select movie,category_namefrom movie_info lateral view explode(category) table_tmp as category_name;# 结果 # 《疑犯追踪》 悬疑 # 《疑犯追踪》 动作 # 《疑犯追踪》 科幻 # 《疑犯追踪》 剧情 # 《Lie to me》 悬疑 # 《Lie to me》 警匪 # 《Lie to me》 动作 # 《Lie to me》 心理 # 《Lie to me》 剧情 # 《战狼 2》 战争 # 《战狼 2》 动作 # 《战狼 2》 灾难 窗口函数 OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化；CURRENT ROW：当前行；n PRECEDING：往前 n 行数据；n FOLLOWING：往后 n 行数据；UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点，UNBOUNDED FOLLOWING 表示到后面的终点；LAG(col,n)：往前第 n 行数据LEAD(col,n)：往后第 n 行数据NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从 1 开始， 对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为 int 类型。 12345678910111213141516# 数据准备name,orderdate,costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 1234567891011121314151617181920212223242526272829303132333435363738394041# 创建本地 business.txt，导入数据vi business.txt# 创建 hive 表并导入数据create table business( name string, orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';load data local inpath \"/opt/module/datas/business.txt\" into table business;# 查询在 2017 年 4 月份购买过的顾客及总人数select name,count(*) over() # group by 统计每一次条件下的数据；over() 开窗把整个数据即开给你用from businesswhere substring(orderdate,1,7) = '2017-04'group by name;# 查询顾客的购买明细及月购买总额select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;# 上述的场景,要将 cost 按照日期进行累加select name,orderdate,cost,sum(cost) over() as sample1,-- 所有行相加sum(cost) over(partition by name) as sample2,-- 按 name 分组，组内数据相加sum(cost) over(partition by name order by orderdate) as sample3,-- 按 name 分组，组内数据累加sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDINGand current row ) as sample4 ,-- 和 sample3 一样,由起点到当前行的聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and currentrow) as sample5, -- 当前行和前面一行做聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1FOLLOWING ) as sample6,-- 当前行和前边一行及后面一行sum(cost) over(partition by name order by orderdate rows between current row andUNBOUNDED FOLLOWING ) as sample7 -- 当前行及后面所有行from business;# 查看顾客上次的购买时间select name,orderdate,cost,lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate ) as time1,lag(orderdate,2) over (partition by name order by orderdate) as time2from business;# 查询前 20% 时间的订单信息select * from (select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business) twhere sorted = 1; Rank RANK() 排序相同时会重复，总数不会变；DENSE_RANK()排序相同时会重复，总数会减少；ROW_NUMBER() 会根据顺序计算。 12345678910111213select name, subject, score,rank() over(partition by subject order by score desc) rp,dense_rank() over(partition by subject order by score desc) drp,row_number() over(partition by subject order by score desc) rmpfrom score;# 结果name subject score rp drp rmp宋宋 英语 84 1 1 1大海 英语 84 1 1 2婷婷 英语 78 3 2 3 函数123456# 查看系统自带的函数hive&gt; show functions;# 显示自带的函数的用法hive&gt; desc function upper;# 详细显示自带的函数的用法hive&gt; desc function extended upper; 自定义函数官方文档地址","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://yoursite.com/tags/Hive/"}]},{"title":"Linux 基础","slug":"开发杂项/Linux 基础","date":"2020-04-26T01:15:17.000Z","updated":"2020-08-17T13:59:56.786Z","comments":true,"path":"2020/04/26/开发杂项/Linux 基础/","link":"","permalink":"http://yoursite.com/2020/04/26/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/Linux%20%E5%9F%BA%E7%A1%80/","excerpt":"汇总 Linux 中常用的操作命令。","text":"汇总 Linux 中常用的操作命令。 系统安装安装VMware 👉 新建虚拟机 👉 安装Linux 👉 初步配置 👉 克隆 👉 完善配置 新建虚拟机自定义类型配置 👉 稍后安装操作系统 👉 客户机操作系统：Linux（版本：CentOS 64位） 👉 使用网络地址转换（NAT）👉 将虚拟磁盘拆分成多个文件 安装 Linux选中新建虚拟机使用 IOS 映像文件 👉 Create Custom Layout 👉 Add Partition 👉 /boot – ext4 – 200（引导文件一般不会超过 200 M） – Fixed size/swap（供系统使用） – swap – 2048（用于内存交换，一般为运存两倍） – Fixed size/ – ext4 – 200 – Fill to maximum allowable size Install boot loader on /dev/sad. 👉 Boot loader operating system list 👉 CentOS /dev/sda3 初步配置设置IP：VM：编辑 -&gt; 虚拟网络编辑器 子网子网掩码NAT设置网关 vi /etc/sysconfig/network-scripts/ifcfg-eth0 12345678910# 删除 UUID 和 MAC 地址ONBOOT=yesBOOTPROTO=dhcp # static# IPADDR= # 与适配器要一致# NETMASK=# GATEWAY= # 与适配器要一致DNS1=114.114.114.114# 网卡重启systemctl restart network.service rm -fr /etc/udev/rules.d/70-persistent-net.rules 接口与硬件地址关联规则，不删除克隆后 enth0 接口失效。 VM：默认维护每一台虚拟机的 MAC 地址，互不重复 关闭防火墙 &amp; Selinux 123456789firewall-cmd --statefirewallsystemctl stop firewalld.service# 禁止开机自启firewallsystemctl disable firewalld.serviceservice iptables stopchkconfig iptables offvi /etc/selinux/config # SELINUX=disabled 关机 👉 拍摄快照！！！ 克隆虚拟机准备四台虚拟机 👉 配置IP 👉 配置主机名 12vi /etc/sysconfig/network # HOSTNAME= 配置 hosts Linux：vi /etc/hostsWindow：/windows/system32/drivers/etc/hosts 关机 👉 拍摄快照（克隆） 系统目录 /boot：系统启动相关文件 /dev：设备文件 /etc：配置 /home：用户的家目录 /root：管理员家目录 /lib：库文件 /opt：自定义程序安装目录 /bin：可执行文件（用户命令） /sbin：管理命令 用户管理12345678910111213groupaddgroupdeluseradduserdelusermodidpasswdsudosu# 权限chmodchownugo -&gt; rwx -&gt; 111/000 -&gt; 7/0 example（创建指定用户的数据共享区域） 1234567891011121314151617181920# 创建用户useradd user01passwd user01useradd user02passwd user02# 创建共享目录mkdir /var/swapdata# 修正权限chmod 770 swapdata == chmod o-rwx g+rwx swapdata# 修正所属组groupadd swapareausermod -a -G swaparea user01usermod -a -G swaparea user02chown root:swaparea swapdata# 修正文件权限chown root:swaparea share.filechmod 770 share.file# 查看用户元素据id user01id user02 软件安装12345yum clean allyum makecacheyum search man-pagesyum install man man-pages man-pages-zh-CNman bash(重点学习) 常用命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# shell bashtype：命令类型 内部命令 外部命令help：内部命令帮助man：帮助手册yum install man man-pages -ywhereis：定位命令位置file：文件类型echo：打印到标准输出 $PATH：环境变量：路径 $LANG echo $$：当前shell的PIDhash -r：清除缓存# 文件系统命令ls -l 文件类型 -：普通文件 d：目录文件 b：块设备文件 c：字符设备文件 l：符号链接文件 p：命令管道文件 s：套接字文件# 文件权限：三位一组（rwx） U（USer） G（Group） O（Other）# 文件硬链接次数（软连接对比）ownergroupsizetimestamp accsee modify changedf：显示磁盘使用情况 -hdu：显示文件系统使用情况ls：显示目录cd：切换工作目录pwd：显示当前工作目录mkdir：创建目录 mkdir -p ./a/b/c mkdir a/&#123;1,2,3&#125;dirrm：删除 rm -f rm -rfcp：拷贝 cp -r ./a ./new cp -lmv：移动ln：链接 -s（软连接）stat：元数据touch# 文本操作cat more less head -num tail -num| cat b.txt | head -3 左边输出为右边的输入 文本处理1234567891011121314151617181920212223242526cut：显示切割的行数据 f：选择显示的列 s：不显示没有分隔符的行 d：自定义分隔符sort：排序文件的行 n：按数值排序 r：倒序 t：自定义分隔符 k：选择排序列 u：合并相同行 f：忽略大小写sed：行编辑器sed [options] '' filename options n：静默模式 i：直接修改原文件 'Command' d：删除符合条件的行 p：显示符合条件的行 a\\string：指定行后添加新行 i\\string：指定行前添加新行 s/pattern/string/：查找并替换 \\(\\),\\(\\)--&gt;\\1,\\2awk：强大的文本分析工具 将文件逐行读入，用默认分隔符将每行切片，切开部分再进行各种分析处理 awk -F'' '&#123;pattern+action&#125;' filename 正则表达式1234567891011121314151617181920grep：显示匹配行 v：反显示 e：使用扩展正则表达式匹配操作符 \\：转义字符 .：匹配任意单个字符 [0-9],[a-z]：字符序列单字符占位 ^：行首 $：行尾 &lt;,&gt;：单词首尾边界 |：连接操作符 (,)：选择操作符 \\n：反向引用重复操作符 ?：匹配0-1次 *：0-多次 +：1-多次 &#123;n&#125;：n次 &#123;n,&#125;：n-多次 &#123;n,m&#125;：n-m次 VI12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 打开文件 vi /path :num :/PATTERN# 关闭文件 :q :wq :q! :w :w! :x == :wq zz# 编辑 i：在字符前转为输入模式 a：在字符后转为输入模式 o：在下一行转为输入模式 O：在上一行转为输入模式 l：在行首转为输入模式 A：在行尾转为输入模式# 单词 w：到下一个单词的词首 e：到当前或下一单词词尾# 行内 0：绝对行首 ^：行首的第一个非空白字符 $：绝对行尾# 行间 G：文章末尾 3G：第三行 gg：文章开头# 翻屏 ctrl：f,b# 单个字符 x：删除光标位置字符 3x：删除光标开始第三个字符 r：替换光标位置字符复制：yw,yy粘贴：p,P撤销：u重做撤销：ctrl+r重复上一步操作：.# 设置 set number set nonumber set readonly# 查找： /PATTERN（向下）,N ?PATTERN（向上）,n# 查找并替换 s/str1/str2/gi g：一行内全部替换 i：忽略大小写","categories":[{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Hadoop","slug":"大数据/Hadoop","date":"2020-04-25T02:09:27.000Z","updated":"2020-07-10T03:39:05.616Z","comments":true,"path":"2020/04/25/大数据/Hadoop/","link":"","permalink":"http://yoursite.com/2020/04/25/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/","excerpt":"大数据技术 Hadoop 的整体入门介绍。","text":"大数据技术 Hadoop 的整体入门介绍。 基本概念 主要解决：海量数据的存储和海量数据的分析计算问题。 一般形式的大数据部门组成结构。 Hadoop 三大发行版本： Apache 版本最原始（最基础）的版本，对于入门学习最好；Cloudera 在大型互联网企业中用的较多；Hortonworks 文档较好。 Hadoop 优势： 高可靠性：Hadoop 底层维护多个数据副本，所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失；高扩展性：在集群间分配仼务数据，可方便的扩展数以干计的节点；高效性：在 MapReduce 的思想下,，Hadoop 是并行工作的，以加快任务处理速度；高容错性：能够自动将失败的任务重新分配。 1.x VS 2.x HDFSNameNode（nn）：存储文件的元数据，如文件名、文件目录结构、文件属性（生成时间、副本数文件权限），以及每个文件的块列表和块所在的 DataNode 等； DataNode（dn）：在本地文件系统存储文件块数据,以及块数据的校验和。 Secondary NameNode（2nn）：用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。 YARN ResourceManager 由两个关键组件 Scheduler 和 ApplicationsManager 组成。 Scheduler 在容量和队列限制范围内负责为运行的容器分配资源。Scheduler 是一个纯调度器（pure scheduler），只负责调度，它不会监视或跟踪应用程序的状态，也不负责重启失败任务，这些全都交给 ApplicationMaster 完成。Scheduler 根据各个应用程序的资源需求进行资源分配。 ApplicationManager 负责接收作业的提交，然后启动一个 ApplicationMaster 容器负责该应用。它也会在ApplicationMaster 容器失败时，重新启动 ApplicationMaster 容器。 Hadoop 2.X 集群中的每个 DataNode 都会运行一个NodeManager 来执行 Yarn 的功能。每个节点上的 NodeManager 执行以下功能： 定时向 ResourceManager 汇报本节点资源使用情况和各个 Container 的运行状况 监督应用程序容器的生命周期 （监控资源）监控、管理和提供容器消耗的有关资源（CPU / 内存）的信息 （监控资源使用情况） （监控容器）监控容器的资源使用情况，杀死失去控制的程序 （启动/停止容器）接受并处理来自 ApplicationMaster 的 Container 启动/停止等各种请求。 提交到 YARN 上的每一个应用都有一个专用的 ApplicationMaster（注意，ApplicationMaster 需要和 ApplicationManager 区分）。ApplicationMaster 运行在应用程序启动时的第一个容器内。ApplicationMaster 会与 ResourceManager 协商获取容器来执行应用中的 mappers 和 reducers，之后会将 ResourceManager 分配的容器资源呈现给运行在每个 DataNode 上的 NodeManager。ApplicationMaster 请求的资源是具体的。包括： 处理作业需要的文件块 为应用程序创建的以容器为单位的资源 容器大小（例如，1GB 内存和一个虚拟核心） 资源在何处分配，这个依据从 NameNode 获取的块存储位置信息（如机器1的节点10上分配4个容器，机器2的节点20上分配8个容器） 资源请求的优先级 ApplicationMaster 是一个特定的框架。例如，MapReduce 程序是 MRAppMaster，spark 是 SparkAppMaster。 Container 是对于资源的抽象, 它封装了某个节点上的多维度资源，如内存、CPU 等。 MapReduce Map 阶段并行处理输入数据； Reduce 阶段对 Map 结果进行汇总。 生态体系 Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive 与传统的数据库（MySql）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。 Kafka 是一种高吞吐量的分布式发布订阅消息系统。 Storm 用于连续计算，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。 Oozie 是一个管理 Hdoop 作业（job）的工作流程调度管理系统。 HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。 其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。 R 语言是用于统计分析、绘图的语言和操作环境。R 是属于 GNU 系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 Apache Mahout 是个可扩展的机器学习和数据挖掘库。 Zookeeper 是 Google 的 Chubby 一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 环境搭建软件安装123456789101112131415161718192021222324252627282930313233# 在 /opt 目录下创建 module、software 文件夹mkdir modulemkdir software# 修改 module、software 文件夹的所有者chown [group]:[user] module/ software/# 安装 JDK# 检查是否安装 Java 软件rpm -qa | grep javarpm -e [software]# 查看 JDK 安装路径which java# 解压 JDK 到 /opt/module 目录下tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/# 配置 JDK 环境变量pwd /opt/module/jdk1.8.0_144vi /etc/profile # 文件末尾添加 JDK 路径 # export JAVA_HOME=/opt/module/jdk1.8.0_144 # export PATH=$PATH:$JAVA_HOME/bin# 配置生效source /etc/profile# 测试环境java -version# 安装 Hadooptar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/# 添加 Hadoop 环境变量pwd /opt/module/hadoop-2.7.2vi /etc/profile # export HADOOP_HOME=/opt/module/hadoop-2.7.2 # export PATH=$PATH:$HADOOP_HOME/bin # export PATH=$PATH:$HADOOP_HOME/sbinsource /etc/profilehadoop version Hadoop 目录结构bin 目录：存放对 Hadoop 相关服务（HDFS，YARN）进行操作的脚本； etc 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件； lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）； sbin 目录：存放启动或停止 Hadoop 相关服务的脚本； share 目录：存放 Hadoop 的依赖 jar 包、文档、和官方案例。 配置文件说明 Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 要获取的默认文件 文件存放在 Hadoop 的 jar 包中的位置 [core-default.xml] hadoop-common-2.7.2.jar/ core-default.xml [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml [mapred-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml 自定义配置文件： ​ core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个配置文件存放在 $HADOOP_HOME/etc/hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。 运行模式本地模式12345678910# hadoop-2.7.2 目录下mkdir wcinputcd wcinputtouch wc.input# 编写分析文本vi wc.input# hadoop-2.7.2 目录下执行hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput# 查看结果cat wcoutput/part-r-00000 伪分布式123# 配置集群# 配置 hadoop-env.sh，修改 JAVA_HOME 路径 # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置 core-site.xml --&gt;&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 Hadoop 运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 1234567&lt;!-- 配置 hdfs-site.xml --&gt;&lt;!-- 指定 HDFS 副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 1234567891011# 启动集群# 格式化 NameNode（第一次启动时需要格式化）bin/hdfs namenode -format# 启动 NameNodesbin/hadoop-daemon.sh start namenode# 启动 DataNodesbin/hadoop-daemon.sh start datanode# 查看集群是否启动成功jps# 查看产生的 Log 日志cd /opt/module/hadoop-2.7.2/logs 格式化 NameNode 后会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，导致集群找不到之前数据。所以，格式 NameNode 前一定要先删除 data 数据和 log 日志，然后才能格式化 NameNode。 启动 YARN12# 配置 yarn-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置yarn-site.xml --&gt;&lt;!-- Reducer 获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt; 1234# 配置 mapred-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144# 配置 mapred-site.xmlmv mapred-site.xml.template mapred-site.xml 12345&lt;!-- 指定 MR 运行在 YARN 上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 1234# 启动集群，启动前必须保证 NameNode 和 DataNode 已经启动sbin/yarn-daemon.sh start resourcemanagersbin/yarn-daemon.sh start nodemanager# 8088 端口查看 web 页面 历史服务器 用于查看程序的历史运行情况。 123456789101112&lt;!-- 配置 mapred-site.xml --&gt;&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器 web 端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; 123# 启动历史服务器sbin/mr-jobhistory-daemon.sh start historyserver# 19888 端口查看 web 页面 日志聚集 应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上，方便的查看到程序运行详情和开发调试。 12345678910111213&lt;!-- yarn-site.xml --&gt;&lt;!-- 开启日志聚集功能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 1234567# 重启后日志聚集才生效sbin/yarn-daemon.sh stop resourcemanagersbin/yarn-daemon.sh stop nodemanagersbin/mr-jobhistory-daemon.sh stop historyserversbin/yarn-daemon.sh start resourcemanagersbin/yarn-daemon.sh start nodemanagersbin/mr-jobhistory-daemon.sh start historyserver 完全分布式 三台虚拟机之间的相互通讯，协调工作。 集群分发 scp（secure copy）可以实现服务器与服务器之间的数据拷贝。 1234567# 在 hadoop101 上将 hadoop101 中 /opt/module 目录下的软件拷贝到 hadoop102 上# -r 表示递归，即全部都拷贝过去scp -r /opt/module root@hadoop102:/opt/module# 将 hadoop101 中 /etc/profile 文件拷贝到 hadoop102 的 /etc/profile 上scp /etc/profile root@hadoop102:/etc/profile# 配置生效source /etc/profile rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。 选项 功能 -r 递归 -v 显示复制过程 -l 拷贝符号连接 12# 把 hadoop101 机器上的 /opt/software 目录同步到 hadoop102 服务器的 root 用户下的 /opt/ 目录rsync -rvl /opt/software/ root@hadoop102:/opt/software 12345# 编写集群分发脚本# /usr/local/bin 目录下的脚本全局可用# /home/wingo/bin 这个目录下存放的脚本，wingo 用户可以在系统任何地方直接执行touch xsyncvi xsync 12345678910111213141516171819202122232425#!/bin/bash# 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi# 获取文件名称，basename 获取路劲最后那个名称p1=$1fname=`basename $p1`echo fname=$fname# 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir# 获取当前用户名称user=`whoami`# 循环，这里根据需求修改循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 集群配置 先在 hadoop102 中进行配置，然后将配置分发到其余的节点。 hadoop102 hadoop103 hadoop104 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager ResourceManager、NodeManager NodeManager 12345678910111213&lt;!-- 配置 core-site.xml --&gt;&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 Hadoop 运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 123# 配置集群# 配置 hadoop-env.sh，修改 JAVA_HOME 路径 # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置 hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;!-- 分片数必须要小于节点数 --&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 Hadoop 辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; 12# 配置 yarn-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置yarn-site.xml --&gt;&lt;!-- Reducer 获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt; 1234# 配置 mapred-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144# 配置 mapred-site.xmlmv mapred-site.xml.template mapred-site.xml 12345&lt;!-- 指定 MR 运行在 YARN 上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 12# 在集群上分发配置好的Hadoop配置文件xsync /opt/module/hadoop-2.7.2/ 单点启动12345# NameNode 第一次启动初始化hadoop namenode -formathadoop-daemon.sh start namenode# 分别在三个节点上启动 DataNodehadoop-daemon.sh start datanode 集群启动1234567891011vi /opt/module/hadoop-2.7.2/etc/hadoop/slaves # hadoop102 # hadoop103 # hadoop104# 同步配置xsync slaves# NameNode 节点bin/hdfs namenode -formatsbin/start-dfs.sh# ResourceManage 节点sbin/start-yarn.sh SSH 配置1234567# ~/.ssh 生成密钥ssh-keygen -t rsa# 每个节点都需要进行此操作# 将公钥拷贝到要免密登录的目标机器上ssh-copy-id hadoop102ssh-copy-id hadoop103ssh-copy-id hadoop104 文件名 功能 known_hosts 记录ssh访问过计算机的公钥(public key) id_rsa 生成的私钥 id_rsa.pub 生成的公钥 authorized_keys 存放授权过得无密登录服务器公钥 时间同步 NPT（Network Time Protocol） 时间同步的方式：找一个节点作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 12345678910111213141516171819202122232425262728# 检查安装rpm -qa | grep ntpvi /etc/ntp.confyum install -y ntp# 修改配置vi /etc/ntp.conf # 授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间 # restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap # 集群在局域网中，不使用其他互联网上的时间 # #server 0.centos.pool.ntp.org iburst # #server 1.centos.pool.ntp.org iburst # #server 2.centos.pool.ntp.org iburst # #server 3.centos.pool.ntp.org iburst # 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步 # server 127.127.1.0 # fudge 127.127.1.0 stratum 10vim /etc/sysconfig/ntpd # SYNC_HWCLOCK=yesservice ntpd restart# 开机启动此服务chkconfig ntpd on# 其它节点配置crontab -e # */10 * * * * /usr/sbin/ntpdate hadoop102# 修改时间date -s \"2017-9-11 11:11:11\"# 一段时间后查看是否同步date 操作集群123456789101112131415161718# 在 HDFS 文件系统上创建一个 input 文件夹bin/hdfs dfs -mkdir -p /user/wingo/input# 将测试文件内容上传到文件系统上bin/hdfs dfs -put wcinput/wc.input /user/wingo/input/# 查看上传的文件bin/hdfs dfs -ls /user/wingo/input/bin/hdfs dfs -cat /user/wingo/input/# 运行 MapReduce 程序bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/wingo/input/ /user/wingo/output# 查看输出结果，可在 50070 端口 Web 查看bin/hdfs dfs -cat /user/wingo/output/*# 下载文件到本地bin/hdfs dfs -get /user/wingo/output/part-r-00000 ./wcoutput/# 删除输出结果bin/hdfs dfs -rm -r /user/wingo/output 启动停止操作123456789# 分别启动 / 停止 HDFS 组件hadoop-daemon.sh start / stop namenode / datanode / secondarynamenode# 启动 / 停止 YARNyarn-daemon.sh start / stop resourcemanager / nodemanager# 配置了 ssh 后可一次性启动集群# 整体启动 / 停止 HDFSstart-dfs.sh / stop-dfs.sh# 整体启动/停止 YARNstart-yarn.sh / stop-yarn.sh HDFSHDFS( Hadoop Distributed File System）它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 具有高容错性，处理大数据，可构建在廉价机器上等优点；具有延迟较高，无法高效存储大量小数据，不支持并发写入及文件随机修改等缺陷。 架构 NameNode 就是 Master，它是一个主管、管理者。 管理HDFS的名称空间；配置副本策略;管理数据块( Block）映射信息;处理客户端读写请求。 DataNode：就是 Slave。 NameNode 下达命令, DataNode 执行实际的操作。 存储实际的数据块;执行数据块的读写操作。 Client：客户端。 文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block 然后进行上传；与 NameNode 交互，获取文件的位置信息；与 DataNode 交互，读取或者写入数据；Client 提供一些命令来管理 HDFS,比如 NameNode 格式化；Client 可以通过一些命令来访问 HDFS，比如对 HDFS 增删查改操作。 Secondary NameNode：并非 NameNode 的热备。 辅助 NameNode，分担其工作量，比如定期合并 Silage 和 Edits，并推送给 NameNode；在紧急情况下,可辅助恢复 NameNode。 文件快大小HDFS 中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定,默认大小在 Hadoop2x 版本中是 128M，老版本中是 64M。 HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置；如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。总结：HDFS 块的大小设置主要取决于磁盘传输速率。（寻址时间为传输时间的 1% 时最合适） Shell 操作 hfs 是 fs 的具体实现类。 1234567891011121314151617181920212223242526272829303132333435363738# 命令大全bin/hadoop fs# 输出这个命令参数hadoop fs -help rm# 显示目录信息hadoop fs -ls /# 在 HDFS 上创建目录hadoop fs -mkdir -p /win/go# 从本地剪切粘贴到 HDFShadoop fs -moveFromLocal ./win.txt /win/go# 追加一个文件到已经存在的文件末尾hadoop fs -appendToFile go.txt /win/go/win.txt# 显示文件内容hadoop fs -cat /win/go/win.txt# 修改文件所属权限，HDFS 操作的用户及用户组必须是系统中存在的hadoop fs -chmod 666 /win/go/win.txthadoop fs -chown win:go /win/go/win.txt# 从 HDFS 拷贝到本地hadoop fs -copyToLocal /win/go/win.txt ./# 从 HDFS 的一个路径拷贝到 HDFS 的另一个路径hadoop fs -cp /win/go/win.txt /other.txt# 在 HDFS 目录中移动文件hadoop fs -mv /other.txt /win/go/# 从 HDFS 下载文件到本地hadoop fs -get /win/go/win.txt ./# 合并下载多个 HDFS 文件hadoop fs -getmerge /user/wingo/test/* ./together.txt# 等同于 copyFromLocalhadoop fs -put ./local.txt /user/wingo/test/# 显示一个文件的末尾hadoop fs -tail /win/go/win.txt# 删除文件或文件夹hadoop fs -rm /user/wingo/test/delete.txthadoop fs -rmdir /test# 统计文件夹的大小信息hadoop fs -du -h /user/wingo/test# 设置 HDFS 中文件的副本数量：若条件允许的话才会达到hadoop fs -setrep 10 /win/go/win.txt 写流程 向 HDFS 写数据时，客户端与 NameNode、DataNode 通信过程。 网络拓扑 在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。 节点距离：两个节点到达最近的共同祖先的距离总和。 机架感知 副本节点选择。 读流程 在 HDFS 读数据时，客户端与 NameNode、DataNode 通信过程。 2NN如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的 FsImage。 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。 但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由 NameNode 节点完成，又会效率过低。因此，引入一个新的节点 SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。 Fsimage 和 Edits12345# oiv 查看 Fsimage 文件pwd # /opt/module/hadoop-2.7.2/data/tmp/dfs/name/currenthdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xmlcat /opt/module/hadoop-2.7.2/fsimage.xml 12345678910&lt;!-- 记录了 NameNode 的信息，DataNode 信息定时同步到 NameNode --&gt;&lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;wingo:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt; 123# oev 查看 Edits 文件hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xmlcat /opt/module/hadoop-2.7.2/edits.xml 12345678910&lt;!-- 记录了操作信息 --&gt;&lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt;&lt;/EDITS &gt; CheckPoint12345678910111213141516171819&lt;!-- 默认配置 hdfs-default.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;!-- 一小时执行一次 --&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作次数达一百万次&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;一分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; 故障处理 NameNode 故障后，可以采用如下两种方法恢复数据。 123456789# 方法一 将 SecondaryNameNode 中数据拷贝到 NameNode 存储数据的目录# 杀掉 NameNode 相关进程kill -9 [pid]# 删除 NameNode 存储的数据rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*# 拷贝 SecondaryNameNode 中数据到原 NameNode 存储数据目录scp -r root@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/# 重新启动 NameNodesbin/hadoop-daemon.sh start namenode 1# 方法二 使用 -importCheckpoint 选项启动 NameNode 守护进程，从而将 Secondary NameNode 中数据拷贝到 NameNode 目录中。 1234567891011&lt;!-- 修改hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; 123456789101112# 杀掉 NameNode 相关进程kill -9 [pid]# 删除 NameNode 存储的数据rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*# 如果 SecondaryNameNode 不和 NameNode 在一个主机节点上，# 需要将 SecondaryNameNode 存储数据的目录拷贝到 NameNode 存储数据的平级目录，并删除 in_use.lock 文件scp -r root@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./rm -rf in_use.lock# 导入检查点数据（等待一会 ctrl+c 结束掉）bin/hdfs namenode -importCheckpoint# 启动 NameNodesbin/hadoop-daemon.sh start namenode 安全模式NameNode 启动时，首先将镜像文件(（Fsimage）载入內存，并执行编辑日志(（Edits）中的各项操作，一旦在内存中成功建立文件系统元数据的映像，则创建一个新的 Silage文件和一个空的编辑日志。此时，NameNode 开始监听 DataNode 请求。这个过程期间，Namenode 一直运行在安全模式，即 NameNode 的文件系统对于客户端来说是只读的。 系统中的数据块的位置并不是由 NameNode 维护的，而是以块列表的形式存储在 DataNode 中。在系统的正常操作期间, NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个 Datanode 会向 NameNode 发送最新的块列表信息,，NameNode 了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”,，NameNode 会在 30 秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中 99.9% 的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的 HDFS 集群时，因为系统中还没有任何块，所以 NameNode 不会进入安全模式。 12345bin/hdfs dfsadmin -safemode get # 查看安全模式状态bin/hdfs dfsadmin -safemode enter # 功能描述：进入安全模式状态bin/hdfs dfsadmin -safemode leave # 离开安全模式状态bin/hdfs dfsadmin -safemode wait # 等待安全模式结束状态# 可以编写脚本，在安全模式下堵塞等待安全模式结束运行接下来的命令 多目录 NameNode 的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。 1234567891011&lt;!-- hdfs-site.xml 配置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; DataNode 也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。 123456&lt;!-- hdfs-site.xml 配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; DataNode DataNode 掉线时限参数设置 12345678910111213&lt;!-- Timeout = 2*dfs.namenode.heartbeat.recheck-interval + 10*dfs.heartbeat.interval --&gt;&lt;!-- hdfs-default.xml 默认配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;!-- 单位 ms --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;!-- 单位 s --&gt;&lt;/property&gt; 数据完整性 检测 DataNode 节点上的数据是否完整？ 当 DataNode 读取 Block 的时候，它会计算 CheckSum，如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。 Client 读取其他 DataNode 上的 Block，DataNode 在其文件创建后周期验证 CheckSum。 黑白名单 添加到白名单的主机节点，都允许访问 NameNode，不在白名单的主机节点，都会被退出。 1234# 白名单# 在 NameNode 的 /opt/module/hadoop-2.7.2/etc/hadoop 目录下创建 dfs.hosts 文件vi dfs.hosts # 添加允许的主机名 123456&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 1234567# 分发配置文件xsync hdfs-site.xml# 更新节点信息hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes# 如果数据不均衡，可以用命令实现集群的再平衡./start-balancer.sh 在黑名单上面的主机都会被强制退出。 123# 黑名单vi dfs.hosts.exclude # 添加不允许的主机名 123456&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 1234567# 更新节点信息hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes# 如果数据不均衡，可以用命令实现集群的再平衡./start-balancer.sh# 检查 Web 浏览器，退役节点的状态为 decommission in progress（退役中），说明数据节点正在复制块到其他节点# 等待退役节点状态为 decommissioned（所有块已经复制完成），停止该节点及节点资源管理器 回收站 开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。 1234# 0 表禁用，其他值表示设置文件的存活时间# fs.trash.interval=0# 检查回收站的间隔时间。如果该值为 0，则该值设置和 fs.trash.interval 的参数值相等。# fs.trash.checkpoint.interval=0 12345678910111213&lt;!-- core-site.xml --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;!-- min --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;wingo&lt;/value&gt; &lt;!-- 进入回收站的名称 --&gt;&lt;/property&gt; 123456789# 回收站在集群中的路径/user/wingo/.Trash/...# 通过程序删除的文件不会经过回收站，需要调用 moveToTrash() 才进入回收站# Trash trash = New Trash(conf);# trash.moveToTrash(path);# 回复回收站数据hadoop fs -mv /user/wingo/.Trash/Current/user/wingo/input /user/wingo/input# 清空回收站hadoop fs -expunge 快照管理12345678hdfs dfsadmin -allowSnapshot [路径] # 开启指定目录的快照功能hdfs dfsadmin -disallowSnapshot [路径] # 禁用指定目录的快照功能，默认是禁用hdfs dts -createSnapshot [路径] # 对目录创建快照hdfs dts -createSnapshot [路径] [名称] # 指定名称创建快照hdts dfs -renameSnapshot [路径] [旧名称] [新名称] # 重命名快照hdfs lsSnapshottableDir # 列出当前用户所有可快照目录hdfs snapshotDiff [路径] [路径] # 比较两个快照目录的不同之处hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotname&gt; # 删除快照 MapReduceMapReduce 是一个分布式运算程序的编程框架,是用户开发“基于 Hadoop 的数据分析应用”的核心框架。 MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。 具有易于编程、扩展性良好、高容错性、PB 级别海量数据处理能力等优点；具有不擅长实时计算、不擅长流式计算、不擅长 DAG （有向图）计算，即需要迭代的计算等缺陷。 实例进程 MrAppMaster：负责整个程序的过程调度及状态协调；MapTask：负责 Map 阶段的整个数据处理流程；ReduceTask：负责 Reduce 阶段的整个数据处理流程。 序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable 编程规范 用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。 Mapper 阶段 用户自定义的 Mapper 要继承指定的父类；Mapper 的输入数据是 K V 对的开式（K V 的类型可自定义）；Mapper 中的业务逻辑写在 map() 方法中；Mapper 的输出数据是 K Ⅴ 对的形式（K V 的类型可自定义）；map() 方法( MapTask 进程）对每一个 K V&gt;调用一次。 123456789101112131415161718192021222324// Key 偏移量 输入数据 Value 类型 输出数据类型的 K V 👉 对应 Reduce 的输入public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); // 此方法每次只获取一行 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 切割 String[] words = line.split(\" \"); // 输出 for (String word : words) &#123; k.set(word); // (word, 1) context.write(k, v); &#125; &#125;&#125; Reduce 阶段 用户自定义的 Reducer 要继承自己的父类；Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 K V 对的形式Reducer 的业务逻辑写在 reduce() 方法中ReduceTask 进程对每一组相同 K 的 K V 组调用一次 reduce() 方法 12345678910111213141516171819public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; int sum; IntWritable v = new IntWritable(); // 这里直接穿入一个迭代器了，直接循环取值即可 @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 累加求和 sum = 0; for (IntWritable count : values) &#123; sum += count.get(); &#125; // 输出 v.set(sum); context.write(key,v); &#125;&#125; Driver 阶段 相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群,提交的是封装了 MapReduce 程序相关运行参数的 job 对象。 123456789101112131415161718192021222324252627282930313233public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 设置 jar 加载路径 job.setJarByClass(WordcountDriver.class); // 设置 map 和 reduce 类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 设置 map 输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设置最终输出 K V 类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; Shuffle Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle，包含了两端的 Combiner 和 Partition。 Partition 要求将统计结果按照条件输出到不同文件中（分区）。 1234567// 默认 Partition 分区机制，用户无法控制 Key 的存储分区public class HashPartironer&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V vlaue, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numberReduceTasks; &#125;&#125; 自定义12345678910111213141516171819202122232425262728293031323334// 需求：将统计结果按照手机归属地不同省份输出到不同文件中（分区）public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 判断是哪个省 if (\"136\".equals(preNum)) &#123; partition = 0; &#125;else if (\"137\".equals(preNum)) &#123; partition = 1; &#125;else if (\"138\".equals(preNum)) &#123; partition = 2; &#125;else if (\"139\".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125;// Driver// 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 同时指定相应数量的 reduce taskjob.setNumReduceTasks(5);// job.setNumReduceTasks(1) 会正常运行,只不过会产生一个输出文件// job.setNumReduceTasks(2) 会报错// job.setNumReduceTasks(6) 大于 5 程序正常运行，会产生空文件 排序MapTask 和 ReduceTask 均会对数据按照 key 进行排序，该操作属 Hadoop 的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上否需要。 默认排序是按照字曲顺序排序，且实现该排序的方法是快速排序。 对于 MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于 ReduceTask，它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后, ReduceTask 统一对内存和磁盘上的所有数据进行一次归并排序。 自定义123456789101112131415161718// bean 对象做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法，就可以实现排序@Overridepublic int compareTo(FlowBean o) &#123; int result; // 按照总流量大小，倒序排列 if (sumFlow &gt; bean.getSumFlow()) &#123; result = -1; &#125;else if (sumFlow &lt; bean.getSumFlow()) &#123; result = 1; &#125;else &#123; result = 0; &#125; return result;&#125; 合并 Combiner 组件的父类就是 Reducer。Combiner 是在每一个 MapTask 所在的节点运行，进行局部汇总，减少网络传输量；Reducer 是接收全局所有 Mapper 的输出结果。 123456789101112131415161718192021// 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法// Combiner K V 对应 Reducer K Vpublic class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text , IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 汇总操作 int count = 0; for(IntWritable v : values)&#123; count += v.get(); &#125; // 写出 context.write(key, new IntWritable(count)); &#125;&#125;// Driverjob.setCombinerClass(WordcountCombiner.class); 分组 对 Reduce 阶段的数据根据某一个或几个字段进行分组。 自定义类继承 WritableComparator；重写 compare() 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// OrderBeanpublic class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; // 序列化 @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; // 反序列化 @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + \"\\t\" + price; &#125; // Getter / Setter // 二次排序，即 map 结束之前的一次排序，排序后输出到 reduce @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125; 123456789101112131415161718192021222324// 编写 OrderSortMapper 类// 0000001 Pdt_01 222.8public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 截取 String[] fields = line.split(\"\\t\"); // 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 写出 context.write(k, NullWritable.get()); &#125;&#125; 12345678910111213141516171819202122232425262728// 编写 OrderSortGroupingComparator 类// 在 reduce 前通过此排序进行分组public class OrderGroupingComparator extends WritableComparator &#123; // 创建一个构造将比较对象的类传给父类 protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; 123456789101112131415// 编写 OrderSortReducer 类public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 只能通过 NullWritable.get() 获取空值 context.write(key, NullWritable.get()); &#125;&#125;// Driver// 设置 reduce 端的分组job.setGroupingComparatorClass(OrderGroupingComparator.class); MapTaskRead 阶段：MapTask 通过用户编写的 RecordReader，从输入 InputSplit 中解析出一个个 key / value； Map 阶段：该阶段主要是将解析出的 key / value 交给用户编写 map() 函数处理，并产生一系列新的 key / value； Collect 收集阶段：在用户编写 map() 函数中，当数据处理完成后，一般会调用 OutputCollector.collect() 输出结果。在该函数内部，它会将生成的 key / value 分区（调用 Partitioner），并写入一个环形内存缓冲区中； Spill 阶段：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 溢写阶段详情： 利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output/spillN.out（N表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件output/spillN.out.index 中。 Combine阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output/file.out 中，同时生成相应的索引文件 output/file.out.index。 在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并 io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 ReduceTaskCopy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 Merge 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 Sort 阶段：按照 MapReduce 语义，用户编写 reduce() 函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。 Reduce 阶段：reduce() 函数将计算结果写到 HDFS上。 ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置。 12// 默认值是 1，手动设置为 4job.setNumReduceTasks(4); 工作流程 Hadoop 序列化序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。反序列化就是序列化的逆过程。 Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后会附带很多额外的信息（各种校验信息、Header、继承体系等）不便于在网络中高传输。 所以 Hadoop 自己开发了一套序列化机制（Writable）。 紧凑：高效使用存储空间； 快速：读写数据的额外开销小； 可扩展：随着通信协议的升级而可升级； 互操作：支持多语言的交互。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 自定义 bean 对象实现序列化接口// 实现 writable 接口public class FlowBean implements Writable&#123; private long upFlow; private long downFlow; private long sumFlow; // 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; // 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; // 反序列化方法 // 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; // Getter / Setter&#125; InputFormatMapTask 并行度决定机制： 数据块：Block 是 HDFS 物理上把数据分成一块一块。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。切片时不考虑数据集整体，而是逐个针对每一个文件单独切片。 Block 的大小：local 32 1.x 64 2.x 128 FileInputFormatTextInputFormat 是默认的 FilelnputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，Long writable类型。值是这行的内容，不包括任何行终止符（换行符和回车符）Text 类型。 123456789101112131415161718192021222324252627282930313233343536// 源码 Debug 主要流程waitForCompletion()submit(); // 建立连接 connect(); // 创建提交 Job 的代理 new Cluster(getConfiguration()); // 判断是 LocalJobRunner 还是 YarnRunner initialize(jobTrackAddr, conf); // 提交 job submitter.submitJobInternal(Job.this, cluster) // 创建给集群提交数据的 Stag 路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 获取 jobid ，并创建 Job 路径 JobID jobId = submitClient.getNewJobID(); // 拷贝 jar 包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 计算切片，生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job); // 向 Stag 路径写 XML 配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 提交 Job，返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 12345// 获取切片信息 API// 获取切片文件名称String name = inputSplit.getPath().getName();// 根据文件类型获取切片信息FileSplit inputSpli = (FileSplit)context. getInputSplit() CombineText框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。 CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。 12345// 如果不设置 InputFormat，它默认用的是 TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);// 虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。 KeyValueText KeyValueTextInputFormat 每一行均为一条记录被分隔符分割为 K V。可以通过在驱动类中设定分隔符。默认分隔符是 tab（\\t）。 1conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR,\"\\t\") 1234567891011121314// 需求：统计输入文件中每一行的第一个单词相同的行数// Mapper 编写public class KVTextMapper extends Mapper&lt;Text, Text, Text, LongWritable&gt;&#123; // 设置 value LongWritable v = new LongWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; // 写出 (第一个单词, 1) context.write(key, v); &#125;&#125; 123456789101112131415161718192021public class KVTextReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;&#123; LongWritable v = new LongWritable(); @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0L; // 汇总统计 for (LongWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); // 输出 context.write(key, v); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536public class KVTextDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); // 设置切割符 conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \"); // 获取 job 对象 Job job = Job.getInstance(conf); // 设置 jar 包位置，关联 mapper 和 reducer job.setJarByClass(KVTextDriver.class); job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 设置 map 输出 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 设置最终输出 k v 类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); // 设置输入数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); // 设置输入格式 job.setInputFormatClass(KeyValueTextInputFormat.class); // 设置输出数据路径 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交 job job.waitForCompletion(true); &#125;&#125; NLine NLineInputFormat 代表每个 map 进程处理的 InputSplit 不再按 Block块去划分,而是按 NLineInputFormat 指定的行数来划分。即输入文件的总行数 N = 切片数，如果不整除则切片数 = 商 + 1。 1234567// 对每个单词进行个数统计,要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中// 设置每个切片 InputSplit 中划分三条记录NLineInputFormat.setNumLinesPerSplit(job, 3);// 使用 NLineInputFormat 处理记录数 job.setInputFormatClass(NLineInputFormat.class); 自定义 无论 HDFS 还是 MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义 InputFormat 实现小文件的合并。 123456789101112131415161718192021222324252627/** * 将多个小文件合并成一个 SequenceFile 文件 * SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对的文件格式 * SequenceFile 里面存储着多个文件，存储的形式为（文件路径 + 名称）key，文件内容 value*/// 自定义 InputFormat// 定义类继承 FileInputFormatpublic class WholeFileInputformat extends FileInputFormat&lt;Text, BytesWritable&gt;&#123; // 定义是否可切割，此程序不可切片，最终把所有文件封装到了 value 中 @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; // 实现一次读取一个完整文件封装为 K V @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader( InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// 自定义 RecordReader 类public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt;&#123; private Configuration configuration; private FileSplit split; private boolean isProgress = true; private BytesWritable value = new BytesWritable(); private Text k = new Text(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; // 初始化数据以及配置 this.split = (FileSplit)split; configuration = context.getConfiguration(); &#125; // 核心业务逻辑，即拼接自定义的 K @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (isProgress) &#123; // 定义缓存区，获取切片长度 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try &#123; // 获取文件系统，通过路径获取文件系统，一波反向操作 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 读取数据 fis = fs.open(path); // 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 输出文件内容 value.set(contents, 0, contents.length); // 获取文件路径及名称 String name = split.getPath().toString(); // 设置输出的 key 值 k.set(name); &#125; catch (Exception e) &#123; &#125;finally &#123; IOUtils.closeStream(fis); &#125; // 本切片已处理完，清标记位 isProgress = false; // 进行其它切片的处理 return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 编写 SequenceFileMapper 类处理流程public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt;&#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125;// 编写 SequenceFileReducer 类处理流程public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, values.iterator().next()); &#125;&#125;// 编写SequenceFileDriver类处理流程public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] &#123; \"e:/input/inputinputformat\", \"e:/output1\" &#125;; // 获取 job 对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 设置 jar 包存储位置、关联自定义的 mapper 和 reducer job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 设置输入的 inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 设置输出的 outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); // 设置 map 输出端的 K V 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 设置最终输出端的 K V 类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交 job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; OutputFormat默认的输出格式是 TextOutputFormat ，它把每条记录写为文本行，它的键和值可以是任意类型，因为 TextOutputFormat调用 toString() 方法把它们转换为字符串。 将 SequenceFileOutputFormat 输出作为后续 MapReduce 任务的输入，因为其格式紧凑，很容易被压缩。 自定义 为了实现控制最终文件的输岀路径和输出格式，可进行自定义。 自定义一个类继承 FileOutputFormat；改写 RecordWriter，具体改写输出数据的方法 write()。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 继承 FileOutputFormatpublic class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; // 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125;// 重写 write()public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream wingoOut = null; FSDataOutputStream otherOut = null; public FilterRecordWriter(TaskAttemptContext job) &#123; FileSystem fs; try &#123; // 获取文件系统 fs = FileSystem.get(job.getConfiguration()); // 创建输出文件路径 Path wingoPath = new Path(\"e:/wingo.log\"); Path otherPath = new Path(\"e:/other.log\"); // 创建输出流 atguiguOut = fs.create(wingoPath); otherOut = fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断是否包含 wingo 输出到不同文件 if (key.toString().contains(\"wingo\")) &#123; wingoOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; // 关闭资源 IOUtils.closeStream(wingoOut); IOUtils.closeStream(otherOut); &#125;&#125;// 将自定义的输出格式组件设置到 job 中job.setOutputFormatClass(FilterOutputFormat.class);// 虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat// 而 fileoutputformat 要输出一个 _SUCCESS 文件，所以，在这还得指定一个输出目录 Reduce Join order(1001 01 1) join pd(01 小米) 👉 result(001 小米 1) Bean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 合并后的 Bean 类public class TableBean implements Writable &#123; private String order_id; // 订单 id private String p_id; // 产品 id private int amount; // 产品数量 private String pname; // 产品名称 private String flag; // 表的标记 public TableBean() &#123; super(); &#125; public TableBean(String order_id, String p_id, int amount, String pname, String flag) &#123; super(); this.order_id = order_id; this.p_id = p_id; this.amount = amount; this.pname = pname; this.flag = flag; &#125; // Getter / Setter @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(order_id); out.writeUTF(p_id); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.order_id = in.readUTF(); this.p_id = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; @Override public String toString() &#123; return order_id + \"\\t\" + pname + \"\\t\" + amount + \"\\t\" ; &#125;&#125; Mapper12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123; String name; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取输入文件切片 FileSplit split = (FileSplit) context.getInputSplit(); // 获取输入文件名称 name = split.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取输入数据 String line = value.toString(); // 不同文件分别处理 if (name.startsWith(\"order\")) &#123;// 订单表处理 // 切割 String[] fields = line.split(\"\\t\"); // 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(\"\"); bean.setFlag(\"order\"); k.set(fields[1]); &#125;else &#123; // 产品表处理 // 切割 String[] fields = line.split(\"\\t\"); // 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(\"pd\"); bean.setAmount(0); bean.setOrder_id(\"\"); k.set(fields[0]); &#125; // 写出 context.write(k, bean); &#125;&#125; Reduce 处理压力大，并且极易产生数据倾斜。 12345678910111213141516171819202122232425262728293031323334353637383940public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 准备 bean 对象 TableBean pdBean = new TableBean(); // 便利 key = 1 的集合 for (TableBean bean : values) &#123; if (\"order\".equals(bean.getFlag())) &#123; // 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123; // 产品表 try &#123; // 此处已在 Map 做好按 p_id 排序工作，最后一定是一个 pd 表信息 // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 表的拼接 for(TableBean bean : orderBeans)&#123; bean.setPname (pdBean.getPname()); // 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125; Driver1234567891011121314151617181920212223242526272829303132333435public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 路径 args = new String[]&#123;\"e:/input/inputtable\",\"e:/output1\"&#125;; // 获取配置信息，或者 job 对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 指定本程序的 jar 包所在的本地路径 job.setJarByClass(TableDriver.class); // 指定本业务 job 要使用的 Mapper / Reducer 业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 指定 Mapper 输出数据的 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 指定最终输出的数据的 k v 类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 指定 job 的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; Map Join 适用于一张表十分小、一张表很大的场景。 采用 Distributed cache：在 Mapper 的 setup 阶段，将文件读取到缓存集合中。 job.addCacheFile(new URI(“file://e:/cache/pd.txt”)); Driver 添加缓存文件。 1234567891011121314151617181920212223242526272829303132333435363738394041public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 路径 args = new String[]&#123;\"e:/input/inputtable\",\"e:/output1\"&#125;; // 获取配置信息，或者 job 对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 指定本程序的 jar 包所在的本地路径 job.setJarByClass(TableDriver.class); // 指定本业务 job 要使用的 Mapper / Reducer 业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 指定 Mapper 输出数据的 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 指定最终输出的数据的 k v 类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 指定 job 的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 加载缓存数据 job.addCacheFile(new URI(\"file:///e:/input/inputcache/pd.txt\")); // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0 job.setNumReduceTasks(0); // 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; Map1234567891011121314151617181920212223242526272829303132333435363738394041424344public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); Text k = new Text(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 获取缓存的文件 URI[] cacheFiles = context.getCacheFiles(); String path = cacheFiles[0].getPath().toString(); BufferedReader reader = new BufferedReader (new InputStreamReader(new FileInputStream(path), \"UTF-8\")); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 切割 String[] fields = line.split(\"\\t\"); // 缓存数据到集合 (01 小米) pdMap.put(fields[0], fields[1]); &#125; // 关流 reader.close(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 截取 String[] fields = line.split(\"\\t\"); // 获取产品 id String pId = fields[1]; // 获取商品名称 String pdName = pdMap.get(pId); // 拼接 k.set(line + \"\\t\"+ pdName); // 写出 context.write(k, NullWritable.get()); &#125;&#125; 数据清洗 ETL Hadoop 为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。 Map12345678910111213141516171819202122232425262728293031323334353637383940414243public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取 1 行数据 String line = value.toString(); // 解析日志，即此条日志是否有意义 boolean result = parseLog(line,context); // 日志不合法退出 if (!result) &#123; return; &#125; // 设置 key k.set(line); // 写出数据 context.write(k, NullWritable.get()); &#125; // 解析日志 private boolean parseLog(String line, Context context) &#123; // 截取 String[] fields = line.split(\" \"); // 日志长度大于 11 的为合法 if (fields.length &gt; 11) &#123; // 系统计数器，记录一条合法的日志 context.getCounter(\"map\", \"true\").increment(1); return true; &#125;else &#123; // 记录一条不合法的日志 context.getCounter(\"map\", \"false\").increment(1); return false; &#125; &#125;&#125; Driver1234567891011121314151617181920212223242526272829303132333435363738394041public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 路径 args = new String[]&#123;\"e:/input/inputlog\", \"e:/output1\"&#125;; // 获取配置信息，或者 job 对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 指定本程序的 jar 包所在的本地路径 job.setJarByClass(TableDriver.class); // 指定本业务 job 要使用的 Mapper / Reducer 业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 指定 Mapper 输出数据的 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 指定最终输出的数据的 k v 类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 指定 job 的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 加载缓存数据 job.addCacheFile(new URI(\"file:///e:/input/inputcache/pd.txt\")); // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0 job.setNumReduceTasks(0); // 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 数据压缩 适当压缩是提高 Hadoop 运行效率的一种优化策略。 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩 / 解压缩算法，Hadoop 引入了编码 / 解码器 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 启动压缩功能的参数配置。 参数 阶段 建议 io.compression.codecs 输入压缩 文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress mapper 输出 这个参数设为 true 启用压缩 mapreduce.map.output.compress.codec mapper 输出 企业多使用 LZO 或 Snappy 编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress reducer 输出 这个参数设为 true 启用压缩 mapreduce.output.fileoutputformat.compress.codec reducer 输出 使用标准工具或者编解码器，如 gzip 和 bzip2 mapreduce.output.fileoutputformat.compress.type reducer 输出 SequenceFile 压缩类型： NONE 和 BLOCK Driver 定义压缩123456789101112131415161718192021// Map 输出端采用压缩public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); // 开启 map 端输出压缩 configuration.setBoolean(\"mapreduce.map.output.compress\", true); // 设置 map 端输出压缩方式 configuration.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class); // ... &#125;&#125;// Reduce 输出端采用压缩// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); YARN 资源调度器 Hadoop 作业调度器主要有三种： FIFO 先到先服务、Capacity Scheduler 和 Fair Schedule。 1234567&lt;!-- yarn-default.xml --&gt;&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; Capacity Scheduler，容量调度器。 支持多队列，每个队列配置一定的资源量，每个队列采用 FIFO 调度策略；为防止同一用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定；运行任务数 / 计算资源 👉 王最闲的队列里交任务。 Fair Scheduler，公平调度器。 支持多队列多用户，每个队列中的资源量可以配置，同队列中的作业公平共享队列中所有资源；每个队列中的 job 按照优先级分配资源，优先级越高分配的资源越多，但是每个 job 都会分配到资源以确保公平；（差额：job 需要的资源 - job 实际获取的资源）在同一个队列中，job 旳资源缺额越大，越先获得资源优先执行。 推测执行 某些任务过慢，则为其启动多个备份任务，同时运行，谁先运行完成用谁的结果。 每个 Task 只能有一个备份任务；当前 Job 已完成的 Task 必须不小于0.05（5%）；开启推测执行参数设置。mapred-site.xml 文件中默认是打开的。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}]},{"title":"sklearn","slug":"机器学习/sklearn","date":"2020-04-23T03:07:44.000Z","updated":"2020-08-02T08:18:31.525Z","comments":true,"path":"2020/04/23/机器学习/sklearn/","link":"","permalink":"http://yoursite.com/2020/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn/","excerpt":"简单了解机器学习的基本概念以及常规流程，并简单介绍 Python 的机器学习库 sklearn 的使用。","text":"简单了解机器学习的基本概念以及常规流程，并简单介绍 Python 的机器学习库 sklearn 的使用。 Jupyter 安装配置123456pip3 install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple/ # 国内镜像下载 jupyterjupyter notebook --generate-config # 生成默认的配置文件## Writing default config to: C:\\Users\\wingo\\.jupyter\\jupyter_notebook_config.py## C.NotebookApp.notebook_dir='' 修改此项配置jupyter notebook # 启动 Jupyter notebookpip3 install -U scikit-learn -i https://pypi.tuna.tsinghua.edu.cn/simple/ # 下载库 机器学习概述数据 👉 模型 👉 预测，即从历史数据中获得规律。 数据集的构成：特征值 + 目标值。 根据是否有目标值，可以分为无监督学习（无目标值）、监督学习（有目标值）。 根据数据的形式，可以分为分类问题（类别）、回归问题（连续性数据）。 机器学习的普遍开发流程： 1）获取数据；2）数据处理；3）特征工程；4）模型训练；5）模型评估；6）实际应用。 数据获取sklearn 自带了许多数据，供开发者进行测试使用。 123456789# 导入满满的数据库from sklearn import datasets# 分割数据的模块，把数据集分为训练集和测试集from sklearn.model_selection import train_test_split# 载入数据，经典鸢尾花数据iris = datasets.load_iris()# random_state 为了保证程序每次运行都分割一样的训练集合测试集。否则，同样的算法模型在不同的训练集和测试集上的效果不一样。x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0) 查看此数据集的详细描述： 数据集的划分： 训练数据，用于训练，构建模型；测试数据，在模型检验时使用，用于评估模型是否有效。 特征工程 Feature Engineering：特征工程就是对特征值进行处理，选择有意义的特征输入机器学习的算法和模型进行训练。 字典特征提取12345678910111213141516171819202122232425262728from sklearn.feature_extraction import DictVectorizer# list:[dict&#123;&#125;, dict&#123;&#125;, dict&#123;&#125;]data = [&#123;'city': '北京','temperature':100&#125;, &#123;'city': '上海','temperature':60&#125;, &#123;'city': '深圳','temperature':30&#125;]# 实例化一个转换器类，此处 sparse=True 设置结果为稀疏矩阵方便查看transfer = DictVectorizer(sparse=True)# 调用 fit_transform() 进行特征提取data_new = transfer.fit_transform(data)# 打印特征提取结果print(\"data_new_sparse:\\n\", data_new, \"\\n\", type(data_new))print(\"data_new:\\n\", data_new.toarray())print(\"特征名字:\\n\", transfer.get_feature_names())\"\"\" 打印结果data_new_sparse: (0, 1) 1.0 (0, 3) 100.0 (1, 0) 1.0 (1, 3) 60.0 (2, 2) 1.0 (2, 3) 30.0 &lt;class 'scipy.sparse.csr.csr_matrix'&gt;data_new: [[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 30.]]特征名字: ['city=上海', 'city=北京', 'city=深圳', 'temperature']\"\"\" 由上面的例子可以看出，经过 sklearn 的字典特征转换后，其类型转换为 matrix，即一个矩阵类型。 原始数据中的特征值只有 2 个：city、temperature；但转换出来的特征矩阵却有 4 列，这是为了将特征值变量转换为机器学习算法易于利用的一种形式，此转换过程称为 one hot 编码。 文本特征提取CountVectorizer123456789101112131415from sklearn.feature_extraction.text import CountVectorizerdata = [\"How to shift your mindset and choose your future\", \"I like python, Do you like python?\"]# 实例化一个转换器类，可以屏蔽一些没有意义的词语transfer = CountVectorizer(stop_words=[\"and\", \"to\"])# 调用 fit_transformdata_new = transfer.fit_transform(data)print(\"data_new:\\n\", data_new.toarray())print(\"特征名字：\\n\", transfer.get_feature_names())\"\"\" 打印结果data_new: [[1 0 1 1 0 1 0 1 0 2] [0 1 0 0 2 0 2 0 1 0]]特征名字： ['choose', 'do', 'future', 'how', 'like', 'mindset', 'python', 'shift', 'you', 'your']\"\"\" 思考：若是对一句中文文本进行文本特征提取，那会出现什么结果？ 对于中文文本，因为词语之间不存在空格，所以需要分词库先进行分词：&quot; &quot;.join(list(jieba.cut(text))) TfidfVectorizer Term Frequency - Inverse Document Frequency，即词频 - 逆文本频率，由 TF 和 IDF 组成。 TF，即普通的词频统计，这个比较好理解，IDF，用于反应一个词的重要性，进而修正仅仅用词频表示的词特征值。比如，”to” 这个词的词频非常高，但它在每一篇文章中都出现了，那么它这个高词频就没有什么太大的意义。 1234567891011121314151617181920212223242526272829303132333435from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerimport jiebadata = [\"今天是星期一，有点快乐\", \"今天是星期二，无敌快乐\", \"今天是星期三，超级快乐\"]data_new = []for sent in data: data_new.append(\" \".join(list(jieba.cut(sent))))\"\"\"data_new['今天 是 星期一 ， 有点 快乐', '今天 是 星期二 ， 无敌 快乐', '今天 是 星期三 ， 超级 快乐']\"\"\"# 转换器transfer = TfidfVectorizer()transfer_normal = CountVectorizer()# 特征提取data_final = transfer.fit_transform(data_new)data_normal = transfer_normal.fit_transform(data_new)print(\"data_new:\\n\", data_final.toarray())print(\"data_new_normal:\\n\", data_normal.toarray())print(\"特征名字：\\n\", transfer.get_feature_names())\"\"\"打印结果data_new: [[0.35959372 0.35959372 0. 0.6088451 0. 0. 0.6088451 0. ] [0.35959372 0.35959372 0.6088451 0. 0. 0.6088451 0. 0. ] [0.35959372 0.35959372 0. 0. 0.6088451 0. 0. 0.6088451 ]]data_new_normal: [[1 1 0 1 0 0 1 0] [1 1 1 0 0 1 0 0] [1 1 0 0 1 0 0 1]]特征名字： ['今天', '快乐', '无敌', '星期一', '星期三', '星期二', '有点', '超级']\"\"\" 分析上述结果，普通的词频分析得出的结果是词频相同，而 tf-idf 分析，对于每个句子中都出现的词的重要性会有所降低。 特征预处理 无量纲化：进行无量纲化后表征不同属性（单位不同）的各特征之间才有可比性，如 1cm 与 0.1kg 如何比较？ 归一化 无法处理异常值（最大值，最小值）； 123456789from sklearn.preprocessing import MinMaxScalerx_mm = [[1,-1,2],[2,0,0],[0,1,-1]]x_mm_new = MinMaxScaler().fit_transform(x_mm)\"\"\"x_mm_newarray([[0.5 , 0. , 1. ], [1. , 0.5 , 0.33333333], [0. , 1. , 0. ]])\"\"\" 标准化 标准差（描述样本集中程度，异常值对总样本的集中程度的影响不大），在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 12345678from sklearn.preprocessing import StandardScalerx = [[1,2,3],[4,5,6],[1,2,1]]x_new = StandardScaler().fit_transform(x)\"\"\"x_newarray([[-0.70710678, -0.70710678, -0.16222142], [ 1.41421356, 1.41421356, 1.29777137], [-0.70710678, -0.70710678, -1.13554995]])\"\"\" 特征降维 这里的降维的维度，不是指矩阵、向量、标量等，而是指降低特征的个数，让最终得到的数据中的每个特征之间互不相关。 思考：特征之间相关度过高会有什么问题？？？若 m、n 都可以得出 r，那么去掉 m 、n 中的任意一个特征对结果都不会影响，在一个庞大的数据中，去掉某些相关性特别高的特征可以节省很多算力与内存空间，模型更加高效。 Filter 方差选择法：低方差特征过滤，即过滤差别不大的特征。 12345678910111213141516171819202122232425262728293031323334from sklearn.feature_selection import VarianceThresholdimport pandas as pdfrom scipy.stats import pearsonr# 获取数据data = pd.read_csv(\"factor_returns.csv\")# 保留每一行，保留从[第二列到倒数第二列)data = data.iloc[:, 1:-2]data_v = data.valuesprint(\"data:\\n\", data_v.shape)# 实例化一个转换器类transfer = VarianceThreshold(threshold=10)# 调用 fit_transformdata_new = transfer.fit_transform(data)print(\"data_new:\\n\",data_new.shape)# 计算某两个变量之间的相关系数r1 = pearsonr(data[\"pe_ratio\"], data[\"pb_ratio\"])print(\"pe_ratio 与 pb_ratio 之间的相关性：\\n\", r1)r2 = pearsonr(data['revenue'], data['total_expense'])print(\"revenue 与 total_expense 之间的相关性：\\n\", r2)\"\"\"打印结果data: (2318, 9)data_new: (2318, 7)pe_ratio 与 pb_ratio 之间的相关性： (-0.0043893227799362685, 0.8327205496564927)revenue 与 total_expense 之间的相关性： (0.9958450413136116, 0.0)\"\"\" pearsonr：皮尔逊相关系数，取值范围：–1≤ r ≤+1。 若特征与特征之间相关性很高： 选取其中一个；加权求和；主成分分析； 主成分分析 PCA，Principal components analysis。 1transfer = PCA(n_components=0.95) # 进行 PCA 降维，并保存 95% 的信息。 案例分析：购物车 预测某个 user_id 下次会购买 aisle。 123456789101112131415161718192021222324252627282930313233343536import pandas as pd# 获取数据# 订单与商品信息order_products = pd.read_csv(\"./instacart/order_products__prior.csv\")# 商品详细信息products = pd.read_csv(\"./instacart/products.csv\")# 订单详细信息orders = pd.read_csv(\"./instacart/orders.csv\")# 商品所属类别信息aisles = pd.read_csv(\"./instacart/aisles.csv\")# 打印特征值找规律，将所有的特征值合并到一张表中然后进行 PCA 降维print(order_products.columns.values.tolist(),\"\\n\", products.columns.values.tolist(), \"\\n\", orders.columns.values.tolist(),\"\\n\", aisles.columns.values.tolist())\"\"\"打印结果['order_id', 'product_id', 'add_to_cart_order', 'reordered'] ['product_id', 'product_name', 'aisle_id', 'department_id'] ['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order'] ['aisle_id', 'aisle']\"\"\"tab1 = pd.merge(aisles, products, on=[\"aisle_id\", \"aisle_id\"])tab2 = pd.merge(tab1, order_products, on=[\"product_id\", \"product_id\"])tab3 = pd.merge(tab2, orders, on=[\"order_id\", \"order_id\"])tab3.columns.values.tolist()\"\"\"打印结果['aisle_id', 'aisle', 'product_id', 'product_name', 'department_id', 'order_id', 'add_to_cart_order', 'reordered', 'user_id', 'eval_set', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\"\"\"# 虽然所有的数据特征都在一个表中，但表的形式不是需要的 user_id 与 aisle 的关系# 利用交叉表反映的 user_id 和 aisle 之间的关系table = pd.crosstab(tab3[\"user_id\"], tab3[\"aisle\"]) 123456789101112# 有大量的特征值，对数据进行降维处理from sklearn.decomposition import PCAdata = table[:10000]data.shape# (10000, 134)from sklearn.decomposition import PCA# 实例化一个转换器类transfer = PCA(n_components=0.95)# 调用 fit_transformdata_new = transfer.fit_transform(data)data_new.shape# (10000, 42) 在上述案例中，通过 PCA 降维，将 134 个特征值降到了 42 个，并且还保留了原本数据 95% 的信息，可见 PCA 降维还是挺好用的。 分类算法sklearn 的转换器和估计器。 转换器（transformer），特征工程的父类； fit_transform() 其实是两个过程：fit() 计算每一列的平均值、标准差；transform() 实现最终的转换。 估计器（estimator），机器学习算法的实现。 1234estimator.fit(x_train, y_train) # 计算生成模型y_predict = estimator.predict(x_test) # 对测试数据进行测试y_test == y_predict # 测试结果与原有结果进行对比accuracy = estimator.score(x_test, y_test) # 计算准确率 K - 近邻算法 即，根据邻近的类型来推测出自身类型。邻近指的是距离： 欧氏距离；曼哈顿距离（绝对值距离），明可夫斯基距离。 鸢尾花分类案例12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier# 获取数据iris = load_iris()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)# 特征工程transfer = StandardScaler()# 训练集和测试集都要做一样的处理x_train = transfer.fit_transform(x_train)# 测试基使用测试机的 fit() 进行计算x_test = transfer.transform(x_test)# KNN 算法预估器，自定义距离（不断尝试出一个准确率较高的距离）estimator = KNeighborsClassifier(n_neighbors=3)estimator.fit(x_train, y_train)# 模型评估y_predict = estimator.predict(x_test)print(\"y_predict:\\n\", y_predict)print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)\"\"\"结果打印y_predict: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]直接比对真实值和预测值: [ True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True False]\"\"\"# 计算准确率score = estimator.score(x_test, y_test)print(\"准确率为：\\n\", score)\"\"\"结果打印准确率为： 0.9736842105263158\"\"\" 优点：简单，易于理解，易于实现，无需训练;缺点：必须指定 K 值，K 值选择不当则分类精度不能保证；懒惰算法，对测试样本分类时的计算量大，内存开销大；使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试。 模型选择与调优 有时候得到的模型的准确率不尽人意，这时就需要使用某些算法对模型进行选择和调优了。 交叉验证 验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成 3、5 和 10 份。 网格搜索 又称为超参数搜索。对 KNN 的 K 值进行参数调优，把一组 K 值传入网格搜索中，找出最优值。 鸢尾花 K 值调优1234567891011121314151617181920212223estimator = KNeighborsClassifier()# 加入网格搜索与交叉验证# 参数准备param_dict = &#123;\"n_neighbors\": [1, 3, 5, 7, 9, 11]&#125;# cv=10，进行10折交叉验证estimator = GridSearchCV(estimator, param_grid=param_dict, cv=10)estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print(\"y_predict:\\n\", y_predict)print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)score = estimator.score(x_test, y_test)print(\"准确率为：\\n\", score)# 最佳参数：best_params_print(\"最佳参数：\\n\", estimator.best_params_)# 最佳结果：best_score_print(\"最佳结果：\\n\", estimator.best_score_)# 最佳估计器：best_estimator_print(\"最佳估计器:\\n\", estimator.best_estimator_)# 交叉验证结果：cv_results_print(\"交叉验证结果:\\n\", estimator.cv_results_) 朴素贝叶斯 Naive Bayesian。朴素？即，假设特征与特征之间是相互独立。 应用场景：文本分类，单词作为特征。 决策树哪个特征值的信息增益大，那就应该先用这个特征值做过滤。 缺点：容易产生过拟合，即一个 overfitted 模型记住太多 training data 的细节从而降低了 generalization 的能力。 欠拟合：光看书不做题觉得自己会了，上了考场啥都不会；过拟合: 做课后题全都能做对，上了考场还是啥都不会。 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier, export_graphviz# 获取数据集iris = load_iris()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)# 决策树预估器estimator = DecisionTreeClassifier(criterion=\"entropy\")estimator.fit(x_train, y_train)# 模型评估# 直接比对真实值和预测值y_predict = estimator.predict(x_test)print(\"y_predict:\\n\", y_predict)print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)# 计算准确率score = estimator.score(x_test, y_test)print(\"准确率为：\\n\", score)\"\"\"结果打印y_predict: [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 1 0 0 1 1 1 0 0 0]直接比对真实值和预测值: [ True True True True True True True False True True True True True True True True True True False True True True True True True True True True True False True True True True True True True True]准确率为： 0.9210526315789473\"\"\"# 可视化决策树export_graphviz(estimator, out_file=\"iris_tree.dot\", feature_names=iris.feature_names) 在线可视化 随机森林 森林：包含多个决策树的分类器。 12345678# 数据同决策树from sklearn.ensemble import RandomForestClassifier# 使用随机森林estimator = RandomForestClassifier()estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print(estimator.score(x_test, y_test)) 案例分析：签到预测 根据给定的数据，预测人们最可能在地图上的哪个位置进行签到。 12345import pandas as pd# 获取数据data = pd.read_csv(\"./FBlocation/train.csv\")data.head() 123456789101112131415161718192021# 缩小范围，方便实验data = data.query(\"x &lt; 2.5 &amp; x &gt; 2 &amp; y &lt; 1.5 &amp; y &gt; 1.0\")# 对数据进行处理，这个时间看的很难受# 时间处理成便于观察的形式time_value = pd.to_datetime(data[\"time\"], unit=\"s\")time_value.head()# 年月的差别不大，可不考虑\"\"\"结果打印112 1970-01-08 05:06:14180 1970-01-08 01:29:55367 1970-01-07 17:01:07874 1970-01-02 15:52:461022 1970-01-03 09:46:33Name: time, dtype: datetime64[ns]\"\"\"# 得到时间列date = pd.DatetimeIndex(time_value)# 给数据增加日期，星期，时间这些特征值data[\"day\"] = date.daydata[\"weekday\"] = date.weekdaydata[\"hour\"] = date.hour 12# 根据 place_id 成组，过滤签到次数比较少的地点place_count = data.groupby(\"place_id\").count()[\"row_id\"] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 过滤place_count[place_count &gt; 3].head()# 根据得到的 place_id 对数据进行过滤data_final = data[data[\"place_id\"].isin(place_count[place_count &gt; 3].index.values)]# 筛选特征值和目标值x = data_final[[\"x\", \"y\", \"accuracy\", \"day\", \"weekday\", \"hour\"]]y = data_final[\"place_id\"]from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import GridSearchCV# 数据集划分x_train, x_test, y_train, y_test = train_test_split(x, y)# 特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# KNN 算法预估器estimator = KNeighborsClassifier()# 加入网格搜索与交叉验证# 参数准备param_dict = &#123;\"n_neighbors\": [3, 5, 7, 9]&#125;estimator = GridSearchCV(estimator, param_grid=param_dict, cv=3)estimator.fit(x_train, y_train)# 模型评估# 直接比对真实值和预测值y_predict = estimator.predict(x_test)print(\"y_predict:\\n\", y_predict)print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)# 计算准确率score = estimator.score(x_test, y_test)print(\"准确率为：\\n\", score)# 最佳参数：best_params_print(\"最佳参数：\\n\", estimator.best_params_)# 最佳结果：best_score_print(\"最佳结果：\\n\", estimator.best_score_)# 最佳估计器：best_estimator_print(\"最佳估计器:\\n\", estimator.best_estimator_)# 交叉验证结果：cv_results_print(\"交叉验证结果:\\n\", estimator.cv_results_) 案例分析：生存预测 根据人的信息预测此人在此灾难中存活下来的概率。 123456789101112131415161718192021222324252627282930313233343536import pandas as pdtitanic = pd.read_csv(\"titanic.csv\")# 筛选特征值和目标值x = titanic[[\"pclass\", \"age\", \"sex\"]]y = titanic[\"survived\"]# 缺失值处理x[\"age\"].fillna(x[\"age\"].mean(), inplace=True)# 转换成字典，方便处理x = x.to_dict(orient=\"records\")from sklearn.model_selection import train_test_split# 数据集划分x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)# 字典特征抽取from sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifiertransfer = DictVectorizer()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print(\"y_predict:\\n\", y_predict)print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)score = estimator.score(x_test, y_test)print(\"准确率为：\\n\", score) 回归算法线性回归 目标值为连续性的数据。 正规方程12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error# 正规方程的优化方法对波士顿房价进行预测# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = LinearRegression()estimator.fit(x_train, y_train)# 得出模型print(\"正规方程-权重系数为：\\n\", estimator.coef_)print(\"正规方程-偏置为：\\n\", estimator.intercept_)# 模型评估y_predict = estimator.predict(x_test)print(\"预测房价：\\n\", y_predict)error = mean_squared_error(y_test, y_predict)print(\"正规方程-均方误差为：\\n\", error) 梯度下降 一个不断逼近的，勤奋努力的算法。 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import SGDRegressorfrom sklearn.metrics import mean_squared_error# 梯度下降的优化方法对波士顿房价进行预测# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = SGDRegressor(learning_rate=\"constant\", eta0=0.01, max_iter=10000, penalty=\"l1\")estimator.fit(x_train, y_train)# 得出模型print(\"梯度下降-权重系数为：\\n\", estimator.coef_)print(\"梯度下降-偏置为：\\n\", estimator.intercept_)# 模型评估y_predict = estimator.predict(x_test)print(\"预测房价：\\n\", y_predict)error = mean_squared_error(y_test, y_predict)print(\"梯度下降-均方误差为：\\n\", error) 岭回归 带有 L2 正则化的线性回归：岭回归 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import Ridgefrom sklearn.metrics import mean_squared_error# 梯度下降的优化方法对波士顿房价进行预测# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = Ridge(alpha=0.5, max_iter=10000)estimator.fit(x_train, y_train)# 得出模型print(\"岭回归-权重系数为：\\n\", estimator.coef_)print(\"岭回归-偏置为：\\n\", estimator.intercept_)# 模型评估y_predict = estimator.predict(x_test)print(\"预测房价：\\n\", y_predict)error = mean_squared_error(y_test, y_predict)print(\"岭回归-均方误差为：\\n\", error) 逻辑回归 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。 精确率 Precision 即，查的准不准。 召回率 Recall 即，查的全不全。 F1-score 模型的稳健性。 案例分析：肿瘤预测12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import pandas as pdimport numpy as np# 读取数据path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']data = pd.read_csv(path, names=column_name)# 缺失值处理# 替换 》 np.nandata = data.replace(to_replace=\"?\", value=np.nan)# 删除缺失样本data.dropna(inplace=True)# 检查是否还存在缺失值data.isnull().any()# 划分数据集from sklearn.model_selection import train_test_split# 筛选特征值和目标值x = data.iloc[:, 1:-1]y = data[\"Class\"]x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)# 标准化from sklearn.preprocessing import StandardScalertransfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 逻辑回归预测from sklearn.linear_model import LogisticRegressionestimator = LogisticRegression()estimator.fit(x_train, y_train)# 逻辑回归的模型参数：回归系数和偏置print(\"逻辑回归-回归系数：\", estimator.coef_, \"逻辑回归-偏置：\" , estimator.intercept_)y_predict = estimator.predict(x_test)print(\"y_predict:\\n\", y_predict)print(\"直接比对真实值和预测值:\\n\", y_test == y_predict)score = estimator.score(x_test, y_test)print(\"准确率为：\\n\", score)# 查看精确率、召回率、F1-scorefrom sklearn.metrics import classification_reportreport = classification_report(y_test, y_predict, target_names=[\"良性\", \"恶性\"])# y_true：每个样本的真实类别，必须为 0 反，1 正标记# 将 y_test 转换成 0 1y_true = np.where(y_test &gt; 3, 1, 0)from sklearn.metrics import roc_auc_scoreroc_auc_score(y_true, y_predict) K-means 无监督算法。 购物车案例123456789101112# 预估器流程from sklearn.cluster import KMeansestimator &#x3D; KMeans(n_clusters&#x3D;3)estimator.fit(data_new)y_predict &#x3D; estimator.predict(data_new)y_predict[:300]# 模型评估-轮廓系数from sklearn.metrics import silhouette_scoresilhouette_score(data_new, y_predict)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"}]},{"title":"RabbitMQ 初步使用","slug":"后台技术/RabbitMQ 初步使用","date":"2020-04-20T06:02:25.000Z","updated":"2020-07-10T04:15:16.053Z","comments":true,"path":"2020/04/20/后台技术/RabbitMQ 初步使用/","link":"","permalink":"http://yoursite.com/2020/04/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/RabbitMQ%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"RabbitMQ 的简单介绍及项目使用。","text":"RabbitMQ 的简单介绍及项目使用。 在虚拟机中基于 Docker 安装 RabbitMQ 镜像，启动镜像并暴露其端口。 1docker run -d -p 5672:5672 -p 15672:15672 --hostname my_rabbitmq --name rabbitmq rabbitmq:management 访问 [虚拟机 IP]:15672 进入 RabbitMQ 的控制台页面，默认账号密码皆为 guest。 基本概念 高级消息队列协议，即 Advanced Message Queuing Protocol（AMQP）是面向消息中间件提供的开放的应用层协议，其设计目标是对于消息的排序、路由（包括点对点和订阅-发布）、保持可靠性、保证安全性。 RabbitMQ 实现了 AMQP。 RabbitMQ 的基本使用流程： 客户端连接到消息队列服务器，打开一个 channel；客户端声明一个 exchange，并设置相关属性；客户端声明一个 queue，并设置相关属性；客户端使用 routing key，在 exchange 和 queue 之间建立好绑定关系；客户端投递消息到 exchange。 Broker：简单来说就是消息队列服务器实体。Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。Queue：消息队列载体，每个消息都会被投入到一个或多个队列。Binding：绑定，它的作用就是把 exchange 和 queue 按照路由规则绑定起来。Routing Key：路由关键字，exchange 根据这个关键字进行消息投递。vhost：虚拟主机，一个 broker 里可以开设多个 vhost，用作不同用户的权限分离。producer：消息生产者，就是投递消息的程序。consumer：消息消费者，就是接受消息的程序。channel：消息通道，在客户端的每个连接里，可建立多个 channel，每个 channel 代表一个会话任务。 简单示例 编写一个基本的 consumer 以及 provider 进行消息的接受与投递。 simple-consumer 编写一个简单的消费者连接到 RabbitMQ，并且初始化所需的 Exchange、Queue、Routing Key、Binding 等。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 123456789spring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000spring.rabbitmq.listener.simple.acknowledge-mode=manualspring.rabbitmq.listener.simple.concurrency=5spring.rabbitmq.listener.simple.max-concurrency=10 Receiver 通过 @RabbitListener进行初始化并绑定：可在配置文件中编写后通过${xxx.xxx...}进行取值。 ！！！注意！！！此处的 Order 类依赖于 provider 模块的 Order 类。 12345678910111213141516171819202122@Componentpublic class RabbitReceiver &#123; @RabbitListener( bindings = @QueueBinding( value = @Queue(value = \"queue-1\", durable=\"true\"), exchange = @Exchange(value = \"exchange-1\", durable = \"true\", type = \"topic\", ignoreDeclarationExceptions = \"true\"), key = \"springboot.*\" ) ) @RabbitHandler public void onOrderMessage(@Payload Order order, Channel channel, @Headers Map&lt;String, Object&gt; headers) throws Exception &#123; System.err.println(\"--------------------------------------\"); System.err.println(\"消费端 Order: \" + order.getId()); Long deliveryTag = (Long)headers.get(AmqpHeaders.DELIVERY_TAG); //手工ACK channel.basicAck(deliveryTag, false); &#125;&#125; 运行项目。 提示：连接成功。打开 RabbitMQ 控制台。 可以看到有一条来自本地端口 9371 的连接。 查看 Exchange 可以看到客户端所绑定的规则已经被自动初始化（当然也可以选择手工绑定） simple-provider 编写一个简单的提供者连接到 RabbitMQ，并发送消息到对应的队列中。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910spring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000# 启用回调spring.rabbitmq.publisher-confirm-type=correlatedspring.rabbitmq.publisher-returns=truespring.rabbitmq.template.mandatory=true 新建一个实体对象用于发送。 12345678910public class Order implements Serializable &#123; private static final long serialVersionUID = -3603050220195298978L; private String id; private String name; // ...&#125; Sender 编写发送方法，并自定义回调形式。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class RabbitSender &#123; @Autowired private RabbitTemplate rabbitTemplate; // 回调函数: confirm 确认 final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.err.println(\"correlationData: \" + correlationData); System.err.println(\"ack: \" + ack); if(!ack)&#123; System.err.println(\"异常处理....\"); &#125; &#125; &#125;; // 回调函数: return 返回 final RabbitTemplate.ReturnCallback returnCallback = new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(org.springframework.amqp.core.Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.err.println(\"return exchange: \" + exchange + \", routingKey: \" + routingKey + \", replyCode: \" + replyCode + \", replyText: \" + replyText); &#125; &#125;; // 发送消息方法调用: 构建自定义对象消息 public void sendOrder(Order order) throws Exception &#123; rabbitTemplate.setConfirmCallback(confirmCallback); rabbitTemplate.setReturnCallback(returnCallback); // id + 时间戳 全局唯一 CorrelationData correlationData = new CorrelationData(\"987654321\"); rabbitTemplate.convertAndSend(\"exchange-1\", \"springboot.order\", order, correlationData); &#125;&#125; 编写测试方法进行测试。 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestclass SimpleProviderApplicationTests &#123; @Autowired private RabbitSender rabbitSender; @Test public void testSender() throws Exception &#123; Order order = new Order(\"001\", \"第一个订单\"); rabbitSender.sendOrder(order); &#125;&#125; 控制台打印了了自定义回调的确认信息。 RabbitMQ 控制台可以查看到发送的消息的详细信息。 启动消费者后，消费者监听到队列中有消息并了进行消费。 可靠交付 可靠交付即将 RabbitMQ 处理订单的状态持久化到数据库中，并根据此状态表对不同状态的交付任务进行处理： 1）在达到规定的最大重试次数之前，定时任务在规定的时间间隔自动进行任务的重新交付；2）任务达到了最大的重试次数后，为避免浪费资源将其状态更新为失败，之后人工处理。 12345678910111213141516171819-- 表 my_order 订单结构CREATE TABLE IF NOT EXISTS `my_order` ( `id` varchar(128) NOT NULL, -- 订单 ID `name` varchar(128), -- 订单名称 其他业务熟悉忽略 `message_id` varchar(128) NOT NULL, -- 消息唯一 ID PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- 表 log 消息记录结构CREATE TABLE IF NOT EXISTS `log` ( `message_id` varchar(128) NOT NULL, -- 消息唯一 ID `message` varchar(4000) DEFAULT NULL, -- 消息内容 `try_count` int(4) DEFAULT '0', -- 重试次数 `status` varchar(10) DEFAULT '', -- 消息投递状态：0 投递中 1 投递成功 2 投递失败 `next_retry` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', -- 下一次重试时间 `create_time` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', -- 创建时间 `update_time` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', -- 更新时间 PRIMARY KEY (`message_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 项目依赖1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- RabbitMQ --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mybatis-plus --&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- 常用工具类 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.12&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819## 配置数据源spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/rabbitmq_test?useSSL=falsespring.datasource.driver-class-name=com.mysql.jdbc.Driver# 打印 SQL 语句#mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImplmybatis-plus.mapper-locations=classpath:/mapper/*.xml## RabbitMQspring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000# 回调配置spring.rabbitmq.publisher-confirm-type=correlatedspring.rabbitmq.publisher-returns=truespring.rabbitmq.template.mandatory=true 新建一个 rabbitmq-entity 模块，编写传输实体类。 12345678910public class Order implements Serializable &#123; private static final long serialVersionUID = 2584584237793643231L; private String id; private String name; private String messageId; //...&#125; 新建一个 stable-provider 模块，编写可靠的交付者。 编写状态的实体类，以及常量类 12345678910111213@Datapublic class Log implements Serializable &#123; private static final long serialVersionUID = -8716210355448974538L; private String messageId; private String message; private Integer tryCount = 0; private String status; private Date nextRetry; private Date createTime; private Date updateTime;&#125; 1234567891011121314151617public class Constant &#123; private Constant() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 订单状态 0 表示待发送 */ public static final String ORDER_SENDING = \"0\"; /** 订单状态 1 表示发送成功 */ public static final String ORDER_SEND_SUCCESS = \"1\"; /** 订单状态 2 表示发送失败 */ public static final String ORDER_SEND_FAILURE = \"2\"; /** 订单超时时间，即一分钟后订单进行交付重试 */ public static final Integer ORDER_TIMEOUT = 1; /** 订单最大重试次数 */ public static final Integer ORDER_RETRY_MAX_TIMES = 3;&#125; 编写 Mapper。 1234567891011121314151617181920212223242526@Componentpublic interface LogMapper extends BaseMapper&lt;Log&gt; &#123; /** * 更新最终消息发送结果 成功 or 失败 * @param messageId 消息的唯一 ID * @param status 订单状态 * @param updateTime 订单状态更新时间 */ void changeBrokerMessageLogStatus(@Param(\"messageId\") String messageId , @Param(\"status\") String status , @Param(\"updateTime\") Date updateTime ); /** * 查询消息状态为 0 且已经超时的消息集合 * @return 符合条件的订单集合 */ List&lt;Log&gt; query4StatusAndTimeoutMessage(); /** * @param messageId 消息的唯一 ID * @param date 时间 */ void update4ReSend(@Param(\"messageId\") String messageId, @Param(\"updateTime\") Date date);&#125; 12345678910// Order 为 SQL 关键字，并且只有一个插入方法，故完全使用自定义方法不继承@Componentpublic interface OrderMapper&#123; /** * @param order 订单 */ void insert(Order order);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wingo.stableprovider.mapper.LogMapper\"&gt; &lt;!-- 通用查询映射结果 --&gt; &lt;resultMap id=\"BaseResultMap\" type=\"com.wingo.stableprovider.entity.Log\"&gt; &lt;id column=\"message_id\" property=\"messageId\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"message\" property=\"message\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"try_count\" property=\"tryCount\" jdbcType=\"INTEGER\" /&gt; &lt;result column=\"status\" property=\"status\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"next_retry\" property=\"nextRetry\" jdbcType=\"TIMESTAMP\" /&gt; &lt;result column=\"create_time\" property=\"createTime\" jdbcType=\"TIMESTAMP\" /&gt; &lt;result column=\"update_time\" property=\"updateTime\" jdbcType=\"TIMESTAMP\" /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=\"Base_Column_List\"&gt; message_id, message, try_count, status, next_retry, create_time, update_time &lt;/sql&gt; &lt;update id=\"changeBrokerMessageLogStatus\" &gt; UPDATE log SET status = #&#123;status&#125;, update_time = #&#123;updateTime&#125; WHERE message_id = #&#123;messageId&#125; &lt;/update&gt; &lt;select id=\"query4StatusAndTimeoutMessage\" resultMap=\"BaseResultMap\"&gt; SELECT &lt;include refid=\"Base_Column_List\" /&gt; FROM log WHERE status = '0' AND next_retry &lt;![CDATA[ &lt;= ]]&gt; sysdate() &lt;/select&gt; &lt;update id=\"update4ReSend\" &gt; UPDATE log SET try_count = try_count + 1, update_time = #&#123;updateTime&#125; WHERE message_id = #&#123;messageId&#125; &lt;/update&gt;&lt;/mapper&gt; 1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wingo.stableprovider.mapper.OrderMapper\"&gt; &lt;!-- 通用查询映射结果 --&gt; &lt;resultMap id=\"BaseResultMap\" type=\"com.wingo.entity.Order\"&gt; &lt;id column=\"id\" property=\"id\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"name\" property=\"name\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"message_id\" property=\"messageId\" jdbcType=\"VARCHAR\" /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=\"Base_Column_List\"&gt; id, name, message_id &lt;/sql&gt; &lt;insert id=\"insert\" parameterType=\"com.wingo.entity.Order\" &gt; INSERT INTO my_order (id, name, message_id) VALUES (#&#123;id&#125;, #&#123;name&#125;, #&#123;messageId&#125;) &lt;/insert&gt;&lt;/mapper&gt; 编写一个服务用于向数据库初始化订单数据以及消息状态信息。 123456789101112131415161718192021222324252627282930313233@Servicepublic class OrderService &#123; @Autowired private OrderMapper orderMapper; @Autowired private LogMapper logMapper; @Autowired private OrderSender orderSender; public void createOrder(Order order) throws Exception &#123; // 数据库插入订单信息 orderMapper.insert(order); // 初始化日志信息 Log log = new Log(); Date orderTime = new Date(); log.setMessageId(order.getMessageId()); // 将订单转为 JSON 字符串在重新发送时使用 String jOrder = JSON.toJSONString(order); log.setMessage(jOrder); log.setStatus(\"0\"); log.setNextRetry(DateUtils.addMinutes(orderTime, Constant.ORDER_TIMEOUT)); log.setCreateTime(new Date()); log.setUpdateTime(new Date()); // 数据库插入日志信息 logMapper.insert(log); // order message sender orderSender.sendOrder(order); &#125;&#125; Sender 编写 RabbitMQ 的交付类。 12345678910111213141516171819202122232425262728293031323334@Componentpublic class OrderSender &#123; @Autowired private RabbitTemplate rabbitTemplate; @Autowired private LogMapper logMapper; // 回调函数: confirm 确认 final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.err.println(\"correlationData: \" + correlationData); String messageId = correlationData.getId(); if(ack)&#123; // 如果 confirm 返回成功，则对 Order 表中对应的订单状态进行更新 logMapper.changeBrokerMessageLogStatus(messageId, Constant.ORDER_SEND_SUCCESS, new Date()); &#125; else &#123; // 失败则进行具体的后续操作：重试或者补偿等手段 System.err.println(\"异常处理...\"); &#125; &#125; &#125;; // 发送消息方法调用: 构建自定义对象消息 public void sendOrder(Order order) throws Exception &#123; // 设置自定义回调方法 rabbitTemplate.setConfirmCallback(confirmCallback); // 消息唯一 ID CorrelationData correlationData = new CorrelationData(order.getMessageId()); rabbitTemplate.convertAndSend(\"order-exchange\", \"order.stable\", order, correlationData); &#125;&#125; Task 编写定时任务，定期对数据库中的任务状态进行读取，对重试满足条件的交付任务进行重新发送。 1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class RetryMessage &#123; @Autowired private OrderSender orderSender; @Autowired private LogMapper logMapper; // 3 秒后执行，之后每 10 秒执行一次 @Scheduled(initialDelay = 3000, fixedDelay = 10000) public void reSend()&#123; System.err.println(\"---------------定时任务开始---------------\"); // 取数据库中所有的状态为 0 并且已经超时的订单 List&lt;Log&gt; list = logMapper.query4StatusAndTimeoutMessage(); if(list.isEmpty())&#123; System.out.println(\"无交付失败订单\"); &#125; list.forEach(messageLog -&gt; &#123; if(messageLog.getTryCount() &gt;= Constant.ORDER_RETRY_MAX_TIMES)&#123; // 如果重试次数超过规定的最大次数则表示交付失败 logMapper.changeBrokerMessageLogStatus(messageLog.getMessageId(), Constant.ORDER_SEND_FAILURE, new Date()); System.out.println(\"有新的交付失败订单\"); &#125; else &#123; // 增加重试次数 logMapper.update4ReSend(messageLog.getMessageId(), new Date()); Order reSendOrder = JSON.parseObject(messageLog.getMessage(), Order.class); try &#123; orderSender.sendOrder(reSendOrder); System.out.println(\"重试订单交付\"); &#125; catch (Exception e) &#123; e.printStackTrace(); System.err.println(\"-----------异常处理-----------\"); &#125; &#125; &#125;); // forEach 结束 &#125;&#125; 主类上还需要加上几个注解。 123@MapperScan(\"com.wingo.stableprovider.mapper\")@EnableScheduling@SpringBootApplication 交付失败时的控制台打印信息。 序列化错误场景： consumer 和 provider 模块各自有一个可序列化的 entity.Order 类； 序列化与反序列化时，两个 Order 虽然代码相同，但底层序列化后的标记不同，虚拟机不会认为它们是同一个对象，出现反序列化错误。 1Caused by: org.springframework.messaging.converter.MessageConversionException: Cannot convert from 解决方案： 1）consumer 和 provider 引用同一个 entity.Order 类； 2）利用 jackson 工具进行转化，传递 Json 串。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://yoursite.com/tags/RabbitMQ/"}]},{"title":"Dubbo","slug":"后台技术/Dubbo 初步使用","date":"2020-04-17T01:50:34.000Z","updated":"2020-07-10T03:04:40.992Z","comments":true,"path":"2020/04/17/后台技术/Dubbo 初步使用/","link":"","permalink":"http://yoursite.com/2020/04/17/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Dubbo%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"Dubbo + Zookeeper 的简单介绍，以及整合使用。","text":"Dubbo + Zookeeper 的简单介绍，以及整合使用。 Dubbo 初步使用基础理论应用的发展与演变： 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架（ORM）是关键； 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的 Web 框架（MVC）是关键； 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架（RPC）是关键； Dubbo 是一款高性能、轻量级的开源 Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心（SOA）即 Service Oriented Architecture 是关键。 环境搭建Javayum search java-1.8 ！！！注意，需要下载两个 java-1.8.0-openjdk.x86_64 / java-1.8.0-openjdk-devel.x86_64 yum install ... whereis java查看 java 路径（/usr/lib/jvm/） 修改环境变量：vim /etc/profile 123export JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-...export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATHexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar javac环境变量是否配置正确 Zookeeper下载 zookeeper ：清华大学开源软件镜像站 wget [下载连接]下载压缩包 tar -zxvf zookeeper-x.x.x.tar.gz 在 /usr/local/ 下新建 zookeeper 目录进行解压 修改 zookeeper 配置文件名：zoo_sample.cfg 改为 zoo.cfg 配置环境变量：vi ~/.bash_profile 123export PATHexport ZOOKEEPER_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper&#x2F;zookeeper-3.4.11&#x2F;export PATH&#x3D;$ZOOKEEPER_HOME&#x2F;bin:$PATH 常用操作 123zkServer.sh start # 开启 zookeeperjps # 显示当前所有 java 进程 pidzkServer.sh stop # 停止 zookeeper dubbo-admin netstat -aon|findstr “port-num” 查看占用端口的程序的 PID Dubbo 官方项目下载 修改 dubbo-admin 项目的配置（！！！zookeeper 的地址修改为自己配置的地址） 在 pom.xml 目录下：cmd 输入命令mvn clean package对项目进行打包 打包成功后生成 target 文件夹，target 目录下有一个可执行 jar 包 java -jar [jara_name]运行项目，运行成功后根据配置的路径进行访问 dubbo-monitor生成的 target 文件中有一个压缩包，解压后的 assembly.bin 目录中运行 start.bat 脚本进行项目运行 项目架构先来看一张官网给的架构图： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 各节点间的调用关系说明： ​ 服务容器负责启动，加载，运行服务提供者。​ 服务提供者在启动时，向注册中心注册自己提供的服务。​ 服务消费者在启动时，向注册中心订阅自己所需的服务。​ 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。​ 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。​ 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 项目创建基于对官网给出的架构分析得知我们需要两个角色：服务的提供者；服务的消费者。 消费者在自身的项目中可以如同调用本地方法一般调用提供者的方法；那么消费者和提供者必须都受到同一套接口的规范，在两个项目中都编写接口显得有些繁琐，所以可以新建一个 Maven 项目专门用于定义接口。 最外层套一层壳，添加三个 module，其中外壳项目和提供接口的项目为 Maven 项目即可。 dubbo-api 此项目用于提供接口规范，不需要添加任何依赖。编写一个实体类和一个服务类接口即可。 1234567891011public class User implements Serializable &#123; private static final long serialVersionUID = 5433406871746033298L; private Integer userId; private String username; private String password; // ...&#125; 定义接口： 123456789public interface UserService &#123; /** * 获取用户列表的方法 * * @return 用户的信息列表 */ List&lt;User&gt; getUserList();&#125; 要让 dubbo-consumer 以及 dubbo-provider 获取此接口，需要给这两个项目添加 dubbo-api 项目的依赖。 123456&lt;!-- 添加自定义接口依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.wingo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; dubbo-provider服务提供者编写了接口的具体实现方法，并且要把实现的方法通过 Dubbo 暴露出去。 123456&lt;!-- 添加 Dubbo 依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; Spring 以及 Dubbo 都要管理这个类。 12345678910@com.alibaba.dubbo.config.annotation.Service@Servicepublic class UserServiceImpl implements UserService &#123; @Override public List&lt;User&gt; getUserList() &#123; User user1 = new User(1,\"wingo\",\"123456\"); User user2 = new User(2,\"admin\",\"654321\"); return Arrays.asList(user1, user2); &#125;&#125; 在主类上标注 @EnableDubbo(scanBasePackages = “com.wingo.dubboprovider.service”)，并添加配置。 12345678910# 此服务的名称dubbo.application.name=user-service-provider# 注册中心的协议及地址dubbo.registry.address=127.0.0.1:2181dubbo.registry.protocol=zookeeper# 暴露的协议及端口dubbo.protocol.name=dubbodubbo.protocol.port=20881## 注册中心获取监控中心的信息dubbo.monitor.protocol=registry 启动项目，来到 dubbo-admin 的页面查看服务。 在 dubbo-admin 多了一个我们刚刚暴露出来的服务，由名称可以得知其暴露的是一个接口。 点击查看其对应的 IP 地址以及域名，正是本机的 IP 以及自定义的接口 20881。 dubbo-monitor 中可以看到这个应用的名称正是配置文件中的 user-service-provider。 dubbo-consumer消费者向 dubbo 请求所需要的服务。 1234567891011@RestControllerpublic class UserController &#123; @Reference UserService userService; @GetMapping(\"/users\") public List&lt;User&gt; userList ()&#123; return userService.getUserList(); &#125;&#125; 12345server.port=8082dubbo.application.name=dubbo-user-consumerdubbo.registry.address=zookeeper://127.0.0.1:2181dubbo.monitor.protocol=registry 启动消费者。 可以看出，提供者与消费者确实是通过接口通讯的。 成功请求数据。 Dubbo 特性介绍一些 Dubbo 的使用特性。 高可用Zookeeper 宕机与 Dubbo 直连：注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯。 集群负载均衡Random LoadBalance： 随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance： 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance： 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance： 一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 服务降级当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 集群容错Failfast Cluster：快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录； Failsafe Cluster：失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作； Failback Cluster：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作； Forking Cluster：并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=”2” 来设置最大并行数； Broadcast Cluster：广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"http://yoursite.com/tags/Dubbo/"}]},{"title":"Redis 点赞","slug":"项目开发/Redis 点赞","date":"2020-04-09T06:29:14.000Z","updated":"2020-07-10T03:58:30.537Z","comments":true,"path":"2020/04/09/项目开发/Redis 点赞/","link":"","permalink":"http://yoursite.com/2020/04/09/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/Redis%20%E7%82%B9%E8%B5%9E/","excerpt":"Redis 实现点赞功能。","text":"Redis 实现点赞功能。 环境搭建12345678910111213141516171819202122232425262728293031323334353637383940&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- Redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- Jackson --&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring-json依赖 --&gt; Redis 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.wingo.springbootredis01.config;import com.fasterxml.jackson.annotation.JsonAutoDetect;import com.fasterxml.jackson.annotation.PropertyAccessor;import com.fasterxml.jackson.databind.ObjectMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;/** * @Author Wingo * @Date 2020-04-11 12:13 * @Description */@Configurationpublic class RedisConfig &#123; @Bean public StringRedisSerializer stringRedisSerializer() &#123; return new StringRedisSerializer(); &#125; @Bean public ObjectMapper objectMapper() &#123; ObjectMapper objectMapper = new ObjectMapper(); // 支持任意对象的 json 序列化和反序列化 objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); return objectMapper; &#125; @Bean public Jackson2JsonRedisSerializer jackson2JsonRedisSerializer() &#123; Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer&lt;&gt;(Object.class); jackson2JsonRedisSerializer.setObjectMapper(objectMapper()); return jackson2JsonRedisSerializer; &#125; @Bean public RedisTemplate redisTemplate(@Autowired RedisConnectionFactory redisConnectionFactory)&#123; RedisTemplate&lt;String,Object&gt; redisTemplate= new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 设置了默认的序列化器 redisTemplate.setDefaultSerializer(jackson2JsonRedisSerializer()); // 开启事务支持 redisTemplate.setEnableTransactionSupport(true); return redisTemplate; &#125;&#125; properties123456789101112131415161718# Redis数据库索引(默认为θ)spring.redis.database=0# Redis服务器地址spring.redis.host=192.168.31.100# Redis服务器连接端口spring.redis.port=6379# Redis服务器连接密码(默认为空)spring.redis.password=# 连接池最大连接数(使用负值表示没有限制)spring.redis.jedis.pool.max-active=10# 连接池最大阻塞等待时间(使用负值表示没有限制)spring.redis.jedis.pool.max-wait=-1# 连接池中的最大空闲连接spring.redis.jedis.pool.max-idle=5# 连接池中的最小空闲连接spring.redis.jedis.pool.min-idle=0# 连接超时时间(毫秒)spring.redis.timeout=1000 单元测试123456789101112131415@RunWith(SpringRunner.class)@Slf4j@SpringBootTestclass SpringBootRedis01ApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; @Test void simpleAdd() &#123; redisTemplate.opsForValue().set(\"test01\",\"这是一个测试01\"); log.info(redisTemplate.opsForValue().get(\"test01\").toString()); &#125;&#125; Redis 客户端存储正常，控制台正常输出，项目环境搭建完成。 点赞类型枚举类 LIKED ARTICLE 文章点赞；LIKED ⅥDEO 视频点赞； 代码实现 12345678910111213141516171819public enum MyType &#123; /** LIKED_ARTICLE */ LIKED_ARTICLE (\"文章点赞\"), /** LIKED_VIDEO */ LIKED_VIDEO (\"视频点赞\"), /** LIKED_VIDEO */ UNKONW_TYPE (\"未知类型\"); private String myType; /** * 构造器类 * @param myType */ MyType(String myType) &#123; this.myType = myType; &#125;&#125; 点赞工具类1234567891011121314151617181920public class LikedUtil &#123; /** 生成 Key */ public static String getKey(MyType myType, String subjectId) &#123; return myType + \":\" + subjectId; &#125; public static String getReportKey(MyType myType) &#123; MyType type; switch (myType) &#123; case LIKED_ARTICLE: type = MyType.LIKED_ARTICLE; break; case LIKED_VIDEO: type = MyType.LIKED_VIDEO; break; default: type = MyType.UNKONW_TYPE; &#125; return type + \"\"; &#125;&#125; 单元测试123456@Testvoid keyTest()&#123; MyType type = MyType.LIKED_ARTICLE; String result = LikedUtil.getKey(type, “1”); log.info(result);&#125; 自定义的 Key 格式：类型 + 类型实体的 id 简单实现该方案 Redis 数据库只存储点赞主体和被点赞主体的ID信息，数据类型采用 Hash 类型。 key ：LIKED_TYPE + subject_id，即使用文章的 id 作为 key 。field ： post_id 点赞人的id ； value 固定值 1 ：表示一个点赞主体对被点赞主体点了一次赞。 自定义接口1234567891011121314151617181920212223242526/** * @Author Wingo * @Date 2020-04-11 14:41 * @Description */public interface LikedSimpleService &#123; /** * 点赞 / 取消点赞功能 * * @param myType 点赞的类型 * @param subjectId 被点赞的主体的 id * @param postId 点赞主体的 id */ void liked(MyType myType, String subjectId, String postId); /** * 得到主体的总点赞数 * * @param myType 点赞的类型 * @param subjectId 被点赞的主体的 id * @return 总点赞数 */ Long count(MyType myType, String subjectId);&#125; 接口实现12345678910111213141516171819202122232425262728293031/** * @Author Wingo * @Date 2020-04-11 15:09 * @Description */@Servicepublic class LikedSimpleServiceImpl implements LikedSimpleService &#123; @Autowired private StringRedisTemplate redisTemplate; @Override public void liked(MyType myType, String subjectId, String postId) &#123; String key = LikedUtil.getKey(myType, subjectId); // 判断用户是否已经点赞，如果点过赞，取消点赞，如果没有点过点赞 Boolean liked = redisTemplate.opsForHash().hasKey(key, postId); if (Boolean.TRUE.equals(liked)) &#123; // 取消点赞 redisTemplate.opsForHash().delete(key, postId); &#125; else &#123; // 点赞 redisTemplate.opsForHash().put(key, postId, \"1\"); &#125; &#125; @Override public Long count(MyType myType, String subjectId) &#123; String key = LikedUtil.getKey(myType, subjectId); return redisTemplate.opsForHash().size(key); &#125;&#125; 单元测试12345678910111213141516171819202122232425262728293031@RunWith(SpringRunner.class)@Slf4j@SpringBootTestclass SpringBootRedis01ApplicationTests &#123; @Autowired private LikedSimpleService likedSimpleService; @Autowired private RedisTemplate redisTemplate; @Test void keyTest()&#123; MyType myType = MyType.LIKED_ARTICLE; for (int j = 1; j &lt;= 100; j++) &#123; for (int i = 1; i &lt;= j; i++) &#123; likedSimpleService.liked(myType, j+\"\", i+\"\"); &#125; &#125; &#125; @Test void countTest()&#123; MyType myType = MyType.LIKED_ARTICLE; int num = 0; while(num &lt;= 10)&#123; int subjectId = new Random().nextInt(100) + 1; long count = likedSimpleService.count(myType, Integer.toString(subjectId)); log.info(\"ID 为\" + subjectId + \"的文章点赞数为\" + count); num++; &#125;&#125; 初始化 100 篇文章的点赞信息，并且 id = num 的文章其点赞数为 num 复杂实现 前面的简单实现只缓存了 ID 的数据，而实际的应用场景中常常还需要更多的信息，比如文章标题、文章内容简介、用户昵称、用户头像、点赞排行等信息。根据需求继续完善项目。 方案设计 点赞数量统计：Hash key ：LIKED_TYPE + subject_id。 field ： post_id； value { json } 点赞的人的信息。 点赞排名统计： zSet key ：LIKED_TYPE_TOP。 field ： value { json } 被点赞主体的内容； score 记录被点赞主体的总赞数。 类型添加123456789101112131415161718192021222324252627public enum MyType &#123; /** LIKED_ARTICLE */ LIKED_ARTICLE (\"文章点赞\"), /** LIKED_VIDEO */ LIKED_VIDEO (\"视频点赞\"), /** LIKED_ARTICLE_TOP */ LIKED_ARTICLE_TOP (\"文章点赞排行\"), /** LIKED_VIDEO_TOP */ LIKED_VIDEO_TOP (\"视频点赞排行\"), /** LIKED_VIDEO */ UNKONW_TYPE (\"未知类型\"); private String myType; /** * 构造器类 * @param myType */ MyType(String myType) &#123; this.myType = myType; &#125; public String getMyType()&#123; return this.myType; &#125;&#125; 1234567891011121314151617181920212223242526public class LikedUtil &#123; LikedUtil()&#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 生成 Key */ public static String getKey(MyType myType, String subjectId) &#123; return myType + \":\" + subjectId; &#125; /** 生成排行榜 Key */ public static String getTopKey(MyType myType) &#123; MyType type; switch (myType) &#123; case LIKED_ARTICLE: type = MyType.LIKED_ARTICLE_TOP; break; case LIKED_VIDEO: type = MyType.LIKED_VIDEO_TOP; break; default: type = MyType.UNKONW_TYPE; &#125; return type.toString(); &#125;&#125; 12345678@Datapublic class Article &#123; private Integer articleId; private String title; private String summary; private String img;&#125; 1234567@Datapublic class User &#123; private Integer userId; private String username; private String avatar;&#125; 自定义接口1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @Author Wingo * @Date 2020-04-12 12:45 * @Description */public interface LikedService &#123; /** * 点赞/取消点赞 * * @param type 类型 * @param subjectId 被点赞主体 ID * @param postId 点赞主体 ID * @param postUser 点赞主体的概要信息 * @param subject 被点赞主体概要信息 */ void liked(MyType type, String subjectId, String postId, Object subject, Object postUser); /** * 获取主体点赞数量 * * @param type 类型 * @param subjectId 被点赞主体 ID * @return 点赞数量 */ Long count(MyType type, String subjectId); /** * 获取点赞主体的详情列表 * * @param type 类型 * @param subjectId 被点赞主体 ID * @return 点赞主体的详情列表 */ List&lt;Object&gt; getPostUserSet(MyType type, String subjectId); /** * 排行查询 * * @param type 业务类型 * @param top 前 top 名 * @return 排行列表 */ Set&lt;Object&gt; getSubjectTopN(MyType type, Long top);&#125; 接口实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.wingo.springbootredis01.service.impl;import com.wingo.springbootredis01.constant.Common;import com.wingo.springbootredis01.constant.MyType;import com.wingo.springbootredis01.service.LikedService;import com.wingo.springbootredis01.util.LikedUtil;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Service;import java.util.List;import java.util.Set;/** * @Author Wingo * @Date 2020-04-12 12:50 * @Description */@Slf4j@Servicepublic class LikedServiceImpl implements LikedService &#123; @Autowired private RedisTemplate&lt;String,Object&gt; redisTemplate; @Override public void liked(MyType type, String subjectId, String postId, Object subject, Object postUser) &#123; String key = LikedUtil.getKey(type, subjectId); Boolean liked = redisTemplate.opsForHash().hasKey(key, postId); if (Boolean.TRUE.equals(liked)) &#123; // 取消赞 redisTemplate.opsForHash().delete(key, postId); &#125; else &#123; // 点赞 redisTemplate.opsForHash().put(key, postId, postUser); &#125; String reportKey = LikedUtil.getTopKey(type); if (Boolean.TRUE.equals(liked)) &#123; redisTemplate.opsForZSet().incrementScore(reportKey, subject, Common.DECLINE); &#125; else &#123; Double score = redisTemplate.opsForZSet().score(reportKey, subject); if (score == null) &#123; redisTemplate.opsForZSet().add(reportKey, subject, Common.INCREASE); &#125; else &#123; redisTemplate.opsForZSet().incrementScore(reportKey, subject, Common.INCREASE); &#125; &#125; &#125; @Override public Long count(MyType type, String subjectId) &#123; String key = LikedUtil.getKey(type, subjectId); return redisTemplate.opsForHash().size(key); &#125; @Override public List&lt;Object&gt; getPostUserSet(MyType type, String subjectId) &#123; String key = LikedUtil.getKey(type, subjectId); return redisTemplate.opsForHash().values(key); &#125; @Override public Set&lt;Object&gt; getSubjectTopN(MyType type, Long top) &#123; String key = LikedUtil.getTopKey(type); return redisTemplate.opsForZSet().reverseRange(key, 0, top); &#125;&#125; 单元测试12345678910111213141516171819202122232425@Testvoid keyTest()&#123; MyType myType = MyType.LIKED_ARTICLE; for (int j = 1; j &lt;= 100; j++) &#123; Article article= new Article(); article.setArticleId(j); article.setTitle(\"文章标题\"+j); article.setImg(\"文章图片\"+j); article.setSummary(\"文章摘要\"+j); for (int i = 1; i &lt;= j; i++) &#123; User user = new User(); user.setUserId (i); user.setUsername(\"user\"+ i); user.setAvatar(\"avatar\"+ i); likedService.liked(myType, j+\"\", i+\"\", article, user); &#125; &#125;&#125;@Testvoid countTest()&#123; MyType myType = MyType.LIKED_ARTICLE; log.info(likedService.getSubjectTopN(myType, (long) 10).toString());&#125; 用户信息被写入缓存。 取得排名前十的文章信息。","categories":[{"name":"项目开发","slug":"项目开发","permalink":"http://yoursite.com/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"点赞","slug":"点赞","permalink":"http://yoursite.com/tags/%E7%82%B9%E8%B5%9E/"}]},{"title":"H2 配置","slug":"环境搭建/H2 配置","date":"2020-04-06T09:33:57.000Z","updated":"2020-07-10T03:01:03.506Z","comments":true,"path":"2020/04/06/环境搭建/H2 配置/","link":"","permalink":"http://yoursite.com/2020/04/06/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/H2%20%E9%85%8D%E7%BD%AE/","excerpt":"Spring Boot 整合 H2 数据库.","text":"Spring Boot 整合 H2 数据库. 添加依赖12345678910111213&lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 常用配置12345678910111213141516171819202122# 若使用内存数据库 spring.datasource.url=jdbc:h2:mem:test# ./ 表示根目录下生成数据库文件 ~/ 表示用户根目录 spring.datasource.url=jdbc:h2:./db/testspring.datasource.driverClassName=org.h2.Driverspring.datasource.username=saspring.datasource.password=# 使用 H2 数据平台spring.datasource.platform=h2# 内存模式的数据库信息读取# spring.datasource.schema=classpath:db/schema.sql# spring.datasource.data=classpath:db/data.sqlspring.h2.console.settings.web-allow-others=true# 浏览器访问路径 &#123;path&#125;/h2spring.h2.console.path=/h2# 程序启动时启动 H2spring.h2.console.enabled=truespring.jpa.database-platform=org.hibernate.dialect.H2Dialectspring.jpa.generate-ddl=truespring.jpa.show-sql=truespring.jpa.hibernate.ddl-auto=update","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"H2","slug":"H2","permalink":"http://yoursite.com/tags/H2/"}]},{"title":"Mybatis-Plus","slug":"环境搭建/Mybatis-Plus","date":"2020-04-06T03:44:35.000Z","updated":"2020-07-10T04:16:55.688Z","comments":true,"path":"2020/04/06/环境搭建/Mybatis-Plus/","link":"","permalink":"http://yoursite.com/2020/04/06/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/Mybatis-Plus/","excerpt":"Mybatis-Plus 整合 H2 数据库小案例。","text":"Mybatis-Plus 整合 H2 数据库小案例。 Mybatis-Plus 官方文档 SQL 语句 12345678910111213141516-- 建表语句CREATE TABLE user( id BIGINT(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID', name VARCHAR(30) NULL DEFAULT NULL COMMENT '姓名', age INT(11) NULL DEFAULT NULL COMMENT '年龄', email VARCHAR(50) NULL DEFAULT NULL COMMENT '邮箱', PRIMARY KEY (id));-- 插入数据INSERT INTO user (name, age, email) VALUES('Jone', 18, 'test1@baomidou.com'),('Jack', 20, 'test2@baomidou.com'),('Tom', 28, 'test3@baomidou.com'),('Sandy', 21, 'test4@baomidou.com'),('Billie', 24, 'test5@baomidou.com'); 环境搭建热部署 ctrl + shift + alt + /勾选 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;!-- mybatis-plus --&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.3.1.tmp&lt;/version&gt; &lt;/dependency&gt; &lt;!-- H2 数据库 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 用于返回 JSON --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;scope&gt;true&lt;/scope&gt; &lt;/dependency&gt; 1234567891011121314151617181920# 若使用内存数据库 spring.datasource.url=jdbc:h2:mem:test# ./ 表示根目录下生成数据库文件 ~/ 表示用户根目录spring.datasource.url=jdbc:h2:./db/testspring.datasource.driverClassName=org.h2.Driverspring.datasource.username=saspring.datasource.password=# 使用 H2 数据平台spring.datasource.platform=h2# 内存模式的数据库信息读取# spring.datasource.schema=classpath:db/schema.sql# spring.datasource.data=classpath:db/data.sqlspring.h2.console.settings.web-allow-others=true# 浏览器访问路径 &#123;path&#125;/h2spring.h2.console.path=/h2# 程序启动时启动 H2spring.h2.console.enabled=true# 打印 SQL 语句mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl 项目搭建VO12345678910111213141516171819202122232425262728293031323334353637383940public class ResponseVO extends HashMap&lt;String, Object&gt; &#123; private static final Integer SUCCESS_STATUS = 200; private static final Integer ERROR_STATUS = -1; private static final String SUCCESS_MSG = \"一切正常\"; private static final String ERROR_MSG = \"出现错误\"; private static final long serialVersionUID = 1L; public ResponseVO success(String msg) &#123; put(\"msg\", SUCCESS_MSG); put(\"status\", SUCCESS_STATUS); return this; &#125; public ResponseVO error(String msg) &#123; put(\"msg\", ERROR_MSG); put(\"status\", ERROR_STATUS); return this; &#125; public ResponseVO setData(String key, Object obj) &#123; @SuppressWarnings(\"unchecked\") HashMap&lt;String, Object&gt; data = (HashMap&lt;String, Object&gt;) get(\"data\"); if (data == null) &#123; data = new HashMap&lt;&gt;(16); put(\"data\", data); &#125; data.put(key, obj); return this; &#125; /** * 返回JSON字符串 */ @Override public String toString() &#123; return JSONObject.toJSONString(this); // com.alibaba.fastjson &#125;&#125; Entity12345678910@Datapublic class User &#123; @TableId(type=IdType.AUTO) // 自增 private Long id; private String name; private Integer age; private String email;&#125; Mapper1234@Component // 可不加：防止编译器报错public interface UserMapper extends BaseMapper&lt;User&gt; &#123;&#125; Controller1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Controllerpublic class UserController &#123; @Autowired private UserMapper userMapper; @GetMapping(value = \"/selectList\") @ResponseBody public ResponseVO selectList()&#123; ResponseVO responseVO = new ResponseVO(); List&lt;User&gt; userList = userMapper.selectList(new QueryWrapper&lt;User&gt;().orderByDesc(\"id\")); responseVO.setData(\"userList\",userList); return responseVO; &#125; @GetMapping(value = \"/insert/&#123;name&#125;/&#123;age&#125;\") public String insert(@PathVariable(\"name\") String name, @PathVariable(\"age\") Integer age)&#123; User user = new User(); user.setName(name); user.setAge(age); user.setEmail(name + age +\"@wingo-email.com\"); userMapper.insert(user); return \"redirect:/selectList\"; &#125; @GetMapping(value = \"/delete/&#123;id&#125;\") public String delete(@PathVariable(\"id\") Integer id)&#123; userMapper.deleteById(id); return \"redirect:/selectList\"; &#125; @GetMapping(value = \"/age-eq/&#123;age&#125;\") public ResponseVO ageEqual(@PathVariable(\"age\") Integer age)&#123; ResponseVO responseVO = new ResponseVO(); QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.eq(\"age\",age); List&lt;User&gt; userList = userMapper.selectList(queryWrapper); responseVO.setData(\"userList\",userList); return responseVO; &#125; @GetMapping(value = \"/age/&#123;gt&#125;/&#123;lt&#125;\") @ResponseBody public ResponseVO age(@PathVariable(\"gt\") Integer gt, @PathVariable(\"lt\") Integer lt)&#123; ResponseVO responseVO = new ResponseVO(); QueryWrapper&lt;User&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.gt(\"age\",gt); queryWrapper.lt(\"age\",lt); List&lt;User&gt; userList = userMapper.selectList(queryWrapper); responseVO.setData(\"userList\",userList); return responseVO; &#125;&#125; 接口测试 分页配置1234567891011121314151617@Configurationpublic class MyBatisConfig &#123; /** * MyBatis-puls 分页插件配置 * @return */ @Bean public PaginationInterceptor paginationInterceptor() &#123; PaginationInterceptor paginationInterceptor = new PaginationInterceptor(); // 设置请求的页面大于最大页后操作， true 调回到首页，false 继续请求 默认 false paginationInterceptor.setOverflow(false); // 设置最大单页限制数量，默认 500 条，-1 不受限制 paginationInterceptor.setLimit(500); return paginationInterceptor; &#125;&#125; 代码生成器1234567891011&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 官方文档查看支持的其它引擎 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.29&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/*** 代码生成器*/public class Generator &#123; /** * 读取控制台内容 */ public static String scanner(String tip) &#123; Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(\"请输入\" + tip + \"：\"); System.out.println(help.toString()); if (scanner.hasNext()) &#123; String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) &#123; return ipt; &#125; &#125; throw new MybatisPlusException(\"请输入正确的\" + tip + \"！\"); &#125; public static void main(String[] args) &#123; // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\"user.dir\"); gc.setOutputDir(projectPath + \"/src/main/java\"); gc.setAuthor(\"wingo\"); gc.setOpen(false); // gc.setSwagger2(true); // 实体属性 Swagger2 注解 gc.setBaseResultMap(true); gc.setBaseColumnList(true); mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\"jdbc:mysql://localhost:3306/[db-name]?useSSL=false\"); dsc.setDriverName(\"com.mysql.jdbc.Driver\"); dsc.setUsername(\"root\"); dsc.setPassword(\"123456\"); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(\"模块名\")); pc.setParent(\"com.wingo.mybatisplus\"); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() &#123; @Override public void initMap() &#123; // to do nothing &#125; &#125;; List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); // 模板引擎是 freemarker focList.add(new FileOutConfig(\"/templates/mapper.xml.ftl\") &#123; @Override public String outputFile(TableInfo tableInfo) &#123; // 自定义输入文件名称 return projectPath + \"/src/main/resources/mapper/\" + pc.getModuleName() + \"/\" + tableInfo.getEntityName() + \"Mapper\" + StringPool.DOT_XML; &#125; &#125;); cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); mpg.setTemplate(new TemplateConfig().setXml(null)); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); strategy.setEntityLombokModel(true); // 使用 Lombok 插件 strategy.setInclude(scanner(\"表名\")); // 自定义继承类，可直接使用官方的顶级实现 // strategy.setSuperServiceClass(\"com.wingo.common.service.IService\"); // strategy.setSuperServiceImplClass(\"com.wingo.common.service.impl.ServiceImpl\"); // Controller配置 strategy.setSkipView(false); strategy.setControllerMappingHyphenStyle(true); strategy.setRestControllerStyle(false); // @Controller @RestController strategy.setTablePrefix(pc.getModuleName() + \"_\"); mpg.setStrategy(strategy); // 选择 freemarker 引擎需要指定如下加，注意 pom 依赖必须有！ mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); &#125;&#125; Service 顶级实现","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"}]},{"title":"Spring Boot 任务","slug":"后台技术/Spring Boot/Spring Boot 任务","date":"2020-04-01T02:50:45.000Z","updated":"2020-07-10T04:13:25.645Z","comments":true,"path":"2020/04/01/后台技术/Spring Boot/Spring Boot 任务/","link":"","permalink":"http://yoursite.com/2020/04/01/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E4%BB%BB%E5%8A%A1/","excerpt":"Spring Boot 中的异步任务，定时任务，邮件任务的使用。","text":"Spring Boot 中的异步任务，定时任务，邮件任务的使用。 异步任务 在 Java 应用中，绝大多数情况下都是通过同步的方式来实现交互处理的。但是在处理与第三方系统交互的时候，容易造成响应迟缓的情况，之前大部分都是使用多线程来完成此类任务。其实，在Spring 3.x 之后，就已经内置了@Async、@EnableAysnc来完美解决这个问题。 在 Spring 中运用 Async 注解需要注意几点： 方法名必须是 public 进行修饰的，且不能是 static 方法； 不能与调用的方法在同一个类中； 需要把该方法注入到 Spring 容器中，就是在一个类中添加异步方法，并在此类上使用@Component之类的注解。 测试实例定义 Task 类，创建三个处理函数分别模拟三个执行任务的操作，操作消耗时间随机取（10 秒内）。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class Task &#123; public static Random random =new Random(); @Async // 注释为异步任务， public Future&lt;String&gt; doTaskOne() throws Exception &#123; System.out.println(\"Doing Task One\"); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(\"Task One Finished：Spend \" + (end - start) + \" Millisecond\"); // 若不用 Future 进行回调，无法确定任务已经完成 return new AsyncResult&lt;&gt;(\"Task One Finished\"); // 返回异步调用结果 &#125; @Async public Future&lt;String&gt; doTaskTwo() throws Exception &#123; System.out.println(\"Doing Task Two\"); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(\"Task Two Finished：Spend \" + (end - start) + \" Millisecond\"); return new AsyncResult&lt;&gt;(\"Task Two Finished\"); &#125; @Async public Future&lt;String&gt; doTaskThree() throws Exception &#123; System.out.println(\"Doing Task Three\"); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(\"Task Three Finished：Spend \" + (end - start) + \" Millisecond\"); return new AsyncResult&lt;&gt;(\"Task Three Finished\"); &#125;&#125; 12345678@SpringBootApplication@EnableAsync // 异步任务生效public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 123456789101112131415161718192021@Testpublic void test() throws Exception &#123; long start = System.currentTimeMillis(); Future&lt;String&gt; taskOne = task.doTaskOne(); Future&lt;String&gt; taskTwo = task.doTaskTwo(); Future&lt;String&gt; taskThree = task.doTaskThree(); while(true) &#123; if(taskOne.isDone() &amp;&amp; taskTwo.isDone() &amp;&amp; taskThree.isDone()) &#123; // 三个任务都调用完成，退出循环等待 break; &#125; Thread.sleep(1000); &#125; long end = System.currentTimeMillis(); System.out.println(\"All Finished，总耗时：\" + (end - start) + \"毫秒\");&#125; 定时任务 项目开发中经常需要执行一些定时任务，比如需要在每天凌晨时候，分析一次前一天的日志信息。Spring 为我们提供了异步执行任务调度的方式，提供 TaskExecutor 、TaskScheduler 接口。 注解：@EnableScheduling、@Scheduled corn 表达式 *******从左到右分别代表：秒 分 时 日 月 星期 年份，这里 * 指所有可能的值。 在线 Cron 表达式生成器 测试1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 1234567891011121314151617@Servicepublic class TaskService &#123; /** * second（秒）, minute（分）, hour（时）, day of month（日）, month（月）, day of week（周几）. * 例子： * 0 0/5 14,18 * * ? 每天14点整，和18点整，每隔5分钟执行一次 * 0 15 10 ? * 1-6 每个月的周一至周六10:15分执行一次 * 0 0 2 ? * 6L 每个月的最后一个周六凌晨2点执行一次 * 0 0 2 LW * ? 每个月的最后一个工作日凌晨2点执行一次 * 0 0 2-4 ? * 1#1 每个月的第一个周一凌晨2点到4点期间，每个整点都执行一次； */ @Scheduled(cron = \"0,1,2,3,4 * * * * MON-SAT\") public void runTask()&#123; System.out.println(new Date()+\"Doing Schedule Task\"); &#125;&#125; 邮件任务 最早期的时候使用 JavaMail 相关 API 来开发，需要自己去封装消息体，代码量比较庞大；后来 Spring 推出了 JavaMailSender 简化了邮件发送过程，JavaMailSender 提供了强大的邮件发送功能，可支持各种类型的邮件发送。现在 Spring Boot 在 JavaMailSender 的基础上又进行了封装，就有了现在的 spring-boot-starter-mail，让邮件发送流程更加简洁和完善。 测试1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 123456spring.mail.username=邮箱账号# 在邮件站点的个人中心中获取spring.mail.password=xxxxxspring.mail.host=smtp.163.com // 邮件协议对应的服务器spring.mail.protocol=smtp // 邮件协议spring.mail.properties.mail.smtp.ssl.enable=true 12345678910111213141516@AutowiredJavaMailSenderImpl mailSender;@Testpublic void testSimpleMessage()&#123; SimpleMailMessage message = new SimpleMailMessage(); message.setSubject(\"第一封测试邮件\"); message.setText(\"邮件测试...\"); message.setFrom(\"你的邮箱账号\"); message.setTo(\"发送对象的邮箱账号\"); mailSender.send(message);&#125; 123456789101112131415161718@Testpublic void testMimeMessage() throws Exception&#123; // 创建复杂消息 MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); helper.setSubject(\"第二封邮件\"); helper.setText(\"&lt;b style='color:red'&gt;邮件测试....&lt;/b&gt;\",true); helper.setFrom(\"你的邮箱账号\"); helper.setTo(\"发送对象的邮箱账号\"); // 发送邮件 helper.addAttachment(\"1.jpg\",new File(\"C:\\\\Users\\\\12746\\\\Pictures\\\\22\\\\1.jpg\")); helper.addAttachment(\"2.jpg\",new File(\"C:\\\\Users\\\\12746\\\\Pictures\\\\22\\\\2.jpg\")); mailSender.send(mimeMessage);&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Aysnc","slug":"Aysnc","permalink":"http://yoursite.com/tags/Aysnc/"},{"name":"Scheduled","slug":"Scheduled","permalink":"http://yoursite.com/tags/Scheduled/"},{"name":"Email","slug":"Email","permalink":"http://yoursite.com/tags/Email/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"}]},{"title":"Spring Boot 安全","slug":"后台技术/Spring Boot/Spring Boot 安全","date":"2020-04-01T02:50:45.000Z","updated":"2020-07-10T04:13:18.238Z","comments":true,"path":"2020/04/01/后台技术/Spring Boot/Spring Boot 安全/","link":"","permalink":"http://yoursite.com/2020/04/01/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E5%AE%89%E5%85%A8/","excerpt":"Shiro 轻量级安全框架的介绍及使用；Sping 安全框架 Spring Security 的介绍及使用。","text":"Shiro 轻量级安全框架的介绍及使用；Sping 安全框架 Spring Security 的介绍及使用。 Spring Security简介 Spring Security 是针对 Spring 项目的安全框架，也是 Spring Boot 底层安全模块默认的技术选型。可以实现强大的 web 安全控制。对于安全控制，我们仅需引入 spring-boot-starter-security 模块，进行少量的配置，即可实现强大的安全管理。 WebSecurityConfigurerAdapter：自定义 Security 策略； AuthenticationManagerBuilder：自定义认证策略； @EnableWebSecurity：开启 WebSecurity。 应用程序的两个主要区域是“认证”和“授权”（或者访问控制）。这两个主要区域是 Spring Security 的两个目标。 “认证”（Authentication），是建立一个经过声明的主体的过程，即你是谁； “授权”（Authorization）指确定一个主体是否允许在你的应用程序执行一个动作的过程，即你能干什么。 入门实例引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 建立一个 Web 层的请求接口： 12345678@RestController@RequestMapping(\"/user\")public class UserController &#123; @GetMapping public String getUsers() &#123; return \"Hello Spring Security\"; &#125;&#125; 自定义用户认证逻辑 系统一般都有自定义的用户体系，Spring Security 提供了接口可以自定义认证逻辑以及登录界面。 1234567// 密码加密接口public interface PasswordEncoder &#123; // 对密码进行加密 String encode(CharSequence var1); // 对密码进行判断匹配 boolean matches(CharSequence var1, String var2);&#125; 12345// 实现 Spring Security 中的 PasswordEncoder 接，这里用提供的默认实现@Beanpublic PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; 配置用户认证逻辑： UserDetails 就是封装了用户信息的对象，返回 UserDetails 的实现类 User 的时候，可以通过 User 的构造方法，设置对应的参数。 12345678910111213141516171819@Componentpublic class MyUserDetailsService implements UserDetailsService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; logger.info(\"用户的用户名: &#123;&#125;\", username); // TODO 根据用户名，查找到对应的密码，与权限 // 封装用户信息，并返回。参数分别是：用户名，密码，用户权限 String password = passwordEncoder.encode(\"123456\"); logger.info(\"password: &#123;&#125;\", password); // 每次打印的密码都不一样，因为经过了加密 // 参数分别是：用户名，密码，用户权限 User user = new User(username, password, AuthorityUtils.commaSeparatedStringToAuthorityList(\"admin\")); return user; &#125;&#125; 自定义登录页面： 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;登录页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;自定义登录页面&lt;/h2&gt; &lt;form action=\"/user/login\" method=\"post\"&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt;&lt;input type=\"text\" name=\"username\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" name=\"password\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=\"2\"&gt;&lt;button type=\"submit\"&gt;登录&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 配置模块12345678910111213141516@Configurationpublic class BrowerSecurityConfig extends WebSecurityConfigurerAdapter &#123; protected void configure(HttpSecurity http) throws Exception &#123; http.formLogin() // 定义当需要用户登录时候，转到的登录页面。 .loginPage(\"/login.html\") // 设置登录页面 .loginProcessingUrl(\"/user/login\") // 自定义的登录接口 .and() .authorizeRequests() // 定义哪些 URL 需要被保护，哪些不需要被保护 .antMatchers(\"/login.html\").permitAll() // 设置所有人都可以访问登录页面 .anyRequest() // 任何请求，登录后可以访问 .authenticated() .and() .csrf().disable(); // 关闭 csrf 防护 &#125;&#125; Shiro Apache Shiro是一个功能强大且易于使用的 Java安 全框架，它为开发人员提供了一种直观，全面的身份验证，授权，加密和会话管理解决方案。 核心 API Subject： 用户主体（把操作交给SecurityManager）；SecurityManager：安全管理器（关联Realm）；Realm：Shiro 连接数据的桥梁。 入门实例SQL 建表语句： 1234567CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varbinary(64) DEFAULT NULL, `password` varchar(64) DEFAULT NULL, `perms` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 导入依赖： 12345678910111213141516&lt;!-- shiro --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 自定义 Realm 类123456789101112131415161718192021222324252627282930313233343536373839@Slf4jpublic class UserRealm extends AuthorizingRealm &#123; @Autowired private UserRepository userRepository; @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection arg0) &#123; log.info(\"执行授权逻辑\"); Subject subject = SecurityUtils.getSubject(); User user = userRepository.findUserByUsername((String) subject.getSession().getAttribute(\"username\")); // 给用户进行授权 SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addStringPermission(user.getPerms()); return info; &#125; @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken arg0) throws AuthenticationException &#123; log.info(\"执行认证逻辑\"); Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); UsernamePasswordToken token = (UsernamePasswordToken)arg0; User user = userRepository.findUserByUsername(token.getUsername()); session.setAttribute(\"username\", token.getUsername()); if(user==null)&#123; // shiro 底层会抛出UnKnowAccountException return null; &#125; // 判断密码 return new SimpleAuthenticationInfo(user,user.getPassword(),getName()); &#125;&#125; Shiro 配置类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Slf4j@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(\"shiroCacheManager\") private ShiroCacheManager shiroCacheManager; @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Autowired DefaultWebSecurityManager securityManager)&#123; ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 设置安全管理器 shiroFilterFactoryBean.setSecurityManager(securityManager); // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // 设置过滤器链 Map&lt;String,String&gt; filterMap = new LinkedHashMap&lt;&gt;(16); filterMap.put(\"/index\", \"anon\"); filterMap.put(\"/favicon.ico\", \"anon\"); // 添加访问权限 filterMap.put(\"/toAdd\", \"perms[user:add]\"); filterMap.put(\"/toUpdate\", \"perms[user:update]\"); // 注意：要放在允许访问的页面后面，否则允许访问无效 filterMap.put(\"/**\", \"authc\"); // 如果没有认证通过 跳转到的 url 地址 shiroFilterFactoryBean.setLoginUrl(\"/login\"); shiroFilterFactoryBean.setSuccessUrl(\"/index\"); // 未授权跳转页面 shiroFilterFactoryBean.setUnauthorizedUrl(\"/error\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterMap); return shiroFilterFactoryBean; &#125; // 安全管理器 @Bean public DefaultWebSecurityManager getDefaultWebSecurityManager(@Autowired UserRealm userRealm)&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 关联 realm securityManager.setRealm(userRealm); return securityManager; &#125; @Bean public UserRealm getRealm()&#123; return new UserRealm(); &#125; /** 配置 ShiroDialect，用于 Thymeleaf 和 Shiro 标签配合使用 */ // 添加 xmlns:shiro=\"http://www.pollix.at/thymeleaf/shiro\" 标签校验规范 @Bean public ShiroDialect getShiroDialect()&#123; return new ShiroDialect(); &#125;&#125; 控制器类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Controllerpublic class UserController &#123; @GetMapping(\"/login\") public String login()&#123; return \"login\"; &#125; @GetMapping(\"/index\") public String hello(Model model)&#123; Subject subject = SecurityUtils.getSubject(); model.addAttribute(\"username\", subject.getSession().getAttribute(\"username\")); return \"index\"; &#125; @GetMapping(\"/toAdd\") public String add()&#123; return \"/user/add\"; &#125; @GetMapping(\"/toUpdate\") public String update()&#123; return \"/user/update\"; &#125; @GetMapping(\"/toExit\") public String exit()&#123; Subject subject = SecurityUtils.getSubject(); subject.logout(); return \"redirect:/login\"; &#125; @PostMapping(\"/login\") public String login(String username, String password, Model model)&#123; Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(username,password); try &#123; Session session = subject.getSession(); session.setAttribute(\"username\", username); subject.login(token); return \"redirect:/index\"; &#125; catch (UnknownAccountException e) &#123; model.addAttribute(\"msg\", \"用户名不存在\"); return \"login\"; &#125;catch (IncorrectCredentialsException e) &#123; model.addAttribute(\"msg\", \"密码错误\"); return \"login\"; &#125; &#125;&#125; 高级自定义缓存管理器：Spring 接管 Shiro 的缓存管理。 实现 Shiro 提供的 Cache 和 CacheManager 接口 👉 实现 Cache 缓存接口底层使用 SpringCache 实现。 ShiroCache12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*** 自定义 Shiro 缓存，实现 ShiroCache 接口** @param &lt;K&gt;* @param &lt;V&gt;*/public class ShiroCache&lt;K,V&gt; implements org.apache.shiro.cache.Cache&lt;K,V&gt; &#123; // Spring 的缓存管理器 private CacheManager springCacheManager; // Spring 的缓存对象 private Cache springCache; // 构造器 public ShiroCache(org.springframework.cache.CacheManager springCacheManager, String cacheName) &#123; this.springCacheManager = springCacheManager; this.springCache = springCacheManager.getCache(cacheName); &#125; // 自定义实现，都由 Spring 的缓存对象进行操作 @Override public V get(K key) throws CacheException &#123; Cache.ValueWrapper valueWrapper = springCache.get(key); if (valueWrapper == null) &#123; return null; &#125; return (V) valueWrapper.get(); &#125; @Override public V put(K key, V value) throws CacheException &#123; springCache.put(key, value); return value; &#125; @Override public V remove(K key) throws CacheException &#123; V value = this.get(key); springCache.evict(key); return value; &#125; /** * 清空缓存 * @throws CacheException */ @Override public void clear() throws CacheException &#123; springCache.clear(); &#125; /** * 获取所有缓存key的集合 * * @return */ @Override public Set&lt;K&gt; keys() &#123; return (Set&lt;K&gt;) springCacheManager.getCacheNames(); &#125; /** * 获取所有缓存value值的集合 * * @return */ @Override public Collection&lt;V&gt; values() &#123; List&lt;V&gt; list = new ArrayList&lt;&gt;(); Set&lt;K&gt; keys = keys(); for (K k : keys) &#123; list.add(this.get(k)); &#125; return list; &#125; /** * 获取缓存对象的数量 * * @return */ @Override public int size() &#123; int size = keys().size(); return size; &#125;&#125; ShiroCacheManager12345678910111213public class ShiroCacheManager&lt;K, V&gt; implements CacheManager &#123; // Spring 的缓存管理器 private org.springframework.cache.CacheManager springCacheManager; public ShiroCacheManager(org.springframework.cache.CacheManager springCacheManager) &#123; this.springCacheManager = springCacheManager; &#125; @Override public &lt;K, V&gt; Cache&lt;K, V&gt; getCache(String cacheName) throws CacheException &#123; // 通过缓存名在缓存管理器中获取对应的缓存 return new ShiroCache&lt;&gt;(springCacheManager, cacheName); &#125;&#125; 1234567891011121314151617/*** shiro 的缓存管理器* @return*/@Bean(name = \"shiroCacheManager\")public ShiroCacheManager shiroCacheManager()&#123; // 创建一个 Redis 缓存的默认配置 RedisCacheConfiguration conf = RedisCacheConfiguration.defaultCacheConfig(); // 设置会话的有效期 conf = conf.entryTtl(Duration.ofSeconds(30)); RedisCacheManager cacheManager = RedisCacheManager .builder(redisConnectionFactory) .cacheDefaults(conf) .build(); // 不用 ShiroCacheManager 返回的其实是一个 RedisCacheManager return new ShiroCacheManager(cacheManager);&#125; 注入自定义的缓存管理器 12345678910111213141516@Slf4j@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(\"shiroCacheManager\") private ShiroCacheManager shiroCacheManager; @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Autowired DefaultWebSecurityManager securityManager)&#123; // ... // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // ... &#125;&#125; ShiroSessionDAO1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Repositorypublic class ShiroSessionDAO extends AbstractSessionDAO &#123; @Autowired private RedisTemplate&lt;Serializable, Session&gt; redisTemplate; private String name = \"shiro-session:\"; private long timeout = 30; @Override protected Serializable doCreate(Session session) &#123; // 生成会话的唯一id Serializable sessionId = generateSessionId(session); super.create(session); redisTemplate.opsForValue().set(name + sessionId, session); return sessionId; &#125; @Override protected Session doReadSession(Serializable sessionId) &#123; super.readSession(sessionId); // 更新缓存 redisTemplate.expire(sessionId, timeout, TimeUnit.MINUTES); return redisTemplate.opsForValue().get(sessionId); &#125; @Override public void update(Session session) throws UnknownSessionException &#123; redisTemplate.opsForValue().set(name + session.getId(), session); &#125; @Override public void delete(Session session) &#123; redisTemplate.delete(name + session.getId()); &#125; @Override public Collection&lt;Session&gt; getActiveSessions() &#123; Set&lt;Serializable&gt; set = redisTemplate.keys(name + \"*\"); List&lt;Session&gt; sessionList = new ArrayList&lt;&gt;(); for (Serializable sessionId : set) &#123; sessionList.add(redisTemplate.opsForValue().get(name + sessionId)); &#125; return sessionList; &#125;&#125; 配置类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(\"shiroCacheManager\") private ShiroCacheManager shiroCacheManager; /** * 配置自定义SessionDAO * @return */ @Bean public SessionDAO sessionDAO() &#123; SessionDAO sessionDAO = new ShiroSessionDAO(shiroCacheManager); return sessionDAO; &#125; /** * 配置会话管理器 * @return */ @Bean public SessionManager sessionManager()&#123; // WEB 环境非 HttpSession 会话管理器 DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); // 注入自定义的 SessionDao sessionManager.setSessionDAO(sessionDAO()); return sessionManager; &#125; /** * 配置安全管理器 * * @return */ @Bean public SecurityManager securityManager() &#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 注入自定义Realm securityManager.setRealm(shiroRealm()); // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // 注入会话管理器 securityManager.setSessionManager(sessionManager()); return securityManager; &#125; /** * 装配 自定义Realm * * @return */ @Bean public ShiroRealm shiroRealm() &#123; return new ShiroRealm(); &#125; /** * 配置权限权限过滤器 * @return */ @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean() &#123; ShiroFilterFactoryBean filter = new ShiroFilterFactoryBean(); // 注入安全管理器 filter.setSecurityManager(securityManager()); // 未认证的跳转地址 filter.setLoginUrl(\"/login\"); Map&lt;String, String&gt; chain = new LinkedHashMap&lt;&gt;(); chain.put(\"/login\", \"anon\"); // 登录链接不拦截 chain.put(\"/css/**\", \"anon\"); chain.put(\"/img/**\", \"anon\"); chain.put(\"/js/**\", \"anon\"); chain.put(\"/lib/**\", \"anon\"); chain.put(\"/**\", \"user\"); filter.setFilterChainDefinitionMap(chain); return filter; &#125; /** * 启用 Shiro 注解 * @return */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor() &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); // 注入安全管理器 advisor.setSecurityManager(securityManager()); return advisor; &#125; /** * 启用 Shiro Thymeleaf 标签支持 * @return */ @Bean public ShiroDialect shiroDialect() &#123; return new ShiroDialect(); &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"},{"name":"Security","slug":"Security","permalink":"http://yoursite.com/tags/Security/"},{"name":"Shiro","slug":"Shiro","permalink":"http://yoursite.com/tags/Shiro/"}]},{"title":"Spring Boot 检索","slug":"后台技术/Spring Boot/Spring Boot 检索","date":"2020-03-31T06:50:45.000Z","updated":"2020-07-10T04:13:23.189Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 检索/","link":"","permalink":"http://yoursite.com/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%A3%80%E7%B4%A2/","excerpt":"ElasticSearch 基本介绍以及在 Spring Boot 中的整合使用。","text":"ElasticSearch 基本介绍以及在 Spring Boot 中的整合使用。 ElasticSearch如今的应用经常需要添加检索功能，开源的 ElasticSearch 是目前全文搜索引擎的首选。他可以快速的存储、搜索和分析海量数据。Spring Boot 通过整合 Spring Data ElasticSearch 为我们提供了非常便捷的检索功能支持。 Elasticsearch 是一个分布式搜索服务，提供 Restful API，底层基于 Lucene，采用多 shard（分片）的方式保证数据安全，并且提供自动 resharding 的功能，github 等大型的站点也是采用了 ElasticSearch 作为其搜索服务。 Elasticsearch 本身就是分布式的，即便只有一个节点，Elasticsearch 默认也会对的数据进行分片和副本操作，向集群添加新数据时，数据也会在新加入的节点中进行平衡 相关概念一个 ElasticSearch 集群可以包含多个索引 ，相应的每个索引可以包含多个类型 。这些不同的类型存储着多个文档 ，每个文档又有多个属性 。 Elasticsearch 也是基于 Lucene 的全文检索库，本质也是存储数据，很多概念与关系型数据库是一致的，如下对照： 索引库 关系型数据库 类型（type） Table 数据表 文档（Document） Row 行 字段（Field） Columns 列 另外，在 Elasticsearch 有一些集群相关的概念： 索引集（Indices，index 的复数）：逻辑上的完整索引； 分片（shard）：数据拆分后的各个部分； 副本（replica）：每个分片的复制。 整合 Spring Boot 提供了两种方式操作 Elasticsearch，Jest 和 SpringData。 Docker 安装部署 ElasticSearch 12345# 下载镜像docker pull elasticsearch# Elasticsearch 启动是会默认分配 2G 的内存 ，我们启动是设置小一点，防止我们内存不够启动失败# 9200 是 Elasticsearch 默认的 web 通信接口，9300 是分布式情况下，Elasticsearch 各个节点的通信端口docker run -e ES_JAVA_OPTS=\"-Xms256m -Xmx256m\" -d -p 9200:9200 -p 9300:9300 --name es01 5c1e1ecfe33a Jest ElasticSearch already has a Java API which is also used by ElasticSearch internally, but Jest fills a gap, it is the missing client for ElasticSearch Http Rest interface. 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt;&lt;/dependency&gt; 配置服务器： 1spring.elasticsearch.jest.uris=http://127.0.0.1:9200 ## Elasticsearch 服务器 编写实体类： 12345678910111213141516171819202122232425public class Article &#123; // If @JestId value is null, it will be set the value of ElasticSearch generated \"_id\". @JestId // @JestId annotation can be used to mark a property of a bean as id private Integer id; private String author; private String title; private String content; // Getter / Setter @Override public String toString() &#123; final StringBuilder sb = new StringBuilder( \"&#123;\\\"Article\\\":&#123;\" ); sb.append( \"\\\"id\\\":\" ) .append( id ); sb.append( \",\\\"author\\\":\\\"\" ) .append( author ).append( '\\\"' ); sb.append( \",\\\"title\\\":\\\"\" ) .append( title ).append( '\\\"' ); sb.append( \",\\\"content\\\":\\\"\" ) .append( content ).append( '\\\"' ); sb.append( \"&#125;&#125;\" ); return sb.toString(); &#125;&#125; 测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootElasticsearchApplicationTests &#123; @Autowired JestClient jestClient; @Test public void createIndex() &#123; // 初始化一个文档 Article article = new Article(); article.setId( 1 ); article.setTitle( \"好消息\" ); article.setAuthor( \"张三\" ); article.setContent( \"Hello World\" ); // 构建一个索引 Index index = new Index.Builder(article).index(\"shekou\").type(\"news\").build(); try &#123; // 执行 jestClient.execute(index); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Test public void search() &#123; // 查询表达式 String query = \"&#123;\\n\" + \" \\\"query\\\" : &#123;\\n\" + \" \\\"match\\\" : &#123;\\n\" + \" \\\"content\\\" : \\\"hello\\\"\\n\" + \" &#125;\\n\" + \" &#125;\\n\" + \"&#125;\"; // 构建搜索功能 Search search = new Search.Builder(query).addIndex(\"shekou\").addType(\"news\").build(); try &#123; // 执行 SearchResult result = jestClient.execute(search); System.out.println(result.getJsonString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 搜索表达式查询： Searching Document Spring Dataspring-data-elasticsearch 使用之前必须先确定 Elasticsearch 版本：点击查看官方文档 Spring Data 通过注解来声明字段的映射属性，注解的详细用法： @Document 作用在类，标记实体类为文档对象，一般有两个属性： indexName：对应索引库名称； type：对应在索引库中的类型； shards：分片数量，默认5； replicas：副本数量，默认1。 @Id作用在成员变量，标记一个字段作为 id 主键。 @Field作用在成员变量，标记为文档的字段，并指定字段映射属性： type：字段类型，是枚举：FieldType，可以是 text、long、short、date、integer、object 等： Text：存储数据时候，会自动分词，并生成索引； Keyword：存储数据时候，不会分词建立索引； Numerical：数值类型，分两类： 基本数据类型：long、interger、short、byte、double、float、half_float； 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如 10 或 100。Elasticsearch 会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 Elasticsearch 可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为 long，节省空间。 index：是否索引，布尔类型，默认是true； store：是否存储，布尔类型，默认是false； analyzer：分词器名称。 CRUD Spring Data 的强大之处，就在于你不用写任何 DAO 处理，自动根据方法名或类的信息进行 CRUD 操作。只要你定义一个接口，然后继承 Repository 提供的一些子接口，就能具备各种基本的 CRUD 功能。 123public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123;&#125; 测试12345678910111213141516171819202122// 创建一个实体类并标注为文档@Document(indexName = \"item\", type = \"docs\", shards = 1, replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text) private String title; @Field(type = FieldType.Keyword) private String category; @Field(type = FieldType.Keyword) private String brand; @Field(type = FieldType.Double) private Double price; @Field(index = false, type = FieldType.Keyword) private String images;&#125; 调用 elasticsearchTemplate 创建索引并映射： 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTest(classes = EsDemoApplication.class)public class EsDemoApplicationTests &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Test public void testCreateIndex() &#123; // 根据 Item 类的 @Document 注解信息来创建 elasticsearchTemplate.createIndex(Item.class); &#125; @Test public void insert() &#123; Item item = new Item(1L, \"Redmi\", \" 手机\", \"小米\", 1499.00, \"RedmiImage.jpg\"); itemRepository.save(item); &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"Spring Boot 消息中间件","slug":"后台技术/Spring Boot/Spring Boot 消息队列","date":"2020-03-31T03:50:45.000Z","updated":"2020-07-10T04:13:46.155Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 消息队列/","link":"","permalink":"http://yoursite.com/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"MQ 基本介绍以及与 Spring Boot 的整合使用。","text":"MQ 基本介绍以及与 Spring Boot 的整合使用。 概述在大多数应用程序中，可通过消息服务中间件来提升系统异步通信、扩展解耦能力。 消息代理（message broker）；目的地（destination）。 当消息发送者发送消息以后，将由消息代理接管，消息代理保证消息传递到指定目的地。消息队列主要有两种形式的目的地。 队列（queue）：点对点消息通信（point-to-point）；主题（topic）：发布（publish）/ 订阅（subscribe）消息通信。 应用场景：异步处理（邮件）、应用解耦、流量削峰（秒杀）。 JMS：Java 消息服务（Java Message Service）应用程序接口是一个 Java 平台中关于面向消息中间件（MOM）的 API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java 消息服务是一个与具体平台无关的API，绝大多数 MOM 提供商都对 JMS 提供支持。 AMQP：高级消息队列协议（Advanced Message Queuing Protocol）, 对于面向消息中间件的应用层协议。 常见消息中间件ActiveMQ Apache ActiveMQ 是 Apache 软件基金会所研发的开放源代码消息中间件。 RabbitMQ RabbitMQ 是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ 服务器是用 Erlang 语言编写的， Kafka 发布订阅消息系统、分布式日志服务。本身是做日志储存的，所以对消息的顺序有严格的要求。开源流处理平台，由 Scala 和 java 编写，目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个按照分布式事务日志架构的大规模发布 / 订阅消息队列。 编码接口JMS 编码接口： ConnectionFactory：创建连接到消息中间件的连接工厂； Connection：通信链路； Destination：消息发布和接收的地点，包括队列和主题； Session：会话，表示一个单线程的上下文； MessageConsumer：会话创建，用于接收消息； MessageProducer：会话创建，用于发送消息； Message：消息对象，包括消息头，消息属性，消息体。 一个 Connection 可以创建多个会话，即一个连接可以供多个线程使用。 Spring 支持 spring-jms 提供了对 JMS 的支持；spring-rabbit 提供了对 AMQP 的支持。 需要 ConnectionFactory 的实现来连接消息代理； 提供 JmsTemplate、RabbitTemplate 来发送消息； @JmsListener（JMS）、@RabbitListener（AMQP）注解在方法上监听消息代理发布的消息； @EnableJms、@EnableRabbit 开启支持。 Spring Boot 自动配置 JmsAutoConfiguration、RabbitAutoConfiguration RabbitMQ核心概念Message 消息：消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者：是一个向交换器发布消息的客户端应用程序。 Exchange 交换器：用来接收生产者发送的消息并将这些消息路由给服务器中的队列。Exchange 有 4 种类型：direct（默认：routing key = binding key）、fanout（广播）、 topic,、和 headers（匹配消息的 Header 而不是路由键），不同类型的 Exchange 转发消息的策略有所区别。 topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符： # 匹配 0 个或多个单词；*匹配一个单词。 Queue 消息队列：用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Binding 绑定：用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Exchange 和 Queue 的绑定可以是多对多的关系。 Connection 网络连接：比如一个TCP连接。 Channel 信道：多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内的虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer消息的消费者：表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker 表示消息队列服务器实体。 RabbitMQ 整合导入模块依赖的 starter： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot 配置： 123456## 配置 rabbitMq 服务器rabbitmq: host: 127.0.0.1 port: 5672 username: root password: root 配置一个直连型的交换机： 123456789101112131415161718192021@Configurationpublic class DirectRabbitConfig &#123; // 初始化队列名：TestDirectQueue @Bean public Queue TestDirectQueue() &#123; return new Queue(\"TestDirectQueue\",true); // true 持久化 &#125; // 初始化 Direct 交换机：TestDirectExchange @Bean DirectExchange TestDirectExchange() &#123; return new DirectExchange(\"TestDirectExchange\"); &#125; // 定义队列和交换机绑定, 并设置用于匹配键：TestDirectRouting @Bean Binding bindingDirect() &#123; return BindingBuilder.bind(TestDirectQueue()).to(TestDirectExchange()).with(\"TestDirectRouting\"); &#125;&#125; 写个简单的接口进行消息推送（根据需求也可以改为定时任务等等，具体看需求）： 1234567891011121314151617181920@RestControllerpublic class SendMessageController &#123; @Autowired RabbitTemplate rabbitTemplate; // 使用 RabbitTemplate：提供了接收 / 发送等方法 @GetMapping(\"/sendDirectMessage\") public String sendDirectMessage() &#123; String messageId = String.valueOf(UUID.randomUUID()); String messageData = \"test message, hello!\"; String createTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(\"messageId\",messageId); map.put(\"messageData\",messageData); map.put(\"createTime\",createTime); // 将消息携带绑定键值 TestDirectRouting 发送到交换机 TestDirectExchange rabbitTemplate.convertAndSend(\"TestDirectExchange\", \"TestDirectRouting\", map); return \"ok\"; &#125;&#125; consumer 创建一个消费者项目，消费者进行消息监听，需要手动创建消息接收的监听类。 123456789@Component@RabbitListener(queues = \"TestDirectQueue\") // 监听的队列名称 TestDirectQueuepublic class DirectReceiver &#123; @RabbitHandler public void process(Map testMessage) &#123; System.out.println(\"DirectReceiver 消费者收到消息: \" + testMessage.toString()); &#125;&#125; 回调函数12345678910111213141516171819202122232425262728293031323334@Configurationpublic class RabbitConfig &#123; @Bean public RabbitTemplate createRabbitTemplate(ConnectionFactory connectionFactory)&#123; RabbitTemplate rabbitTemplate = new RabbitTemplate(); rabbitTemplate.setConnectionFactory(connectionFactory); // 设置开启 Mandatory 才能触发回调函数，无论消息推送结果怎么样都强制调用回调函数 rabbitTemplate.setMandatory(true); // 交换机相关信息 rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.out.println(\"ConfirmCallback: \"+\"相关数据：\"+correlationData); System.out.println(\"ConfirmCallback: \"+\"确认情况：\"+ack); System.out.println(\"ConfirmCallback: \"+\"原因：\"+cause); &#125; &#125;); // 队列相关信息 rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.out.println(\"ReturnCallback: \"+\"消息：\"+message); System.out.println(\"ReturnCallback: \"+\"回应码：\"+replyCode); System.out.println(\"ReturnCallback: \"+\"回应信息：\"+replyText); System.out.println(\"ReturnCallback: \"+\"交换机：\"+exchange); System.out.println(\"ReturnCallback: \"+\"路由键：\"+routingKey); &#125; &#125;); return rabbitTemplate; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://yoursite.com/tags/MQ/"},{"name":"Springboot","slug":"Springboot","permalink":"http://yoursite.com/tags/Springboot/"}]},{"title":"Spring Boot 缓存","slug":"后台技术/Spring Boot/Spring Boot 缓存","date":"2020-03-31T01:59:45.000Z","updated":"2020-07-10T04:13:20.856Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 缓存/","link":"","permalink":"http://yoursite.com/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E7%BC%93%E5%AD%98/","excerpt":"JSR107、Spring 缓存抽象。","text":"JSR107、Spring 缓存抽象。 JRS107 Java Caching 定义了 5 个核心接口，分别是 CachingProvider, CacheManager, Cache, Entry 和 Expiry。 • CachingProvider：定义了创建、配置、获取、管理和控制多个 CacheManager。一个应用可以在运行期访问多个 CachingProvider； • CacheManager：定义了创建、配置、获取、管理和控制多个唯一命名的 Cache，这些 Cache存在于 CacheManager 的上下文中。一个 CacheManager 仅被一个 CachingProvider 所拥有； • Cache：是一个类似 Map 的数据结构并临时存储以 Key 为索引的值。一个 Cache 仅被一个 CacheManager 所拥有； • Entry：是一个存储在 Cache 中的 key-value 对； • Expiry：每一个存储在 Cache 中的条目有一个定义的有效期。一旦超过这个时间，条目为过期的状态。一旦过期，条目将不可访问、更新和删除。缓存有效期可以通过ExpiryPolicy设置。 Spring 缓存抽象Spring 3.1 以上版本定义了 org.springframework.cache.Cache 和 org.springframework.cache.CacheManager 接口来统一不同的缓存技术；并支持使用 JCache（JSR-107）注解简化我们开发。 Cache 接口为缓存的组件规范定义，包含缓存的各种操作集合；Cache 接口下 Spring 提供了各种 xxxCache 的实现；如 RedisCache、EhCacheCache 、ConcurrentMapCache等。 每次调用需要缓存功能的方法时，Spring 会检查检查指定参数的指定的目标方法是否已经被调用过：如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户。下次调用直接从缓存中获取。 确定方法需要被缓存以及他们的缓存策略；从缓存中读取之前缓存存储的数据。 注解基本流程添加缓存模块的 stater： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt; 引入 Redis 的 starter，容器中默认保存的是 RedisCacheManager。RedisCacheManager 负责创建RedisCache，RedisCache 负责操作 Redis 缓存数据。默认保存数据 Key / Value 都是 Object，默认使用的是 JDK 序列化器。 在 Spring Boot 主类中添加 EnableCaching 注解开启缓存功能： 1234567@SpringBootApplication@EnableCachingpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 在类上添加CacheConfig进行缓存配置；在方法上添加@Cacheable，标注此方法返回值需要被缓存。 12345@CacheConfig(cacheNames = \"users\")public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Cacheable User findByName(String name);&#125; 注解详解@CacheConfig：主要用于配置该类中会用到的一些共用的缓存配置。在这里@CacheConfig(cacheNames = &quot;users&quot;)：配置了该数据访问对象中返回的内容将存储于名为 users 的缓存对象中，我们也可以不使用该注解，直接通过@Cacheable自己配置缓存集的名字来定义。 @Cacheable：配置了 findByName 函数的返回值将被加入缓存。同时在查询时，会先从缓存中获取，若不存在才再发起对数据库的访问。该注解主要有下面几个参数： value、cacheNames：两个等同的参数（cacheNames为 Spring 4 新增，作为value的别名），用于指定缓存存储的集合名。由于 Spring 4 中新增了@CacheConfig，因此在 Spring 3 中原本必须有的value属性，也成为非必需项了； key：缓存对象存储在 Map 集合中的 key 值，非必需，缺省按照函数的所有参数组合作为 key 值，若自己配置需使用 SpEL 表达式，比如：@Cacheable(key = &quot;#p0&quot;)：使用函数第一个参数作为缓存的 key 值，更多关于 SpEL 表达式的详细内容可参考官方文档； condition：缓存对象的条件，非必需，也需使用 SpEL 表达式，只有满足表达式条件的内容才会被缓存，比如：@Cacheable(key = &quot;#p0&quot;, condition = &quot;#p0.length() &lt; 3&quot;)，表示只有当第一个参数的长度小于3的时候才会被缓存； unless：另外一个缓存条件参数，非必需，需使用 SpEL 表达式。它不同于condition参数的地方在于它的判断时机，该条件是在函数被调用之后才做判断的，所以它可以通过对 result 进行判断； keyGenerator：用于指定 key 生成器，非必需。若需要指定一个自定义的 key 生成器，需要开发者实现org.springframework.cache.interceptor.KeyGenerator接口，并使用该参数来指定。需要注意的是：该参数与key是互斥的； cacheManager：用于指定使用哪个缓存管理器，非必需。只有当有多个时才需要使用； cacheResolver：用于指定使用那个缓存解析器，非必需。需通过org.springframework.cache.interceptor.CacheResolver接口来实现自己的缓存解析器，并用该参数指定。 @CachePut：配置于函数上，能够根据参数定义条件来进行缓存，它与@Cacheable不同的是，它每次都会真实调用函数，所以主要用于数据新增和修改操作上。它的参数与@Cacheable类似，具体功能可参考上面对@Cacheable参数的解析。 @CacheEvict：配置于函数上，通常用在删除方法上，用来从缓存中移除相应数据。除了同@Cacheable一样的参数之外，它还有下面两个参数： allEntries：非必需，默认为 false。当为 true 时，会移除所有数据； beforeInvocation：非必需，默认为 false，会在调用方法之后移除数据。当为 true 时，会在调用方法之前移除数据。 TemplateSpring Boot 提供了帮助操作 Redis 的 Helper 类 RedisTemplate：RedisTemplate的 K:V 均为 Object；StringRedisTemplate 继承自 RedisTemplate，是专为 String:String 类型的 K:V 提供服务。 在 RedisAutoConfiguration 中 Spring Boot 自动注册了 RedisTemplate 和 StringRedisTemplate。 数据操作 Redis 中常见的五大类数据类型：String，List，Set，Hash 和 ZSet。 RedisTemplate 封装了对五大类数据进行操作的方法，每一个方法都会返回一个 Operations 对象。 123456789RedisTemplate.opsForValue(); // StringRedisTemplate.opsForList();RedisTemplate.opsForSet();RedisTemplate.opsForHash();RedisTemplate.opsForZSet(); Redis 实现缓存入门基础 String 最基础的数据类型；List 元素不具有唯一性；有序；是一个双向链表；既可以是栈，也可以是队列；Set 元素具有唯一性；无序；Hash 存储的是key-value结构，key必须是string；类似于 MySQL 中的一条记录。 Redis 命令接口 Redis 提供了许多命令供我们使用，同样在 Spring Boot 中也封装了对应类型的命令接口供开发者使用。 123456789101112131415161718192021/** * Interface for the commands supported by Redis. * * @author Costin Leau * @author Christoph Strobl */public interface RedisCommands extends RedisKeyCommands, RedisStringCommands, RedisListCommands, RedisSetCommands, RedisZSetCommands, RedisHashCommands, RedisTxCommands, RedisPubSubCommands, RedisConnectionCommands, RedisServerCommands, RedisScriptingCommands, RedisGeoCommands, HyperLogLogCommands &#123; /** * 'Native' or 'raw' execution of the given command along-side the given arguments. The command is executed as is, * with as little 'interpretation' as possible - it is up to the caller to take care of any processing of arguments or * the result. * * @param command Command to execute * @param args Possible command arguments (may be null) * @return execution result. */ Object execute(String command, byte[]... args);&#125; 在 Spring Boot 配置文件中添加 Redis 配置： 1234567891011121314151617## 默认密码为空redis: host: 127.0.0.1 # Redis服务器连接端口 port: 6379 jedis: pool: # 连接池最大连接数（使用负值表示没有限制） max-active: 100 # 连接池中的最小空闲连接 max-idle: 10 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: 100000 # 连接超时时间（毫秒） timeout: 5000 # 默认是索引为 0 的数据库 database: 0 代码示例1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringBootCacheApplicationTests &#123; @Autowired EmployeeMapper employeeMapper; @Autowired StringRedisTemplate stringRedisTemplate; @Autowired RedisTemplate redisTemplate; // 向 Redis 中保存一个对象并取出打印 @Test public void test() &#123; Employee employee = employeeMapper.getEmpById(1); redisTemplate.opsForValue().set(\"emp01\",employee); Object emp01 = redisTemplate.opsForValue().get(\"emp01\"); System.out.println(emp01); &#125;&#125; 注意，Redis 中对象的保存是需要进行序列化的，默认使用 JdkSerializationRedisSerializer 序列化器，所以在 Redis 中查看保存的对象是序列化后的二进制编码，正常使用是没有问题的，查询出来的时候会自动反序列化。 如果习惯于使用 String，那么可以将其 JSON 化，两种方式：使用第三方 JSON 或者向容器中添加自定义 RedisTemplate 改变其默认序列化器。 添加自定义 RedisTemplate： 123456789101112@Configurationpublic class MyRedisConfig &#123; @Bean public RedisTemplate&lt;Object,Employee&gt; empRedisTemplate(RedisConnectionFactory connectionFactory)&#123; RedisTemplate&lt;Object,Employee&gt; template = new RedisTemplate&lt;Object, Employee&gt;(); template.setConnectionFactory(connectionFactory); Jackson2JsonRedisSerializer&lt;Employee&gt; serializer = new Jackson2JsonRedisSerializer&lt;Employee&gt;(Employee.class); template.setDefaultSerializer(serializer); return template; &#125;&#125; Lettuce Jedis 在实现上是直接连接 Redis 服务器，在多个线程间共享一个 Jedis 实例时是线程不安全的，如果想要在多线程场景下使用 Jedis ，需要使用连接池，每个线程都使用自己的 Jedis 实例，当连接数量增多时，会消耗较多的物理资源。 与 Jedis 相比， Lettuce 则完全克服了其线程不安全的缺点： Lettuce 是一个可伸缩的线程安全的 Redis 客户端，支持同步、异步和响应式模式。多个线程可以共享一个连接实例，而 不必担心多线程并发问题。它基于优秀 Netty NIO 框架构建，支持 Redis 的更多高级功能。 12345678910&lt;!-- redis 访问启动器 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- redis 客户端 Lettuce 数据库连接池依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 123456789101112131415161718# Redis数据库索引（默认为0）spring.redis.database=0# Redis服务器地址 （默认localhost）spring.redis.host=localhost# Redis服务器连接端口 （默认6379）spring.redis.port=6379# Redis服务器连接密码（默认为空）spring.redis.password=123456# 连接超时时间spring.redis.timeout=10000ms# 最大连接数（使用负值表示没有限制） 默认 8spring.redis.lettuce.pool.max-active=8# 最小空闲连接 默认 0spring.redis.lettuce.pool.min-idle=0# 最大空闲连接 默认 8spring.redis.lettuce.pool.max-idle=8# 最大阻塞等待时间（使用负值表示没有限制） 默认 -1msspring.redis.lettuce.pool.max-wait=-1ms 缓存原理 要使用 Spring Boot 的缓存功能，还需要提供一个缓存的具体实现。 Spring Boot 一定的顺序去侦测缓存实现。 Spring 提供了一个统一访问缓存的接口：CacheManager ctrl + alt + b 可查看这个接口的实现 在 RedisCacheManager 中查看 Redis 缓存的默认配置类：RedisCacheConfiguration： 从源码了解到 Sping Boot 的 Reids 缓存对 Key 和 Value 默认的序列化器分别是 String 类型和 JDK 序列器。 在 RedisCacheManager 中可以看到如何实例化这个类： 实例化这个类必须传入一个 connectionFactory，用 redis 缓存所以传入一个 redisConnectionFactory。 1234567891011121314@Configurationpublic class CacheConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; /** * SpringCacheManager 缓存管理器：使用的缓存产品是Redis * @return */ @Bean(name = \"springCacheManager\") public RedisCacheManager springCacheManager()&#123; RedisCacheManager cacheManager = RedisCacheManager.create(redisConnectionFactory); return cacheManager; &#125;&#125; Redis 配置123456789101112131415161718192021222324252627282930313233@Configurationpublic class RedisConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public StringRedisSerializer stringRedisSerializer() &#123; StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); return stringRedisSerializer; &#125; @Bean public StringRedisTemplate stringRedisTemplate() &#123; StringRedisTemplate stringRedisTemplate =new StringRedisTemplate(); stringRedisTemplate.setConnectionFactory(redisConnectionFactory); // 开启事务支持 stringRedisTemplate.setEnableTransactionSupport(true); return stringRedisTemplate; &#125; @Bean public RedisTemplate redisTemplate()&#123; RedisTemplate&lt;Object,Object&gt; redisTemplate= new RedisTemplate&lt;&gt;(); // 序列化器 redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setKeySerializer(stringRedisSerializer()); //redisTemplate.setHashKeySerializer(stringRedisSerializer()); // 开启事务支持 redisTemplate.setEnableTransactionSupport(true); return redisTemplate; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Springboot","slug":"Springboot","permalink":"http://yoursite.com/tags/Springboot/"},{"name":"Cache","slug":"Cache","permalink":"http://yoursite.com/tags/Cache/"}]},{"title":"MyBatis 插件之拦截器","slug":"开发杂项/Mybatis 插件之拦截器","date":"2020-03-29T08:18:40.000Z","updated":"2020-07-10T04:16:03.620Z","comments":true,"path":"2020/03/29/开发杂项/Mybatis 插件之拦截器/","link":"","permalink":"http://yoursite.com/2020/03/29/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/Mybatis%20%E6%8F%92%E4%BB%B6%E4%B9%8B%E6%8B%A6%E6%88%AA%E5%99%A8/","excerpt":"MyBatis 拦截器的详细用法介绍。","text":"MyBatis 拦截器的详细用法介绍。 在很多业务场景下我们需要去拦截 SQL，达到不入侵原有代码业务处理一些东西，比如：分页操作，数据权限过滤操作， SQL 执行时间性能监控等等。在某些应用场景下就需要使用到 MyBatis 的拦截器。 Mybatis 核心对象Configuration 初始化基础配置：比如 MyBatis 的别名等，一些重要的类型对象，如：插件，映射器，ObjectFactory 和 typeHandler 对象等，MyBatis 所有的配置信息都保存在 Configuration 对象之中； SqlSessionFactory SqlSession 工厂：SqlSession 作为 MyBatis工作的主要顶层 API，表示与数据库交互的会话，完成必要数据库增删改查功能； Executor MyBatis 执行器：是 MyBatis 调度的核心，负责 SQL 语句的生成和查询缓存的维护； ParameterHandler：负责对用户传递的参数转换成 JDBC Statement 所需要的参数； ResultSetHandler： 负责将 JDBC 返回的 ResultSet 结果集对象转换成 List 类型的集合； TypeHandler：负责 Java 数据类型和 JDBC 数据类型之间的映射和转换； MappedStatement：维护了一条 &lt;select | update | delete | insert&gt; 节点的封装； SqlSource：负责根据用户传递的 parameterObject，动态地生成 SQL 语句，将信息封装到 BoundSql 对象中，并返回； BoundSql：表示动态生成的 SQL 语句以及相应的参数信息。 拦截器原理实现Mybatis 支持对 Executor、StatementHandler、PameterHandler 和 ResultSetHandler 接口进行拦截，也就是说可以对这个四种类型的对象进行代理。 注解@Intercepts在实现 Interceptor 接口的类声明，使该类注册成为拦截器。 @Signature(type = Executor.class,method = &quot;&quot;, args = {) type：表示拦截的类，这里是 Executor 的实现类； method：表示拦截的方法，这里是拦截 Executor 的 query 方法； args：表示方法参数。 123456789@Slf4j@Component@Intercepts(&#123;@Signature(type = Executor.class, method = \"update\", args = &#123;MappedStatement.class, Object.class&#125;), @Signature(type = Executor.class, method = \"query\", args = &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125;)&#125;)public class SqlInterceptor implements Interceptor &#123; private Properties properties; // ...后面给出详细方法&#125; intercept 方法 要想拦截目标对象的目标方法的执行，可以通过 invocation 来获取拦截的目标方法及参数，以及执行目标方法，包含代理的几个重要元素 method、target、args。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124// 通过 invocation 拦截目标对象的目标方法执行中需要被打印输出的信息@Overridepublic Object intercept(Invocation invocation) throws Throwable &#123; // MappedStatement 维护了一条 mapper.xml 文件里面 select 、update、delete、insert 节点的封装 MappedStatement mappedStatement = (MappedStatement) invocation.getArgs()[0]; Object parameter = null; if (invocation.getArgs().length &gt; 1) &#123; parameter = invocation.getArgs()[1]; &#125; // 取得 SQL 标识符 String sqlId = mappedStatement.getId(); // 取得动态生成的 SQL 语句以及相应的参数信息对象 BoundSql boundSql = mappedStatement.getBoundSql(parameter); // 取得 Mybatis 的配置信息 Configuration configuration = mappedStatement.getConfiguration(); Object returnValue = null; long start = System.currentTimeMillis(); // 继续执行原目标方法 returnValue = invocation.proceed(); // 获取实体类 Class&lt;?&gt; parameterType = getParameterType(mappedStatement); // 判断是否包含 SQL 语句打印注解 if (!hasSqlPrintAnnotation(parameterType)) &#123; return returnValue; &#125; long end = System.currentTimeMillis(); long time = (end - start); // 筛选出执行时间较长的 SQL 语句以便优化 if (time &gt; 1) &#123; // 自定义输出的 SQL 语句 String sql = getSql(configuration, boundSql, sqlId, time); log.info(sql); &#125; // 返回执行后的结果 return returnValue;&#125;// 获取实体对象类private Class&lt;?&gt; getParameterType(MappedStatement statement) &#123; if (statement.getParameterMap() == null || statement.getParameterMap().getType() == null) &#123; return null; &#125; return statement.getParameterMap().getType();&#125;// 判断实体类上是否包含自定义的 SQl 打印的注解 @SqlPrintprivate static boolean hasSqlPrintAnnotation(Class&lt;?&gt; classType) &#123; return classType == null ? Boolean.FALSE : classType.isAnnotationPresent(SqlPrint.class);&#125;private static String getSql(Configuration configuration, BoundSql boundSql, String sqlId, long time) &#123; String sql = showSql(configuration, boundSql); StringBuilder str = new StringBuilder(100); str.append(sqlId); str.append(\" : \"); // 具体执行的 SQL 语句 str.append(sql); str.append(\" &gt;&gt;&gt;&gt;&gt;&gt; \"); // SQL 执行的时间 str.append(\"SQL执行耗时: \"); str.append(time); str.append(\"ms\"); return str.toString();&#125;// 组装需要打印的 SQLprivate static String showSql(Configuration configuration, BoundSql boundSql) &#123; // 取得参数的对象，实际上就是一个多个参数的 map 结构 Object parameterObject = boundSql.getParameterObject(); // 查询 SQL 中的参数 List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); // 将 SQL 中一个或多个回车换行符号替换成一个空格 String sql = boundSql.getSql().replaceAll(\"[\\\\s]+\", \" \"); if (!parameterMappings.isEmpty() &amp;&amp; parameterObject != null) &#123; // Mybatis 在启动时就会通过 TypeHandlerRegistry 进行注册，即建立 JdbcType, JavaType, TypeHandler 三者之间的关系。 // 因此，这意味着在 Mybatis 启动时我们也需要通过 TypeHandlerRegistry 将我们的所有的枚举类型（JavaType）与自定义的枚举 // TypeHandler（EnumTypeHandler）建立联系 TypeHandlerRegistry typeHandlerRegistry = configuration.getTypeHandlerRegistry(); // 自定义 TypeHandler 时会走这个逻辑，建立 JavaType 和 JdbcType 之间的联系 if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123; // 替换 SQL 中的占位符 “?” 为具体的参数，replaceFirst 作为是替换匹配到的第一个占位符 “?” sql = sql.replaceFirst(\"\\\\?\", getFormatParameterValue(parameterObject)); &#125; // 没有自定义的 TypeHandler，走通用逻辑 else &#123; // 拿到 target（拦截对象）的元数据 MetaObject metaObject = configuration.newMetaObject(parameterObject); // 参数是按顺序存储的，下面逻辑按顺序来替换 for (ParameterMapping parameterMapping : parameterMappings) &#123; // 获取参数的名称 String propertyName = parameterMapping.getProperty(); // 如果元数据对象中存在该参数，则替换相应的占位符 “?” if (metaObject.hasGetter(propertyName)) &#123; Object obj = metaObject.getValue(propertyName); sql = sql.replaceFirst(\"\\\\?\", getFormatParameterValue(obj)); &#125; // 如果有额外的参数，走下面的逻辑 else if (boundSql.hasAdditionalParameter(propertyName)) &#123; Object obj = boundSql.getAdditionalParameter(propertyName); sql = sql.replaceFirst(\"\\\\?\", getFormatParameterValue(obj)); &#125; &#125; &#125; &#125; // 返回组装好的 SQL return sql;&#125;// 按类型格式化 SQL 里的参数private static String getFormatParameterValue(Object obj) &#123; String value = null; if (obj instanceof String) &#123; value = \"'\" + obj.toString() + \"'\"; &#125; else if (obj instanceof Date) &#123; DateFormat formatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT, DateFormat.DEFAULT, Locale.CHINA); value = \"'\" + formatter.format(new Date()) + \"'\"; &#125; else &#123; if (obj != null) &#123; value = obj.toString(); &#125; else &#123; value = \"\"; &#125; &#125;&#125; plugin 方法 包装目标对象，包装的意思就是为目标对象创建一个代理。 1234567891011@Overridepublic Object plugin(Object target) &#123; // 判断目标对象类型是否为所需类型 if (target instanceof Executor) &#123; // 为 target 创建一个动态代理，以此来实现方法拦截和增强功能：回调 intercept 方法 return Plugin.wrap(target, this); &#125; else &#123; // 无需拦截则直接返回目标对象本身 return target; &#125;&#125;","categories":[{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://yoursite.com/tags/MyBatis/"}]},{"title":"线程与进程","slug":"基础知识/线程与进程","date":"2020-03-28T06:25:21.000Z","updated":"2020-07-10T02:47:57.922Z","comments":true,"path":"2020/03/28/基础知识/线程与进程/","link":"","permalink":"http://yoursite.com/2020/03/28/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E7%BA%BF%E7%A8%8B%E4%B8%8E%E8%BF%9B%E7%A8%8B/","excerpt":"进程、线程、锁、同步等。 。","text":"进程、线程、锁、同步等。 。 线程与进程在 Java 中，一个应用程序对应着一个 JVM 实例。Java采用的是单线程编程模型，即在程序中如果没有主动创建线程的话，只会创建一个线程，通常称为主线程。但是要注意，虽然只有一个线程来执行任务，不代表 JVM 中只有一个线程，JVM 实例在创建的时候，同时会创建很多其他的线程（比如垃圾收集器线程）。 由于 Java 采用的是单线程编程模型，因此在进行 UI 编程时要注意将耗时的操作放在子线程中进行，以避免阻塞主线程（在UI编程时，主线程即UI线程，用来处理用户的交互事件）。 线程的创建 1）继承 Thread 类； 2）实现 Runnable 接口。 继承 Thread 类12345678910111213class MyThread extends Thread&#123; private static int num = 0; public MyThread()&#123; num++; &#125; @Override public void run() &#123; System.out.println(\"主动创建的第\"+num+\"个线程\"); &#125;&#125; 创建好了自己的线程类之后，就可以创建线程对象了，然后通过start()方法去启动线程。 123456public class Test &#123; public static void main(String[] args) &#123; MyThread thread = new MyThread(); // 创建一个子线程对象 thread.start(); // 运行子线程的 run() 方法 &#125;&#125; 注意，不是调用run()方法启动线程，run()方法中只是定义需要执行的任务，如果调用run()方法，即相当于在主线程中执行run()方法，跟普通的方法调用没有任何区别，此时并不会创建一个新的线程来执行定义的任务。 123456789101112131415161718192021222324// 错误案例public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"主线程ID:\"+Thread.currentThread().getId()); // 获取主线程的 ID MyThread thread01 = new MyThread(\"Thread01\"); thread01.start(); // 正确调用，开启子线程运行 run() 方法 MyThread thread02 = new MyThread(\"Thread02\"); thread02.run(); // 错误调用，运行主程序的 run() 方法 &#125;&#125; class MyThread extends Thread&#123; private String name; public MyThread(String name)&#123; this.name = name; &#125; @Override public void run() &#123; System.out.println(\"name:\"+name+\" 子线程ID:\"+Thread.currentThread().getId()); &#125;&#125; 实现 Runnable 接口Runnable 的中文意思是”任务“，顾名思义，通过实现 Runnable 接口，我们定义了一个子任务，然后将子任务交由 Thread 去执行。注意，这种方式必须将 Runnable 作为 Thread 类的参数，然后通过 Thread 的start()方法来创建一个新线程来执行该子任务。如果调用 Runnable 的run()方法的话，是不会创建新线程的，这根普通的方法调用没有任何区别。 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"主线程ID：\"+Thread.currentThread().getId()); MyRunnable runnable = new MyRunnable(); Thread thread = new Thread(runnable); // 将任务 Runnable 交由 Thread 处理 thread.start(); &#125;&#125; class MyRunnable implements Runnable&#123; public MyRunnable() &#123; &#125; @Override public void run() &#123; System.out.println(\"子线程ID：\"+Thread.currentThread().getId()); &#125;&#125; Thread VS. Runnable 事实上，查看 Thread 类的实现源代码会发现 Thread 类是实现了 Runnable 接口的。 在 Java 中，这两种方式都可以用来创建线程去执行子任务，具体选择哪一种方式要看自己的需求。直接继承 Thread 类的话，可能比实现 Runnable 接口看起来更加简洁，但是由于 Java 只允许单继承，所以如果自定义类需要继承其他类，则只能选择实现 Runnable 接口。 线程讲解线程的状态NEW (新建)：一个尚未启动的线程处于这一状态。 RUNNABLE (可运行) ：一个正在 Java 虚拟机中执行的线程处于这一状态。 处于 runnable 状态下的线程正在 Java 虚拟机中执行，但它可能正在等待来自于操作系统的其它资源，比如处理器。当进行阻塞式的 IO 操作时，或许底层的操作系统线程确实处在阻塞状态，但我们关心的是 JVM 的线程状态。JVM 把那些都视作资源，CPU 也好，硬盘，网卡也罢，有东西在为线程服务，它就认为线程在“执行”。如果 JVM 中的线程状态发生改变了，通常是自身机制引发的。 BLOCKED (阻塞) ：一个正在阻塞等待一个监视器锁的线程处于这一状态。 synchronize 机制有可能让线程进入 BLOCKED 状态。当因为获取不到锁而无法进入同步块时，线程处于 BLOCKED 状态。如果有线程长时间处于 BLOCKED 状态，要考虑是否发生了死锁（deadlock）的状况。BLOCKED 状态可以视作为一种特殊的 waiting，是传统 waiting 状态的一个细分。 WAITING (等待) ：一个正在无限期等待另一个线程执行一个特别的动作的线程处于这一状态。 wait / notify，join 方法进入 WATING 状态。其中 wait / notify 可用于协作关系：当条件不满足时调用 wait 释放锁，让补充条件的线程运行，但条件满足后由此补充线程唤醒，唤醒后原有线程处于阻塞队列等待取锁。wait(0)：无限等待。 TIMED_WAITING (计时等待) ：一个正在限时等待另一个线程执行一个动作的线程处于这一状态。 双保险：即使条件永远得不到满足，线程也会在规定时间后自我唤醒。调用 sleep 方法进入 TIMED_WAITING 状态，此状态不释放锁（与锁无关），只能等其自我唤醒。sleep(0)：几乎不等待。 TERMINATED (终止) ：一个已经退出的线程处于这一状态。 常用方法start 方法 start() 用来启动一个线程，当调用 start 方法后，系统才会开启一个新的线程来执行用户定义的子任务，在这个过程中，会为相应的线程分配需要的资源。 run 方法 run() 方法是不需要用户来调用的，当通过 start 方法启动一个线程之后，当线程获得了 CPU 执行时间，便进入 run 方法体去执行具体的任务。注意，继承 Thread 类必须重写 run 方法，在 run 方法中定义具体要执行的任务。 sleep 方法 sleep 方法相当于让线程睡眠，交出 CPU，让 CPU 去执行其他的任务。注意 sleep 方法与锁无关。 123456789101112131415161718192021222324252627282930313233public class Test &#123; private int i = 10; private Object object = new Object(); public static void main(String[] args) throws IOException &#123; Test test = new Test(); MyThread thread1 = test.new MyThread(); MyThread thread2 = test.new MyThread(); thread1.start(); thread2.start(); // 等待上一个线程运行完同步代码块才可运行 &#125; class MyThread extends Thread&#123; @Override public void run() &#123; synchronized (object) &#123; i++; System.out.println(\"i:\"+i); try &#123; System.out.println(\"线程\"+Thread.currentThread().getName()+\"进入睡眠状态\"); Thread.currentThread().sleep(10000); &#125; catch (InterruptedException e) &#123; // TODO: handle exception &#125; System.out.println(\"线程\"+Thread.currentThread().getName()+\"睡眠结束\"); i++; System.out.println(\"i:\"+i); &#125; &#125; &#125;&#125; yield 方法 调用 yield 方法会让当前线程交出 CPU 权限，让 CPU 去执行其他的线程。它跟 sleep 方法类似，同样不会释放锁。但是 yield 不能控制具体的交出 CPU 的时间，另外，yield 方法只能让拥有相同优先级的线程有获取 CPU 执行时间的机会。 注意，调用 yield 方法并不会让线程进入阻塞状态，而是让线程重回就绪状态，它只需要等待重新获取 CPU 执行时间，这一点是和 sleep 方法不一样的。 join 方法 假如在 main 线程中，调用 thread.join 方法，则 main 方法会等待 thread 线程执行完毕或者等待一定的时间。如果调用的是无参 join 方法，则等待 thread 执行完毕，如果调用的是指定了时间参数的 join 方法，则等待一定的时间。 123456789101112131415161718192021222324252627282930public class Test &#123; public static void main(String[] args) throws IOException &#123; System.out.println(\"进入线程\"+Thread.currentThread().getName()); Test test = new Test(); MyThread thread1 = test.new MyThread(); thread1.start(); try &#123; System.out.println(\"线程\"+Thread.currentThread().getName()+\"等待\"); thread1.join(); System.out.println(\"线程\"+Thread.currentThread().getName()+\"继续执行\"); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; class MyThread extends Thread&#123; @Override public void run() &#123; System.out.println(\"进入线程\"+Thread.currentThread().getName()); try &#123; Thread.currentThread().sleep(5000); &#125; catch (InterruptedException e) &#123; // TODO: handle exception &#125; System.out.println(\"线程\"+Thread.currentThread().getName()+\"执行完毕\"); &#125; &#125;&#125; interrupt 方法 单独调用 interrupt 方法可以使得处于阻塞状态的线程抛出一个异常，也就说，它可以用来中断一个正处于阻塞状态的线程；另外，通过可以通过 interrupt 方法和 isInterrupted 方法来停止正在运行的线程。 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) throws IOException &#123; Test test = new Test(); MyThread thread = test.new MyThread(); thread.start(); try &#123; Thread.currentThread().sleep(2000); // 此线程睡眠 2s &#125; catch (InterruptedException e) &#123; &#125; thread.interrupt(); // 阻塞状态中被打断，抛出异常 InterruptedException &#125; class MyThread extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"进入睡眠状态\"); Thread.currentThread().sleep(10000); System.out.println(\"睡眠完毕\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"得到中断异常\"); &#125; System.out.println(\"run方法执行完毕\"); &#125; &#125;&#125; 上面的例子表明了 interrupt 方法可以中断处于阻塞状态的线程，那么能不能中断处于非阻塞状态的线程呢？答案是不能。但是如果配合 isInterrupted 方法能够中断正在运行的线程，因为调用 interrupt 方法相当于将中断标志位置为 true，那么可以通过调用 isInterrupted 方法判断中断标志是否被置位来中断线程的执行。 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) throws IOException &#123; Test test = new Test(); MyThread thread = test.new MyThread(); thread.start(); try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; thread.interrupt(); &#125; class MyThread extends Thread&#123; @Override public void run() &#123; int i = 0; // 不推荐这么用，可自定义一个标识符用于退出循环 while(!isInterrupted() &amp;&amp; i&lt;Integer.MAX_VALUE)&#123; System.out.println(i+\" while循环\"); i++; &#125; &#125; &#125;&#125; 线程属性getId()：获取线程 ID；getName()、setName()：设置、获取线程名称；getPriority()、setPriority()：设置、获取优先级；setDaemon()、isDaemon()：设置、判断守护进程；Thread 类有一个比较常用的静态方法 currentThread() 用来获取当前线程。 守护线程和用户线程的区别在于：守护线程依赖于创建它的线程，而用户线程则不依赖。举个简单的例子：如果在 main 线程中创建了一个守护线程，当 main 方法运行完毕之后，守护线程也会随着消亡。而用户线程则不会，用户线程会一直运行直到其运行完毕。在 JVM 中，像垃圾收集器线程就是守护线程。 进程的创建 通过 Runtime.exec() 方法来； 通过 ProcessBuilder 的 start() 方法。 首先要讲的是 Process 类，Process 类是一个抽象类，在它里面主要有几个抽象的方法。 12345678910// java.lang.Processpublic abstract class Process &#123; abstract public OutputStream getOutputStream(); // 获取进程的输出流 abstract public InputStream getInputStream(); // 获取进程的输入流 abstract public InputStream getErrorStream(); // 获取进程的错误流 abstract public int waitFor() throws InterruptedException; // 让进程等待 abstract public int exitValue(); // 获取进程的退出标志 abstract public void destroy(); // 摧毁进程&#125; ProcessBuilderProcessBuilder 是一个 final 类，它有两个构造器。 123456789101112131415161718192021public final class ProcessBuilder &#123; private List&lt;String&gt; command; private File directory; private Map&lt;String,String&gt; environment; private boolean redirectErrorStream; // 传递创建进程需要的命令参数 public ProcessBuilder(List&lt;String&gt; command) &#123; if (command == null) throw new NullPointerException(); this.command = command; &#125; // 不定长字符串的形式 public ProcessBuilder(String... command) &#123; this.command = new ArrayList&lt;String&gt;(command.length); for (String arg : command) this.command.add(arg); &#125; // ....&#125; 通过 ProcessBuilder 来启动一个进程打开 cmd，并获取 ip 地址信息。 123456789101112public class Test &#123; public static void main(String[] args) throws IOException &#123; ProcessBuilder pb = new ProcessBuilder(\"cmd\",\"/c\",\"ipconfig/all\"); // 也可以是一个 List Process process = pb.start(); Scanner scanner = new Scanner(process.getInputStream()); while(scanner.hasNextLine())&#123; System.out.println(scanner.nextLine()); &#125; scanner.close(); &#125;&#125; Runtime.exec()由于任何进程只会运行于一个虚拟机实例当中，所以在Runtime中采用了单例模式，即只会产生一个虚拟机实例. 12345678910111213141516171819public class Runtime &#123; private static Runtime currentRuntime = new Runtime(); public static Runtime getRuntime() &#123; return currentRuntime; &#125; private Runtime() &#123;&#125; // ... public Process exec(String[] cmdarray, String[] envp, File dir) // 所有实现的最终实现 throws IOException &#123; return new ProcessBuilder(cmdarray) // 调用 ProcessBuilder 进行实现 .environment(envp) .directory(dir) .start(); &#125;&#125; 通过 Runtime.exec() 来启动一个进程打开 cmd，并获取 ip 地址信息。 123456789101112public class Test &#123; public static void main(String[] args) throws IOException &#123; String cmd = \"cmd \"+\"/c \"+\"ipconfig/all\"; // 不支持不定长参数，必须进行拼接 Process process = Runtime.getRuntime().exec(cmd); Scanner scanner = new Scanner(process.getInputStream()); while(scanner.hasNextLine())&#123; System.out.println(scanner.nextLine()); &#125; scanner.close(); &#125;&#125; synchronized &amp; Lock如果对临界资源加上互斥锁，当一个线程在访问该临界资源时，其他线程便只能等待。 在 Java 中，每一个对象都拥有一个锁标记（monitor），也称为监视器，多线程同时访问某个对象时，线程只有获取了该对象的锁才能访问。 synchronized在 Java 中，可以使用 synchronized 关键字来标记一个方法或者代码块，当某个线程调用该对象的 synchronized 方法或者访问 synchronized 代码块时，这个线程便获得了该对象的锁，其他线程暂时无法访问这个方法，只有等待这个方法执行完毕或者代码块执行完毕，这个线程才会释放该对象的锁，其他线程才能执行这个方法或者代码块。 synchronized 方法两个线程分别调用 insertData() 对象插入数据。（非同步） 1234567891011121314151617181920212223242526272829public class Test &#123; public static void main(String[] args) &#123; final InsertData insertData = new InsertData(); // 匿名内部类 new Thread() &#123; public void run() &#123; insertData.insert(Thread.currentThread()); &#125;; &#125;.start(); new Thread() &#123; public void run() &#123; insertData.insert(Thread.currentThread()); &#125;; &#125;.start(); &#125; &#125;class InsertData &#123; private ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); public void insert(Thread thread)&#123; for(int i=0;i&lt;5;i++)&#123; System.out.println(thread.getName()+\"在插入数据\"+i); arrayList.add(i); &#125; &#125;&#125; 输出结果表明两个线程在同时执行insert方法。而如果在insert方法前面加上关键字 synchronized。 12345678910class InsertData &#123; private ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); // 同步方法 public synchronized void insert(Thread thread)&#123; for(int i=0;i&lt;5;i++)&#123; System.out.println(thread.getName()+\"在插入数据\"+i); arrayList.add(i); &#125; &#125;&#125; 输出结果表明两线程顺序执行。 synchronized 代码块123456789101112131415class InsertData &#123; private ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); // private Object object = new Object(); public void insert(Thread thread)&#123; // 同步代码块 // this 代表获取当前对象的锁，也可以是类中的一个属性，代表获取该属性的锁。 // synchronized (object) &#123; synchronized (this) &#123; for(int i=0;i&lt;100;i++)&#123; System.out.println(thread.getName()+\"在插入数据\"+i); arrayList.add(i); &#125; &#125; &#125;&#125; 类锁 VS. 对象锁 另外，每个类也会有一个锁，它可以用来控制对 static 数据成员的并发访问。并且如果一个线程执行一个对象的非 static synchronized 方法，另外一个线程需要执行这个对象所属类的 static synchronized 方法，此时不会发生互斥现象，因为访问 static synchronized 方法占用的是类锁，而访问非 static synchronized 方法占用的是对象锁，所以不存在互斥现象。 Lock 接口Lock 不是 Java 语言内置的，synchronized 是 Java 语言的关键字，因此是内置特性。Lock 是一个类，通过这个类可以实现同步访问； Lock 和 synchronized 有一点非常大的不同，采用 synchronized 不需要用户去手动释放锁，当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占用；而 Lock 则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 12345678public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; 采用 Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用 Lock 必须在 try catch 块中进行，并且将释放锁的操作放在 finally 块中进行，以保证锁一定被被释放，防止死锁的发生。 123456789Lock lock = ...;lock.lock();try&#123; // 处理任务&#125;catch(Exception ex)&#123; &#125;finally&#123; lock.unlock(); // 释放锁&#125; tryLock 方法是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回 true，如果获取失败（即锁已被其他线程获取），则返回 false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit) 和 tryLock 方法是类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还拿不到锁，就返回 false。如果如果一开始拿到锁或者在等待期间内拿到了锁，则返回 true。 123456789101112Lock lock = ...;if(lock.tryLock()) &#123; try&#123; // 处理任务 &#125;catch(Exception ex)&#123; &#125;finally&#123; lock.unlock(); // 释放锁 &#125; &#125; else &#123; // 如果不能获取锁，则直接做其他事情&#125; lockInterruptibly 方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过 lock.lockInterruptibly() 想获取某个锁时，假若此时线程 A 获取到了锁，而线程B只有继续等待，那么对线程 B 调用 threadB.interrupt() 能够中断线程 B的等待过程。 123456789public void method() throws InterruptedException &#123; lock.lockInterruptibly(); try &#123; // ... &#125; finally &#123; lock.unlock(); &#125; &#125; ReentrantLock可重入锁的一种，后面详细介绍，先来看一下常用用法。 123456789101112131415161718192021222324252627282930313233// 错误案例public class Test &#123; public static void main(String[] args) &#123; final Test test = new Test(); new Thread()&#123; public void run() &#123; test.insert(Thread.currentThread()); &#125;; &#125;.start(); new Thread()&#123; public void run() &#123; test.insert(Thread.currentThread()); &#125;; &#125;.start(); &#125; public void insert(Thread thread) &#123; // 注意！！！这是一个局部变量 Lock lock = new ReentrantLock(); // 实例化一个 ReentrantLock 类型对象 // 每个线程执行到 lock.lock() 处获取的是不同的锁，此锁无效 lock.lock(); try &#123; System.out.println(thread.getName()+\"得到了锁\"); &#125; catch (Exception e) &#123; &#125;finally &#123; System.out.println(thread.getName()+\"释放了锁\"); lock.unlock(); &#125; &#125;&#125; 将 lock 声明为类的属性，即全局变量。 12345public class Test &#123; private Lock lock = new ReentrantLock(); // 全局变量 //...&#125; lockInterruptibly() 响应中断的使用方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Test &#123; // 全局变量 private Lock lock = new ReentrantLock(); public static void main(String[] args) &#123; Test test = new Test(); MyThread thread1 = new MyThread(test); MyThread thread2 = new MyThread(test); thread1.start(); thread2.start(); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; thread2.interrupt(); &#125; public void insert(Thread thread) throws InterruptedException&#123; lock.lockInterruptibly(); // 线程 1 获得锁 线程 2 处于等待，并且将被打断等待 try &#123; System.out.println(thread.getName()+\"得到了锁\"); long startTime = System.currentTimeMillis(); for( ; ;) &#123; if(System.currentTimeMillis() - startTime &gt;= 10000) break; // 插入数据 &#125; &#125; finally &#123; System.out.println(Thread.currentThread().getName()+\"执行finally\"); lock.unlock(); System.out.println(thread.getName()+\"释放了锁\"); &#125; &#125;&#125;class MyThread extends Thread &#123; private Test test = null; public MyThread(Test test) &#123; this.test = test; &#125; @Override public void run() &#123; try &#123; test.insert(Thread.currentThread()); &#125; catch (InterruptedException e) &#123; System.out.println(Thread.currentThread().getName()+\"被中断\"); &#125; &#125;&#125; ReadWriteLock 接口将文件的读写操作分开，分成 2 个锁来分配给线程，从而使得多个线程可以同时进行读操作。 1234567public interface ReadWriteLock &#123; // 读锁 Lock readLock(); // 写锁 Lock writeLock();&#125; ReentrantReadWriteLock 实现了 ReadWriteLock 接口。 12345678910111213141516171819202122232425262728293031323334public class Test &#123; private ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); public static void main(String[] args) &#123; final Test test = new Test(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); &#125;; &#125;.start(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); &#125;; &#125;.start(); &#125; public void get(Thread thread) &#123; rwl.readLock().lock(); try &#123; long start = System.currentTimeMillis(); while(System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName()+\"正在进行读操作\"); &#125; System.out.println(thread.getName()+\"读操作完毕\"); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125; 两线程同时进行读操作。 锁的种类介绍与锁相关的几个概念。 可重入锁如果说锁具备可重入性，那这个锁为可重入锁即基于线程的分配，而不是基于方法调用的分配。像 synchronized 和 ReentrantLock 都是可重入锁。 当一个线程执行到某个 synchronized 方法时，比如说 methodA，而在 methodA 中会调用另外一个 synchronized 方法 methodB，此时线程不必重新去申请锁，而是可以直接执行方法 methodB。因为可重入锁的分配是基于线程的，同一线程不必重新申请锁。 可中断锁在 Java 中，synchronized 就不是可中断锁，而 Lock 是可中断锁。 公平锁公平锁即尽量以请求锁的顺序来获取锁。比如同是有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会获得该所，这种就是公平锁。 非公平锁即无法保证锁的获取是按照请求锁的顺序进行的。这样就可能导致某个或者一些线程永远获取不到锁。 在 Java 中，synchronized 就是非公平锁，它无法保证等待的线程获取锁的顺序。而对于 ReentrantLock 和 ReentrantReadWriteLock，它默认情况下是非公平锁，但是可以设置为公平锁。 读写锁读写锁将对一个资源（比如文件）的访问分成了 2 个锁，一个读锁和一个写锁。正因为有了读写锁，才使得多个线程之间的读操作不会发生冲突。","categories":[{"name":"基础知识","slug":"基础知识","permalink":"http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"高并发编程","slug":"高并发编程","permalink":"http://yoursite.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"设计模式","slug":"技术原理/设计模式","date":"2020-03-20T02:21:09.000Z","updated":"2020-07-10T02:54:14.246Z","comments":true,"path":"2020/03/20/技术原理/设计模式/","link":"","permalink":"http://yoursite.com/2020/03/20/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"Java 设计模式。","text":"Java 设计模式。 设计模式设计模式的作用？ 找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起； 针对接口编程，而不是针对实现编程； 解耦。 七大原则 单一职责原则 接口隔离原则 依赖倒转（倒置）原则 里氏替换原则 开闭原则 迪米特法则 合成复用原则 单一职责此原则是对于类来说的，即一个类应该只负责一项职责。 123456789101112131415class RoadVehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \"公路运行\"); &#125;&#125;class AirVehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \"天空运行\"); &#125;&#125;class WaterVehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \"水中运行\"); &#125;&#125; 在方法级别上遵守单一职责 12345678910class Vehicle &#123; public void runRoad(String vehicle) &#123; System.out.println(vehicle + \" 在公路上运行...\"); &#125; public void runAir(String vehicle) &#123; System.out.println(vehicle + \" 在天空上运行...\"); &#125; public void runWater(String vehicle) &#123; System.out.println(vehicle + \" 在水中行...\"); &#125; 降低类的复杂度，一个类只负责一项职责； 提高类的可读性，可维护性； 降低变更引起的风险。 通常情况下，我们应当遵守单一职责原则，只有逻辑足够简单，才可以在代码级违反单一职责原则；只有类中方法数量足够少，可以在方法级别保持单一职责原则。 接口隔离即一个类对另一个类的依赖应该建立在最小的接口上。先看看不符合接口隔离原则的设计是怎么样的。 123456789101112131415161718192021222324252627282930313233// 接口有 5 个抽象方法，则每个实现了此接口的类都需要实现这 5 个抽象方法interface Interface1 &#123; void operation1(); void operation2(); void operation3(); void operation4(); void operation5();&#125;// Class B、D 省略class A &#123; //A 类通过接口 Interface1 依赖(使用) B 类，但是只会用到 1,2,3 方法 public void depend1(Interface1 i) &#123; i.operation1(); &#125; public void depend2(Interface1 i) &#123; i.operation2(); &#125; public void depend3(Interface1 i) &#123; i.operation3(); &#125;&#125;class C &#123; //C 类通过接口 Interface1 依赖(使用) D 类，但是只会用到 1,4,5 方法 public void depend1(Interface1 i) &#123; i.operation1(); &#125; public void depend4(Interface1 i) &#123; i.operation4(); &#125; public void depend5(Interface1 i) &#123; i.operation5(); &#125;&#125; 从上面的例子中可以看到，接口中的 5 个抽象方法 B、D 不管是否使用都要对其进行实现，这就造成了一定程度上的代码冗杂。接下来看看符合接口隔离原则的设计是怎么样的。 1234567891011121314151617181920212223interface Interface1 &#123; void operation1();&#125;interface Interface2 &#123; void operation2(); void operation3();&#125;interface Interface3 &#123; void operation4(); void operation5();&#125;// 测试public static void main(String[] args) &#123; A a = new A(); a.depend1(new B()); // A 类通过接口去依赖 B 类 a.depend2(new B()); a.depend3(new B()); C c = new C(); c.depend1(new D()); // C 类通过接口去依赖(使用) D 类 c.depend4(new D()); c.depend5(new D());&#125; 依赖倒转依赖倒转原则是基于这样的设计理念：相对于细节的多变性，抽象的东西要稳定的多。以抽象为基础搭建的架构比以细节为基础的架构要稳定的多。在 Java 中，抽象指的是接口或抽象类，细节就是具体的实现类，使用接口或抽象类的目的是制定好规范，而不涉及任何具体的操作，把展现细节的任务交给他们的实现类去完成： 高层模块不应该依赖低层模块，二者都应该依赖其抽象； 抽象不应该依赖细节，细节应该依赖抽象； 依赖倒转（倒置）的中心思想是面向接口编程； 123456789101112131415161718192021222324252627282930public class DependecyInversion &#123; public static void main(String[] args) &#123; // 客户端无需改变 Person person = new Person(); person.receive(new Email()); person.receive(new WeiXin()); &#125;&#125;class Person &#123; // 这里我们是对接口的依赖，不需要考虑接口的具体实现 public void receive(IReceiver receiver) &#123; System.out.println(receiver.getInfo()); &#125;&#125;// 定义接口interface IReceiver &#123; public String getInfo();&#125;// 实现类进行是实现class Email implements IReceiver &#123; public String getInfo() &#123; return \"电子邮件信息: hello,world\"; &#125;&#125;// 实现类进行实现class WeiXin implements IReceiver &#123; public String getInfo() &#123; return \"微信信息: hello,ok\"; &#125;&#125; 依赖传递 接口传递； 构造方法传递； Setter 方法传递。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 方式一：通过接口传递实现依赖interface IOpenAndClose &#123; public void open(ITV tv); // 抽象方法，接收接口实现以来传递&#125;interface ITV &#123; public void play();&#125;class OpenAndClose implements IOpenAndClose&#123; public void open(ITV tv)&#123; tv.play(); &#125;&#125;class ChangHong implements ITV &#123; @Override public void play() &#123; System.out.println(\"长虹电视机，打开\"); &#125;&#125;// 方式二：通过构造方法依赖interface IOpenAndClose &#123; public void open();&#125;interface ITV &#123; public void play();&#125;class OpenAndClose implements IOpenAndClose&#123; private ITV tv; // 成员 public OpenAndClose(ITV tv)&#123; // 构造器 this.tv = tv; &#125; public void open()&#123; this.tv.play(); &#125;&#125;// 方式三：通过 Setter 方法传递interface IOpenAndClose &#123; public void open(); public void setTv(ITV tv); // Setter 方法&#125;interface ITV &#123; public void play();&#125;class OpenAndClose implements IOpenAndClose &#123; private ITV tv; public void setTv(ITV tv) &#123; this.tv = tv; &#125; public void open() &#123; this.tv.play(); &#125;&#125; 里氏替换继承在给程序设计带来便利的同时，也带来了弊端。比如使用继承会给程序带来侵入性、程序的可移植性降低、增加对象间的耦合性等问题。如果一个类被其他的类所继承，则当这个类需要修改时，必须考虑到所有的子类，并且父类修改后，所有涉及到子类的功能都有可能产生故障。 用于解决如何正确使用继承？ 👉 里氏替换原则。 所有引用基类的地方必须能透明地使用其子类的对象；在子类中尽量不要重写父类的方法；继承实际上让两个类耦合性增强了，在适当的情况下可以通过聚合、组合、依赖来解决问题。 1234567891011121314151617181920212223242526272829public class Liskov &#123; public static void main(String[] args) &#123; A a = new A(); System.out.println(\"11-3=\" + a.func1(11, 3)); System.out.println(\"1-8=\" + a.func1(1, 8)); System.out.println(\"-----------------------\"); B b = new B(); System.out.println(\"11-3=\" + b.func1(11, 3)); // 这里本意是求出 11-3 System.out.println(\"1-8=\" + b.func1(1, 8)); // 1-8 System.out.println(\"11+3+9=\" + b.func2(11, 3)); &#125;&#125;class A &#123; // 返回两个数的差 public int func1(int num1, int num2) &#123; return num1 - num2; &#125;&#125;// B 类继承了 A 并且增加了一个新功能：完成两个数相加，然后和 9 求和class B extends A &#123; // 这里重写了 A 类的方法, 造成原有方法错误 public int func1(int a, int b) &#123; return a + b; &#125; public int func2(int a, int b) &#123; return func1(a, b) + 9; &#125;&#125; 改进方案：原来的父类和子类都继承一个更通俗的基类，原有的继承关系去掉，采用依赖、聚合、组合等关系代替。 123456789101112131415class B extends Base &#123; // 如果 B 需要使用 A 类的方法，使用组合关系 private A a = new A(); // 此方法不再是重写方法 public int func1(int a, int b) &#123; return a + b; &#125; public int func2(int a, int b) &#123; return func1(a, b) + 9; &#125; // 使用 A 类的方法 public int func3(int a, int b) &#123; return this.a.func1(a, b); &#125;&#125; 开闭原则类，模块和函数应该对扩展开放（对提供方），对修改关闭（对使用方）。用抽象构建框架，用实现扩展细节；当软件需要变化时，尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Ocp &#123; public static void main(String[] args) &#123; GraphicEditor graphicEditor = new GraphicEditor(); graphicEditor.drawShape(new Rectangle()); graphicEditor.drawShape(new Circle()); &#125;&#125;// 此写法当发生扩展时，比如添加可以绘制的图形类型，需要修改使用方// 这是一个用于绘图的类，此类为使用方class GraphicEditor &#123; // 接收 Shape 对象，然后根据 type，来绘制不同的图形 public void drawShape(Shape s) &#123; if (s.m_type == 1) drawRectangle(); else if (s.m_type == 2) drawCircle(); else if (s.m_type == 3) drawTriangle(); &#125; // 绘制矩形 public void drawRectangle() &#123; System.out.println(\" 绘制矩形 \"); &#125; // 绘制圆形 public void drawCircle() &#123; System.out.println(\" 绘制圆形 \"); &#125; // 绘制三角形 public void drawTriangle() &#123; System.out.println(\" 绘制三角形 \"); &#125;&#125;// Shape 类，基类class Shape &#123; int m_type;&#125;class Rectangle extends Shape &#123; Rectangle() &#123; super.m_type = 1; &#125;&#125;class Circle extends Shape &#123; Circle() &#123; super.m_type = 2; &#125;&#125; 这种代码的编写方式违反了设计模式的 OCP 原则，即对扩展开放(提供方)，对修改关闭(使用方)。即当我们给类增加新功能的时候，尽量不修改代码，或者尽可能少修改代码。并且，就单一职责而言，使用方类的职责就只有使用而已，至于如何去进行实现这些细节不应该在此类中出现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Ocp &#123; public static void main(String[] args) &#123; GraphicEditor graphicEditor = new GraphicEditor(); graphicEditor.drawShape(new Rectangle()); graphicEditor.drawShape(new Circle()); graphicEditor.drawShape(new Triangle()); graphicEditor.drawShape(new OtherGraphic()); &#125;&#125;class GraphicEditor &#123; // 仅仅是使用 draw() 方法 public void drawShape(Shape s) &#123; s.draw(); &#125;&#125;abstract class Shape &#123; int m_type; public abstract void draw(); //抽象方法&#125;class Rectangle extends Shape &#123; Rectangle() &#123; super.m_type = 1; &#125; @Override public void draw() &#123; System.out.println(\" 绘制矩形 \"); &#125;&#125;class Circle extends Shape &#123; Circle() &#123; super.m_type = 2; &#125; @Override public void draw() &#123; System.out.println(\" 绘制圆形 \"); &#125;&#125;//新增画三角形class Triangle extends Shape &#123; Triangle() &#123; super.m_type = 3; &#125; @Override public void draw() &#123; System.out.println(\" 绘制三角形 \"); &#125;&#125;//新增一个图形class OtherGraphic extends Shape &#123; OtherGraphic() &#123; super.m_type = 4; &#125; @Override public void draw() &#123; System.out.println(\" 绘制其它图形 \"); &#125;&#125; 迪米特法则一个对象应该对其他对象保持最少的了解；类与类关系越密切，耦合度越大。 迪米特法则（Demeter Principle）又叫最少知道原则，即一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类不管多么复杂，都尽量将逻辑封装在类的内部。对外除了提供的 public 方法，不对外泄露任何信息。 迪米特法则还有个更简单的定义：只与直接的朋友通信。直接的朋友：每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系。耦合的方式很多，依赖，关联，组合，聚合等。其中，我们称出现成员变量，方法参数，方法返回值中的类为直接的朋友，而出现在局部变量中的类不是直接的朋友。也就是说，陌生的类最好不要以局部变量的形式出现在类的内部。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class Demeter &#123; public static void main(String[] args) &#123; // 创建了一个 SchoolManager 对象 SchoolManager schoolManager = new SchoolManager(); // 输出学院的员工 id 和学校总部的员工信息 schoolManager.printAllEmployee(new CollegeManager()); &#125;&#125;// 学校总部员工类class Employee &#123; private String id; public void setId(String id) &#123; this.id = id; &#125; public String getId() &#123; return id; &#125;&#125;// 学院的员工类class CollegeEmployee &#123; private String id; public void setId(String id) &#123; this.id = id; &#125; public String getId() &#123; return id; &#125;&#125;// 管理学院员工的管理类class CollegeManager &#123; // 返回学院的所有员工 public List&lt;CollegeEmployee&gt; getAllEmployee() &#123; List&lt;CollegeEmployee&gt; list = new ArrayList&lt;CollegeEmployee&gt;(); for (int i = 0; i &lt; 10; i++) &#123; //这里我们增加了 10 个员工到 list CollegeEmployee emp = new CollegeEmployee(); emp.setId(\"学院员工 id= \" + i); list.add(emp); &#125; return list; &#125;&#125;// 学校管理类// 分析 SchoolManager 类的直接朋友类有哪些 Employee、CollegeManagerclass SchoolManager &#123; // 返回学校总部的员工 public List&lt;Employee&gt; getAllEmployee() &#123; List&lt;Employee&gt; list = new ArrayList&lt;Employee&gt;(); for (int i = 0; i &lt; 5; i++) &#123; // 这里我们增加了 5 个员工到 list Employee emp = new Employee(); emp.setId(\"学校总部员工 id= \" + i); list.add(emp); &#125; return list; &#125; // 该方法完成输出学校总部和学院员工信息(id) void printAllEmployee(CollegeManager sub) &#123; // 获取到学院员工 CollegeEmployee 不是直接朋友而是一个陌生类，这样违背了迪米特法则 List&lt;CollegeEmployee&gt; list1 = sub.getAllEmployee(); System.out.println(\"------------学院员工------------\"); for (CollegeEmployee e : list1) &#123; System.out.println(e.getId()); &#125; // 获取到学校总部员工 List&lt;Employee&gt; list2 = this.getAllEmployee(); System.out.println(\"------------学校总部员工------------\"); for (Employee e : list2) &#123; System.out.println(e.getId()); &#125; &#125;&#125; 简单来讲，也就是学校的总部只管理总部的员工，其分部的员工对它而言是个“陌生类”，分布员工的管理工作（打印）应该交给分部进行。 12345678910111213141516171819class CollegeManager &#123; public List&lt;CollegeEmployee&gt; getAllEmployee() &#123; List&lt;CollegeEmployee&gt; list = new ArrayList&lt;CollegeEmployee&gt;(); for (int i = 0; i &lt; 10; i++) &#123; CollegeEmployee emp = new CollegeEmployee(); emp.setId(\"学院员工 id= \" + i); list.add(emp); &#125; return list; &#125; // 学院的雇员由学院负责 public void printEmployee() &#123; List&lt;CollegeEmployee&gt; list1 = getAllEmployee(); System.out.println(\"------------学院员工------------\"); for (CollegeEmployee e : list1) &#123; System.out.println(e.getId()); &#125; &#125;&#125; 合成复用尽量使用合成 / 聚合的方式，而不是使用继承。 UML 类图依赖元素 A 的变化会影响元素 B，但反之不成立，那么 B 和 A 的关系是依赖关系，B 依赖 A；类属关系和实现关系在语义上讲也是依赖关系，但由于其有更特殊的用途，所以被单独描述。UML 中用带箭头的虚线表示 Dependency 关系，箭头指向被依赖元素。 泛化通常所说的继承（B is kind of A）关系，UML 中用带空心箭头的实线表示 Generalization 关系，箭头指向 A。 实现元素 A 定义一个约定，元素 B 实现这个约定，则 B 和 A 的关系是 Realize，B realize A。这个关系最常用于接口。UML 中用空心箭头和虚线表示 Realize 关系，箭头指向定义约定的元素。 关联元素间的结构化关系，是一种弱关系，被关联的元素间通常可以被独立的考虑。UML 中用带箭头实线表示 Association 关系，箭头指向被依赖元素。 聚合聚合关系的一种特例，表示部分和整体（All has a Part）的关系。UML 中用带空心菱形头的实线表示 Aggregation 关系，菱形头指向整体。 组合组合是聚合关系的变种，表示元素间更强的组合关系。如果是组合关系，如果整体被破坏则个体一定会被破坏，而聚合的个体则可能是被多个整体所共享的，不一定会随着某个整体的破坏而被破坏。UML 中用带实心菱形头的实线表示 Composition 关系，菱形头指向整体。 创建型模式单例模式饿汉式静态变量123456789101112//饿汉式：静态变量class Singleton &#123; // 构造器私有化, 外部不能 new private Singleton() &#123; &#125; // 本类内部创建对象实例 private final static Singleton instance = new Singleton(); // 提供一个公有的静态方法，返回实例对象 public static Singleton getInstance() &#123; return instance; &#125;&#125; 静态代码块1234567891011121314// 饿汉式：静态代码块class Singleton &#123; private Singleton() &#123; &#125; // 本类内部创建对象实例 private static Singleton instance; static &#123; // 在静态代码块中，创建单例对象 instance = new Singleton(); &#125; // 提供一个公有的静态方法，返回实例对象 public static Singleton getInstance() &#123; return instance; &#125;&#125; 懒汉式同步方法12345678910111213// 懒汉式：线程安全，同步方法class Singleton &#123; private static Singleton instance; private Singleton() &#123;&#125; // 提供一个静态的公有方法，加入同步处理的代码，解决线程安全问题 // 每一个线程获取实例时都要进到同步方法中进行判读，大大降低了效率 public static synchronized Singleton getInstance() &#123; if(instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 双重检查12345678910111213141516// 懒汉式：双重检查class Singleton &#123; private static Singleton instance; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if(instance == null) &#123; // 一次只有一个线程可进入此同步代码块中 synchronized (Singleton.class) &#123; if(instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 静态内部类静态内部类方式在 Singleton 类被装载时并不会立即实例化，而是在需要实例化时，调用 getInstance 方法，才会装载 SingletonInstance 类，从而完成 Singleton 的实例化。 类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM 帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。 1234567891011121314// 懒汉式：静态内部类class Singleton &#123; private static volatile Singleton instance; // 构造器私有化 private Singleton() &#123;&#125; // 写一个静态内部类，该类中有一个静态属性 Singleton private static class SingletonInstance &#123; private static final Singleton INSTANCE = new Singleton(); &#125; // 提供一个静态的公有方法，直接返回 SingletonInstance.INSTANCE public static synchronized Singleton getInstance() &#123; return SingletonInstance.INSTANCE; &#125;&#125; 枚举12345678// 懒汉式：枚举enum Singleton &#123; INSTANCE; //属性 public void sayOK() &#123; System.out.println(\"ok~\"); &#125;&#125;// 不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象 总结单例模式保证了系统内存中该类只存在一个对象，节省了系统资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能；当想实例化一个单例类的时候，必须要记住使用相应的获取对象的方法，而不是使用 new； 单例模式使用的场景：需要频繁的进行创建和销毁的对象、创建对象时耗时过多或耗费资源过多（即：重量级对象），但又经常用到的对象、工具类对象、频繁访问数据库或文件的对象（比如数据源、session 工厂等）。 工厂模式简单工厂简单工厂模式是属于创建型模式，是工厂模式的一种。简单工厂模式是由一个工厂对象决定创建出哪一种产品类的实例。 简单工厂模式是工厂模式家族中最简单实用的模式。在软件开发中，当我们会用到大量的创建某种、某类或者某批对象时，就会使用到工厂模式。 简单工厂模式：定义了一个创建对象的类，由这个类来封装实例化对象的行为（代码）。 1234567891011121314151617181920212223242526272829303132333435363738// 此类为使用者，当提供方进行扩展时需要进行比较大的代码修改，违反 OCP 原则public OrderPizza() &#123; Pizza pizza = null; String orderType; // 订购披萨的类型 do &#123; orderType = getType(); if (orderType.equals(\"greek\")) &#123; pizza = new GreekPizza(); pizza.setName(\" 希腊披萨 \"); &#125; else if (orderType.equals(\"cheese\")) &#123; pizza = new CheesePizza(); pizza.setName(\" 奶酪披萨 \"); &#125; else if (orderType.equals(\"pepper\")) &#123; pizza = new PepperPizza(); pizza.setName(\"胡椒披萨\"); &#125; else &#123; break; &#125; // 输出 pizza 制作过程 pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); &#125; while (true);&#125;// 写一个方法，可以获取客户希望订购的披萨种类private String getType() &#123; try &#123; BufferedReader strin = new BufferedReader(new InputStreamReader(System.in)); System.out.println(\"input pizza 种类:\"); String str = strin.readLine(); return str; &#125; catch (IOException e) &#123; e.printStackTrace(); return \"\"; &#125;&#125; 如果我们增加一个 Pizza 类，只要是订购 Pizza 的代码都需要修改。当然修改代码是可以接受，但是如果我们在其它的地方也有创建 Pizza 的代码，就意味着，也需要修改，而创建 Pizza 的代码，往往有多处。 简单工厂模式的设计方案：定义一个可以实例化 Pizaa 对象的类，封装创建对象的代码。当所有的对象的实例化都由一个工厂进行负责时，新增一个 Pizza 类只用修改工厂类即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 简单工厂类public class SimpleFactory &#123; // 添加 orderType 返回对应的 Pizza 对象 public Pizza createPizza(String orderType) &#123; // 根据需求还可以是静态方法 Pizza pizza = null; System.out.println(\"简单工厂模式\"); if (orderType.equals(\"greek\")) &#123; pizza = new GreekPizza(); pizza.setName(\" 希腊披萨 \"); &#125; else if (orderType.equals(\"cheese\")) &#123; pizza = new CheesePizza(); pizza.setName(\" 奶酪披萨 \"); &#125; else if (orderType.equals(\"pepper\")) &#123; pizza = new PepperPizza(); pizza.setName(\"胡椒披萨\"); &#125; return pizza; &#125;&#125;// 使用者public OrderPizza() &#123; // 定义一个简单工厂对象 SimpleFactory simpleFactory; Pizza pizza = null; // 构造器 public OrderPizza(SimpleFactory simpleFactory) &#123; setFactory(simpleFactory); &#125; public void setFactory(SimpleFactory simpleFactory) &#123; String orderType = \"\"; // 用户输入的 this.simpleFactory = simpleFactory; // 设置简单工厂对象 do &#123; orderType = getType(); pizza = this.simpleFactory.createPizza(orderType); // 输出 pizza if(pizza != null) &#123; pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); &#125; else &#123; System.out.println(\" 订购披萨失败 \"); break; &#125; &#125;while(true); &#125;&#125; 工厂方法工厂方法模式：定义了一个创建对象的抽象方法，由子类决定要实例化的对象。工厂方法模式将对象的实例化推迟到子类。 简单工厂模式需要一个工厂进行操作，当更多的不同的需求时，可以通过创建多个简单工厂类进行实现；工厂方法模式直接把会变化的方法抽象到子工厂中去实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243public abstract class OrderPizza &#123; // 定义一个抽象方法，createPizza , 让各个工厂子类自己实现 abstract Pizza createPizza(String orderType); // 构造器 public OrderPizza() &#123; Pizza pizza = null; String orderType; // 订购披萨的类型 do &#123; orderType = getType(); pizza = createPizza(orderType); // 抽象方法，由工厂子类完成 // 输出 pizza 制作过程 pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); &#125; while (true); &#125;&#125;// 子工厂public class BJOrderPizza extends OrderPizza &#123; @Override Pizza createPizza(String orderType) &#123; Pizza pizza = null; if(orderType.equals(\"cheese\")) &#123; pizza = new BJCheesePizza(); &#125; else if (orderType.equals(\"pepper\")) &#123; pizza = new BJPepperPizza(); &#125; return pizza; &#125;&#125;public class LDOrderPizza extends OrderPizza &#123; Pizza createPizza(String orderType) &#123; Pizza pizza = null; if(orderType.equals(\"cheese\")) &#123; pizza = new LDCheesePizza(); &#125; else if (orderType.equals(\"pepper\")) &#123; pizza = new LDPepperPizza(); &#125; return pizza; &#125;&#125; 抽象工厂抽象工厂模式：定义了一个 interface 用于创建相关或有依赖关系的对象簇，而无需指明具体的类。 抽象工厂模式可以将简单工厂模式和工厂方法模式进行整合。从设计层面看，抽象工厂模式就是对简单工厂模式的改进。（或者称为进一步的抽象）将工厂抽象成两层，AbsFactory（抽象工厂）和具体实现的工厂子类。程序员可以根据创建对象类型使用对应的工厂子类。这样将单个的简单工厂类变成了工厂簇，更利于代码的维护和扩展。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 一个抽象工厂模式的抽象层：接口public interface AbsFactory &#123; // 让下面的工厂子类来具体实现 public Pizza createPizza(String orderType);&#125;public class BJFactory implements AbsFactory &#123; @Override public Pizza createPizza(String orderType) &#123; System.out.println(\"抽象工厂模式\"); Pizza pizza = null; if(orderType.equals(\"cheese\")) &#123; pizza = new BJCheesePizza(); &#125; else if (orderType.equals(\"pepper\"))&#123; pizza = new BJPepperPizza(); &#125; return pizza; &#125;&#125;public class LDFactory implements AbsFactory &#123; @Override public Pizza createPizza(String orderType) &#123; System.out.println(\"抽象工厂模式\"); Pizza pizza = null; if (orderType.equals(\"cheese\")) &#123; pizza = new LDCheesePizza(); &#125; else if (orderType.equals(\"pepper\")) &#123; pizza = new LDPepperPizza(); &#125; return pizza; &#125;&#125;public class OrderPizza &#123; AbsFactory factory; public OrderPizza(AbsFactory factory) &#123; setFactory(factory); &#125; private void setFactory(AbsFactory factory) &#123; Pizza pizza = null; String orderType = \"\"; this.factory = factory; do &#123; orderType = getType(); // factory 可能是北京的工厂子类，也可能是伦敦的工厂子类 pizza = factory.createPizza(orderType); if (pizza != null) &#123; pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); &#125; else &#123; System.out.println(\"订购失败\"); break; &#125; &#125; while (true); &#125;&#125; 总结工厂模式的意义：将实例化对象的代码提取出来，放到一个类中统一管理和维护，达到和主项目的依赖关系的解耦。从而提高项目的扩展和维护性。 设计模式的依赖抽象原则： 创建对象实例时，不要直接 new 类, 而是把这个 new 类的动作放在一个工厂的方法中，并返回； 不要让类继承具体类，而是继承抽象类或者是实现 interface； 不要覆盖基类中已经实现的方法。 原型模式原型模式（Prototype 模式）是指：用原型实例指定创建对象的种类，并且通过拷贝这些原型，创建新的对象； 原型模式是一种创建型设计模式，允许一个对象再创建另外一个可定制的对象，无需知道如何创建的细节； 工作原理：通过将一个原型对象传给那个要发动创建的对象，这个要发动创建的对象通过请求原型对象拷贝它们自己来实施创建，即 Object.clone() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Sheep implements Cloneable &#123; private String name; private int age; private String color; private String address = \"蒙古羊\"; public Sheep friend; // 克隆如何处理引用对象? 默认是浅拷贝 public Sheep(String name, int age, String color) &#123; super(); this.name = name; this.age = age; this.color = color; &#125; // Getter / Setter @Override public String toString() &#123; return \"Sheep [name=\" + name + \", age=\" + age + \", color=\" + color + \", address=\" + address + \"]\"; &#125; // 克隆该实例，使用默认的 clone 方法来完成 @Override protected Object clone() &#123; Sheep sheep = null; try &#123; sheep = (Sheep)super.clone(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; return sheep; &#125;&#125;public class Client &#123; public static void main(String[] args) &#123; System.out.println(\"原型模式完成对象的创建\"); Sheep sheep = new Sheep(\"tom\", 1, \"白色\"); sheep.friend = new Sheep(\"jack\", 2, \"黑色\"); Sheep sheep2 = (Sheep)sheep.clone(); //克隆 Sheep sheep3 = (Sheep)sheep.clone(); //克隆 Sheep sheep4 = (Sheep)sheep.clone(); //克隆 Sheep sheep5 = (Sheep)sheep.clone(); //克隆 System.out.println(\"sheep2 =\" + sheep2 + \"sheep2.friend=\" + sheep2.friend.hashCode()); System.out.println(\"sheep3 =\" + sheep3 + \"sheep3.friend=\" + sheep3.friend.hashCode()); System.out.println(\"sheep4 =\" + sheep4 + \"sheep4.friend=\" + sheep4.friend.hashCode()); System.out.println(\"sheep5 =\" + sheep5 + \"sheep5.friend=\" + sheep5.friend.hashCode()); &#125;&#125; 浅拷贝对于数据类型是基本数据类型的成员变量，浅拷贝会直接进行值传递，也就是将该属性值复制一份给新的对象； 对于数据类型是引用数据类型的成员变量，比如说成员变量是某个数组、某个类的对象等，那么浅拷贝会进行引用传递，也就是只是将该成员变量的引用值（内存地址）复制一份给新的对象。因为实际上两个对象的该成员变量都指向同一个实例。在这种情况下，在一个对象中修改该成员变量会影响到另一个对象的该成员变量值。 1Object.clone(); // 默认为浅拷贝 深拷贝复制对象的所有基本数据类型的成员变量值； 为所有引用数据类型的成员变量申请存储空间，并复制每个引用数据类型成员变量所引用的对象，直到该对象可达的所有对象。也就是说，对象进行深拷贝要对整个对象（包括对象的引用类型）进行拷贝。 重写 clone 方法123456789101112131415161718192021222324252627282930313233public class DeepCloneableTarget implements Serializable, Cloneable &#123; private static final long serialVersionUID = 1L; private String cloneName; private String cloneClass; // 构造器 public DeepCloneableTarget(String cloneName, String cloneClass) &#123; this.cloneName = cloneName; this.cloneClass = cloneClass; &#125; // 因为该类的属性，都是 String , 因此我们这里使用默认的 clone 完成即可 @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125;public class DeepProtoType implements Serializable, Cloneable&#123; public String name; // String 属性 public DeepCloneableTarget deepCloneableTarget; // 引用类型 public DeepProtoType() &#123; super(); &#125; // 深拷贝：使用 clone 方法 @Override protected Object clone() throws CloneNotSupportedException &#123; Object deep = null; // 这里完成对基本数据类型（属性）和 String 的克隆 deep = super.clone(); // 对引用类型的属性，进行单独处理 DeepProtoType deepProtoType = (DeepProtoType)deep; deepProtoType.deepCloneableTarget = (DeepCloneableTarget)deepCloneableTarget.clone(); return deepProtoType; &#125;&#125; 对象序列化实现使用序列化实现对象的深拷贝。 12345678910111213141516171819202122232425262728293031public Object deepClone() &#123; // 创建流对象 ByteArrayOutputStream bos = null; ObjectOutputStream oos = null; ByteArrayInputStream bis = null; ObjectInputStream ois = null; try &#123; // 序列化 bos = new ByteArrayOutputStream(); oos = new ObjectOutputStream(bos); oos.writeObject(this); // 当前这个对象以对象流的方式输出 // 反序列化 bis = new ByteArrayInputStream(bos.toByteArray()); ois = new ObjectInputStream(bis); DeepProtoType copyObj = (DeepProtoType)ois.readObject(); return copyObj; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; finally &#123; // 关闭流 try &#123; bos.close(); oos.close(); bis.close(); ois.close(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 总结创建新的对象比较复杂时，可以利用原型模式简化对象的创建过程，同时也能够提高效率； 不用重新初始化对象，而是动态地获得对象运行时的状态； 如果原始对象发生变化（增加或者减少属性），其它克隆对象的也会发生相应的变化，无需修改代码； 在实现深克隆的时候可能需要比较复杂的代码； 缺点：需要为每一个类配备一个克隆方法，这对全新的类来说不是很难，但对已有的类进行改造时，需要修改其源代码，违背了 ocp 原则。 建造者模式建造者模式（Builder Pattern）又叫生成器模式，是一种对象构建模式。它可以将复杂对象的建造过程抽象出来（抽象类别），使这个抽象过程的不同实现方法可以构造出不同表现（属性）的对象。 建造者模式是一步一步创建一个复杂的对象，它允许用户只通过指定复杂对象的类型和内容就可以构建它们，用户不需要知道内部的具体构建细节。 角色： Product（产品角色）： 一个具体的产品对象； Builder（抽象建造者）： 创建一个 Product 对象的各个部件指定的 接口 / 抽象类； ConcreteBuilder（具体建造者）： 实现接口，构建和装配各个部件； Director（指挥者）： 构建一个使用 Builder 接口的对象。它主要是用于创建一个复杂的对象。它主要有两个作用： 隔离了客户与对象的生产过程； 负责控制产品对象的生产过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class Client &#123; public static void main(String[] args) &#123; // 盖普通房子 CommonHouse commonHouse = new CommonHouse(); // 准备创建房子的指挥者 HouseDirector houseDirector = new HouseDirector(commonHouse); // 完成盖房子，返回产品（普通房子） House house = houseDirector.constructHouse(); System.out.println(\"--------------------------\"); //盖高楼 HighBuilding highBuilding = new HighBuilding(); //重置建造者 houseDirector.setHouseBuilder(highBuilding); //完成盖房子，返回产品（高楼） houseDirector.constructHouse(); &#125;&#125;public class CommonHouse extends HouseBuilder &#123; @Override public void buildBasic() &#123; System.out.println(\" 普通房子打地基 5 米 \"); &#125; @Override public void buildWalls() &#123; System.out.println(\" 普通房子砌墙 10cm \"); &#125; @Override public void roofed() &#123; System.out.println(\" 普通房子屋顶 \"); &#125;&#125;public class HighBuilding extends HouseBuilder &#123; @Override public void buildBasic() &#123; System.out.println(\" 高楼的打地基 100 米 \"); &#125; @Override public void buildWalls() &#123; System.out.println(\" 高楼的砌墙 20cm \"); &#125; @Override public void roofed() &#123; System.out.println(\" 高楼的透明屋顶 \"); &#125;&#125;public class House &#123; private String baise; private String wall; private String roofed; // Getter / Setter&#125;// 抽象的建造者public abstract class HouseBuilder &#123; protected House house = new House(); // 将建造的流程写好, 抽象的方法 public abstract void buildBasic(); public abstract void buildWalls(); public abstract void roofed(); // 建造房子好， 将产品返回 public House buildHouse() &#123; return house; &#125;&#125;public class HouseDirector &#123; HouseBuilder houseBuilder = null; // 构造器传入 houseBuilder public HouseDirector(HouseBuilder houseBuilder) &#123; this.houseBuilder = houseBuilder; &#125; // 如何处理建造房子的流程，交给指挥者 public House constructHouse() &#123; houseBuilder.buildBasic(); houseBuilder.buildWalls(); houseBuilder.roofed(); return houseBuilder.buildHouse(); &#125;&#125; Factory VS. Builder抽象工厂模式实现对产品家族的创建，一个产品家族是这样的一系列产品：具有不同分类维度的产品组合，采用抽象工厂模式不需要关心构建过程，只关心什么产品由什么工厂生产即可。 建造者模式则是要求按照指定的蓝图建造产品，可以更加精细地控制产品的创建过程 。将复杂产品的创建步骤分解在不同的方法中，使得创建过程更加清晰，也更方便使用程序来控制创建过程它的主要目的是通过组装零配件而产生一个新品；建造者模式所创建的产品一般具有较多的共同点，其组成部分相似，如果产品之间的差异性很大，则不适合使用建造者模式，因此其使用范围受到一定的限制。 结构型模式适配器模式适配器模式（Adapter Pattern）将某个类的接口转换成客户端期望的另一个接口表示，主的目的是兼容性，让原本因接口不匹配不能一起工作的两个类可以协同工作，其别名为包装器（Wrapper）。 种类：类 / 对象 / 接口适配器模式。 类适配器 123456789101112131415161718192021222324252627282930313233343536373839404142public class Client &#123; public static void main(String[] args) &#123; System.out.println(\" === 类适配器模式 ===\"); Phone phone = new Phone(); phone.charging(new VoltageAdapter()); &#125;&#125;// 适配接口public interface IVoltage5V &#123; public int output5V();&#125;public class Phone &#123; // 充电 public void charging(IVoltage5V iVoltage5V) &#123; if(iVoltage5V.output5V() == 5) &#123; System.out.println(\"电压为 5V, 可以充电\"); &#125; else if (iVoltage5V.output5V() &gt; 5) &#123; System.out.println(\"电压大于 5V, 不能充电\"); &#125; &#125;&#125;// 被适配的类public class Voltage220V &#123; // 输出 220V 的电压 public int output220V() &#123; int src = 220; System.out.println(\"电压=\" + src + \"伏\"); return src; &#125;&#125;// 适配器，Java 单继承机制，所以目标类必须是一个接口public class VoltageAdapter extends Voltage220V implements IVoltage5V &#123; @Override public int output5V() &#123; // 获取到 220V 电压 int srcV = output220V(); int dstV = srcV / 44 ; // 转成 5v return dstV; &#125;&#125; 对象适配器基本思路和类的适配器模式相同，只是将 Adapter 类作修改，不是继承 resource 类，而是持有其的实例，以解决兼容性的问题。即根据合成复用原则，在系统中尽量使用关联关系（聚合）来替代继承关系。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Client &#123; public static void main(String[] args) &#123; System.out.println(\" === 对象适配器模式 ===\"); Phone phone = new Phone(); phone.charging(new VoltageAdapter(new Voltage220V())); &#125;&#125;public interface Voltage5V &#123; public int output5V();&#125;public class Phone &#123; public void charging(IVoltage5V iVoltage5V) &#123; if(iVoltage5V.output5V() == 5) &#123; System.out.println(\"电压为 5V, 可以充电\"); &#125; else if (iVoltage5V.output5V() &gt; 5) &#123; System.out.println(\"电压大于 5V, 不能充电\"); &#125; &#125;&#125;public class Voltage220V &#123; public int output220V() &#123; int src = 220; System.out.println(\"电压=\" + src + \"伏\"); return src; &#125;&#125;public class VoltageAdapter implements IVoltage5V &#123; private Voltage220V voltage220V; // 关联关系-聚合 // 通过构造器，传入一个 Voltage220V 实例 public VoltageAdapter(Voltage220V voltage220v) &#123; this.voltage220V = voltage220v; &#125; @Override public int output5V() &#123; int dst = 0; if(null != voltage220V) &#123; int src = voltage220V.output220V(); // 获取 220V 电压 System.out.println(\"使用对象适配器，进行适配\"); dst = src / 44; System.out.println(\"适配完成，输出的电压为=\" + dst); &#125; return dst; &#125;&#125; 对象适配器和类适配器其实算是同一种思想，只不过实现方式不同。根据合成复用原则，使用组合替代继承， 所以它解决了类适配器必须继承 resource 的局限性问题，也不再要求 destination 必须是接口。 接口适配器核心思路：当不需要全部实现接口提供的方法时，可先设计一个抽象类实现接口，并为该接口中每个方法提供一个默认实现（空方法），那么该抽象类的子类可有选择地覆盖父类的某些方法来实现需求。该模式适用于一个接口不想使用其所有的方法的情况。 123456789101112131415161718192021222324252627282930public interface Interface &#123; public void m1(); public void m2(); public void m3(); public void m4();&#125;// 在 AbsAdapter 我们将 Interface 的方法进行默认实现public abstract class AbsAdapter implements Interface &#123; public void m1() &#123; &#125; public void m2() &#123; &#125; public void m3() &#123; &#125; public void m4() &#123; &#125;&#125;public class Client &#123; public static void main(String[] args) &#123; AbsAdapter absAdapter = new AbsAdapter() &#123; // 只需覆盖需要使用接口方法 @Override public void m1() &#123; System.out.println(\"使用了 m1 的方法\"); &#125; &#125;; absAdapter.m1(); &#125;&#125; Spring MVCSpring MVC 中的 HandlerAdapter, 就使用了适配器模式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class DispatchServlet &#123; public static List&lt;HandlerAdapter&gt; handlerAdapters = new ArrayList&lt;HandlerAdapter&gt;(); public DispatchServlet() &#123; // 初始化各种适配器 handlerAdapters.add(new AnnotationHandlerAdapter()); handlerAdapters.add(new HttpHandlerAdapter()); handlerAdapters.add(new SimpleHandlerAdapter()); &#125; public void doDispatch() &#123; // 此处模拟 Spring MVC 从 request 取 handler 的对象 HttpController controller = new HttpController(); // 得到对应适配器 HandlerAdapter adapter = getHandler(controller); // 通过适配器执行对应的 controller 的对应方法 adapter.handle(controller); &#125; public HandlerAdapter getHandler(Controller controller) &#123; // 遍历：根据得到的 controller 即 handler, 返回对应适配器 for (HandlerAdapter adapter : this.handlerAdapters) &#123; // 判读此适配器是否支持此类型的 controller if (adapter.supports(controller)) &#123; return adapter; &#125; &#125; return null; &#125; public static void main(String[] args) &#123; new DispatchServlet().doDispatch(); &#125;&#125;public interface Controller &#123;&#125;// 多种 Controller 类 class HttpController implements Controller &#123; public void doHttpHandler() &#123; System.out.println(\"http...\"); &#125;&#125;class SimpleController implements Controller &#123; public void doSimplerHandler() &#123; System.out.println(\"simple...\"); &#125;&#125;class AnnotationController implements Controller &#123; public void doAnnotationHandler() &#123; System.out.println(\"annotation...\"); &#125;&#125;public interface HandlerAdapter &#123; public boolean supports(Object handler); public void handle(Object handler);&#125;// 多种 Adapter 类class SimpleHandlerAdapter implements HandlerAdapter &#123; public void handle(Object handler) &#123; ((SimpleController) handler).doSimplerHandler(); &#125; public boolean supports(Object handler) &#123; return (handler instanceof SimpleController); &#125;&#125;class HttpHandlerAdapter implements HandlerAdapter &#123; public void handle(Object handler) &#123; ((HttpController) handler).doHttpHandler(); &#125; public boolean supports(Object handler) &#123; return (handler instanceof HttpController); &#125;&#125;class AnnotationHandlerAdapter implements HandlerAdapter &#123; public void handle(Object handler) &#123; ((AnnotationController) handler).doAnnotationHandler(); &#125; public boolean supports(Object handler) &#123; return (handler instanceof AnnotationController); &#125;&#125; 桥接模式将实现与抽象放在两个不同的类层次中，使两个层次可以独立改变；Bridge 模式基于类的最小设计原则，通过使用封装、聚合及继承等行为让不同的类承担不同的职责。它的主要特点是把抽象（Abstraction）与行为实现（Implementation）分离开来，从而可以保持各部分的独立性以及应对他们的功能扩展。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class Client &#123; public static void main(String[] args) &#123; Phone phone1 = new FoldedPhone(new XiaoMi()); phone1.open(); phone1.call(); phone1.close(); System.out.println(\"=======\"); Phone phone2 = new FoldedPhone(new Vivo()); phone2.open(); phone2.call(); phone2.close(); System.out.println(\"=======\"); UpRightPhone phone3 = new UpRightPhone(new XiaoMi()); phone3.open(); phone3.call(); phone3.close(); System.out.println(\"=======\"); UpRightPhone phone4 = new UpRightPhone(new Vivo()); phone4.open(); phone4.call(); phone4.close(); &#125;&#125;// 手机这一角色的抽象public abstract class Phone &#123; // 组合品牌 private Brand brand; // 构造器 public Phone(Brand brand) &#123; super(); this.brand = brand; &#125; protected void open() &#123; this.brand.open(); &#125; protected void close() &#123; brand.close(); &#125; protected void call() &#123; brand.call(); &#125;&#125;public class FoldedPhone extends Phone &#123; public FoldedPhone(Brand brand) &#123; super(brand); &#125; public void open() &#123; super.open(); System.out.println(\" 折叠样式手机 \"); &#125; public void close() &#123; super.close(); System.out.println(\" 折叠样式手机 \"); &#125; public void call() &#123; super.call(); System.out.println(\" 折叠样式手机 \"); &#125;&#125;public class UpRightPhone extends Phone &#123; // 构造器 public UpRightPhone(Brand brand) &#123; super(brand); &#125; public void open() &#123; super.open(); System.out.println(\" 直立样式手机 \"); &#125; public void close() &#123; super.close(); System.out.println(\" 直立样式手机 \"); &#125; public void call() &#123; super.call(); System.out.println(\" 直立样式手机 \"); &#125;&#125;// 手机的行为public interface Brand &#123; void open(); void close(); void call();&#125;public class Vivo implements Brand &#123; @Overrid public void open() &#123; System.out.println(\" Vivo 手机开机 \"); &#125; @Override public void close() &#123; System.out.println(\" Vivo 手机关机 \"); &#125; @Override public void call() &#123; System.out.println(\" Vivo 手机打电话 \"); &#125;&#125;public class XiaoMi implements Brand &#123; @Override public void open() &#123; System.out.println(\" 小米手机开机 \"); &#125; @Override public void close() &#123; System.out.println(\" 小米手机关机 \"); &#125; @Override public void call() &#123; System.out.println(\" 小米手机打电话 \"); &#125;&#125; JDBC 这里稍微有点不同的是没有进行抽象。 装饰者模式动态的将新功能附加到对象上。在对象功能扩展方面，它比继承更有弹性，装饰者模式也体现了开闭原则 OCP。装饰者可以包含被装饰者：一层一层就像包快递一样，比一样的物品包装的方式不同。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public abstract class Drink &#123; public String des; // 饮品的描述 private float price = 0.0f; // Getter / Setter // 规定子类都要有计算价格的方法 public abstract float cost();&#125;public class Coffee extends Drink &#123; @Override public float cost() &#123; return super.getPrice(); &#125;&#125;public class DeCaf extends Coffee &#123; public DeCaf() &#123; setDes(\" DeCaf \"); setPrice(1.0f); &#125;&#125;public class LongBlack extends Coffee &#123; public LongBlack() &#123; setDes(\" longblack \"); setPrice(5.0f); &#125;&#125;public class ShortBlack extends Coffee&#123; public ShortBlack() &#123; setDes(\" shortblack \"); setPrice(4.0f); &#125;&#125;public class Decorator extends Drink &#123; private Drink obj; public Decorator(Drink obj) &#123; // 组合 this.obj = obj; &#125; @Override public float cost() &#123; // getPrice 自己价格 return super.getPrice() + obj.cost(); // 递归列出所有价格 &#125; @Override public String getDes() &#123; // obj.getDes() 输出被装饰者的信息 return des + \" \" + getPrice() + \" &amp;&amp; \" + obj.getDes(); &#125;&#125;// 具体的 Decorator， 这里就是调味品public class Chocolate extends Decorator &#123; public Chocolate(Drink obj) &#123; super(obj); setDes(\" 巧克力 \"); setPrice(3.0f); // 调味品的价格 &#125;&#125;public class Milk extends Decorator &#123; public Milk(Drink obj) &#123; super(obj); setDes(\" 牛奶 \"); setPrice(2.0f); &#125;&#125;public class CoffeeBar &#123; public static void main(String[] args) &#123; // 装饰者模式下的订单：2 份巧克力 + 1 份牛奶的 LongBlack // 点一份 LongBlack Drink order = new LongBlack(); System.out.println(\"费用 = \" + order.cost()); System.out.println(\"描述 = \" + order.getDes()); // order 加入一份牛奶 order = new Milk(order); System.out.println(\"order 加入一份牛奶 费用 = \" + order.cost()); System.out.println(\"order 加入一份牛奶 描述 = \" + order.getDes()); // order 加入一份巧克力 order = new Chocolate(order); System.out.println(\"order 加入一份牛奶 加入一份巧克力 费用 = \" + order.cost()); System.out.println(\"order 加入一份牛奶 加入一份巧克力 描述 = \" + order.getDes()); // order 加入一份巧克力 order = new Chocolate(order); System.out.println(\"order 加入一份牛奶 加入 2 份巧克力 费用 = \" + order.cost()); System.out.println(\"order 加入一份牛奶 加入 2 份巧克力 描述 = \" + order.getDes()); System.out.println(\"=======\"); Drink order2 = new DeCaf(); System.out.println(\"order2 无因咖啡 费用 = \" + order2.cost()); System.out.println(\"order2 无因咖啡 描述 = \" + order2.getDes()); order2 = new Milk(order2); System.out.println(\"order2 无因咖啡 加入一份牛奶 费用 = \" + order2.cost()); System.out.println(\"order2 无因咖啡 加入一份牛奶 描述 = \" + order2.getDes()); &#125;&#125; JDK-FilterInputStream 装饰者 FilterInputSteam 的部分代码。 组合模式组合模式（Composite Pattern），又叫部分整体模式，它创建了对象组的树形结构，将对象组合成树状结构以表示（整体-部分）层次关系。组合模式使得用户对单个对象和组合对象的访问具有一致性，即：组合能让客户以一致的方式处理个别对象以及组合对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136public class Client &#123; public static void main(String[] args) &#123; // 组合关系：系 &lt; 学院 &lt; 大学 OrganizationComponent university = new University(\"清华大学\", \" 中国顶级大学 \"); OrganizationComponent computerCollege = new College(\"计算机学院\", \" 计算机学院 \"); OrganizationComponent infoEngineercollege = new College(\"信息工程学院\", \" 信息工程学院 \"); // 将系加入到学院 computerCollege.add(new Department(\"软件工程\", \" 软件工程不错 \")); computerCollege.add(new Department(\"网络工程\", \" 网络工程不错 \")); computerCollege.add(new Department(\"计算机科学与技术\", \" 计算机科学与技术是老牌的专业 \")); infoEngineercollege.add(new Department(\"通信工程\", \" 通信工程不好学 \")); infoEngineercollege.add(new Department(\"信息工程\", \" 信息工程好学 \")); // 将学院加入到学校 university.add(computerCollege); university.add(infoEngineercollege); //university.print(); infoEngineercollege.print(); &#125;&#125;public abstract class OrganizationComponent &#123; private String name; // 名字 private String des; // 说明 protected void add(OrganizationComponent organizationComponent) &#123; // 默认实现 throw new UnsupportedOperationException(); &#125; protected void remove(OrganizationComponent organizationComponent) &#123; // 默认实现 throw new UnsupportedOperationException(); &#125; // 构造器 public OrganizationComponent(String name, String des) &#123; super(); this.name = name; this.des = des; &#125; // Getter / Setter // 子类都需要实现的抽象方法 protected abstract void print();&#125;// University 为非叶子节点, 可以管理 Collegepublic class University extends OrganizationComponent &#123; List&lt;OrganizationComponent&gt; organizationComponents = new ArrayList&lt;OrganizationComponent&gt;(); // 构造器 public University(String name, String des) &#123; super(name, des); &#125; // 重写 add @Override protected void add(OrganizationComponent organizationComponent) &#123; organizationComponents.add(organizationComponent); &#125; // 重写 remove @Override protected void remove(OrganizationComponent organizationComponent) &#123; organizationComponents.remove(organizationComponent); &#125; @Override public String getName() &#123; return super.getName(); &#125; @Override public String getDes() &#123; return super.getDes(); &#125; @Override protected void print() &#123; System.out.println(\"---\" + getName() + \"---\"); //遍历 organizationComponents for (OrganizationComponent organizationComponent : organizationComponents) &#123; organizationComponent.print(); &#125; &#125;&#125;public class College extends OrganizationComponent &#123; // 用于存放 Department 的 List List&lt;OrganizationComponent&gt; organizationComponents = new ArrayList&lt;OrganizationComponent&gt;(); // 构造器 public College(String name, String des) &#123; super(name, des); &#125; // 根据需求重写方法，不同层次的实现可能不同 @Override protected void add(OrganizationComponent organizationComponent) &#123; organizationComponents.add(organizationComponent); &#125; @Override protected void remove(OrganizationComponent organizationComponent) &#123; organizationComponents.remove(organizationComponent); &#125; @Override public String getName() &#123; return super.getName(); &#125; @Override public String getDes() &#123; return super.getDes(); &#125; // 输出学院 @Override protected void print() &#123; // TODO Auto-generated method stub System.out.println(\"---\" + getName() + \"---\"); //遍历 organizationComponents for (OrganizationComponent organizationComponent : organizationComponents) &#123; organizationComponent.print(); &#125; &#125;&#125;// 叶子节点public class Department extends OrganizationComponent &#123; // 无 List public Department(String name, String des) &#123; super(name, des); &#125; // 无需重写 add remove 方法 @Override public String getName() &#123; return super.getName(); &#125; @Override public String getDes() &#123; return super.getDes(); &#125; @Override protected void print() &#123; System.out.println(getName()); &#125;&#125; JDK-HashMap 外观模式外观模式通过定义一个一致的接口，用以屏蔽内部子系统的细节，使得调用端只需跟这个接口发生调用，而无需关心这个子系统的内部细节。 享元模式运用共享技术有效地支持大量细粒度的对象；常用于系统底层开发，解决系统的性能问题。像数据库连接池，里面都是创建好的连接对象，在这些连接对象中有我们需要的则直接拿来用，避免重新创建，如果没有我们需要的，则创建一个；享元模式能够解决重复对象的内存浪费的问题，当系统中有大量相似对象，创建缓冲池，不需总是创建新对象，可以从缓冲池里拿。这样可以降低系统内存，同时提高效率； 享元模式经典的应用场景就是池技术了，String 常量池、数据库连接池、缓冲池等等都是享元模式的应用，享元模式是池技术的重要实现方式。 享元模式提出了两个要求：细粒度和共享对象。这里就涉及到内部状态和外部状态了，即将对象的信息分为两个部分：内部状态和外部状态： 内部状态指对象共享出来的信息，存储在享元对象内部且不会随环境的改变而改变； 外部状态指对象得以依赖的一个标记，是随环境改变而改变的、不可共享的状态。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Client &#123; public static void main(String[] args) &#123; // 创建一个工厂类 WebSiteFactory factory = new WebSiteFactory(); // 客户要一个以新闻形式发布的网站 WebSite webSite1 = factory.getWebSiteCategory(\"新闻\"); webSite1.use(new User(\"tom\")); // 客户要一个以博客形式发布的网站 WebSite webSite2 = factory.getWebSiteCategory(\"博客\"); webSite2.use(new User(\"jack\")); // 客户要一个以博客形式发布的网站 WebSite webSite3 = factory.getWebSiteCategory(\"博客\"); webSite3.use(new User(\"smith\")); // 客户要一个以博客形式发布的网站 WebSite webSite4 = factory.getWebSiteCategory(\"博客\"); webSite4.use(new User(\"king\")); System.out.println(\"网站的分类共=\" + factory.getWebSiteCount()); &#125;&#125;public abstract class WebSite &#123; public abstract void use(User user); // 抽象方法&#125;// 具体网站public class ConcreteWebSite extends WebSite &#123; // 共享的部分，内部状态 private String type = \"\"; // 网站发布的类型 // 构造器 public ConcreteWebSite(String type) &#123; this.type = type; &#125; @Override public void use(User user) &#123; System.out.println(\"网站的发布形式为:\" + type + \" 使用者是\" + user.getName()); &#125;&#125; // 使用者public class User &#123; private String name; public User(String name) &#123; super(); this.name = name; &#125; // Getter / Setter&#125;// 网站工厂类，根据需要返回一个相应的网站public class WebSiteFactory &#123; // 集合：相当于一个池 private HashMap&lt;String, ConcreteWebSite&gt; pool = new HashMap&lt;&gt;(); // 根据网站的类型，返回一个网站, 如果没有就创建一个网站，并放入到池中,并返回 public WebSite getWebSiteCategory(String type) &#123; if(!pool.containsKey(type)) &#123; // 无此类型则创建一个网站，并放入到池中 pool.put(type, new ConcreteWebSite(type)); &#125; return (WebSite)pool.get(type); &#125; // 获取网站分类的总数 public int getWebSiteCount() &#123; return pool.size(); &#125;&#125; JDK-Integer[127,-128]：此范围内直接从池返回 。 享元模式提高了系统的复杂度。需要分离出内部状态和外部状态，而外部状态具有固化特性，不应该随着内部状态的改变而改变，这是我们使用享元模式需要注意的地方；使用享元模式时，注意划分内部状态和外部状态，并且需要有一个工厂类加以控制。 代理模式代理模式：为一个对象提供一个替身，以控制对这个对象的访问。即通过代理对象访问目标对象。这样做的好处是可以在目标对象实现的基础上，增强额外的功能操作，即扩展目标对象的功能；被代理的对象可以是远程对象、创建开销大的对象或需要安全控制的对象； 代理模式有不同的形式, 主要有三种： 静态代理 动态代理 （JDK 代理、接口代理） Cglib 代理 （可以在内存动态的创建对象，而不需要实现接口， 他是属于动态代理的范畴） 静态代理静态代理在使用时，需要定义接口或者父类，被代理对象（即目标对象）与代理对象一起实现相同的接口或者是继承相同父类。 1234567891011121314151617181920212223242526272829303132333435363738public class Client &#123; public static void main(String[] args) &#123; // 创建目标对象：被代理对象 TeacherDaoImpl teacherDaoImpl = new TeacherDaoImpl(); // 创建代理对象, 同时将被代理对象传递给代理对象 TeacherDaoProxy teacherDaoProxy = new TeacherDaoProxy(teacherDaoImpl); // 通过代理对象，调用到被代理对象的方法 // 即：执行的是代理对象的方法，代理对象再去调用目标对象的方法 teacherDaoProxy.teach(); &#125;&#125;// 接口public interface TeacherDao &#123; void teach(); // 授课的方法&#125;public class TeacherDaoImpl implements TeacherDao &#123; @Override public void teach() &#123; System.out.println(\" 老师授课中 。。。。。\"); &#125;&#125;// 代理对象：静态代理public class TeacherDaoProxy implements TeacherDao&#123; private TeacherDao target; // 目标对象，通过接口来聚合 // 构造器 public TeacherDaoProxy(TeacherDao target) &#123; this.target = target; &#125; @Override public void teach() &#123; System.out.println(\"开始代理 完成某些操作...\"); // 目标方法前操作 target.teach(); // 目标方法 System.out.println(\"提交...\"); // 目标方法后操作 &#125;&#125; 动态代理代理对象不需要实现接口，但是目标对象要实现接口，否则不能用动态代理。代理对象的生成，是利用 JDK 的 API，动态的在内存中构建代理对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Client &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub // 创建目标对象 TeacherDao target = new TeacherDaoImpl(); // 给目标对象，创建代理对象, 可以转成 TeacherDao TeacherDao proxyInstance = (TeacherDao)new ProxyFactory(target).getProxyInstance(); // proxyInstance = class com.sun.proxy.$Proxy0 内存中动态生成了代理对象 System.out.println(\"proxyInstance=\" + proxyInstance.getClass()); // 通过代理对象，调用目标对象的方法 proxyInstance.teach(); proxyInstance.sayHello(\" tom \"); &#125;&#125;// 接口public interface teacherDao &#123; void teach(); // 授课方法 void sayHello(String name);&#125;public class ProxyFactory &#123; // 维护一个目标对象 , Object private Object target; // 构造器：对 target 进行初始化 public ProxyFactory(Object target) &#123; this.target = target; &#125; // 给目标对象 生成一个代理对象 public Object getProxyInstance() &#123; // ClassLoader loader ： 指定当前目标对象使用的类加载器, 获取加载器的方法固定 // Class&lt;?&gt;[] interfaces: 目标对象实现的接口类型，使用泛型方法确认类型 // InvocationHandler h : 事情处理，执行目标对象的方法时，会触发事情处理器方法, 会把当前执行 return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"JDK 代理开始\"); //反射机制调用目标对象的方法 Object returnVal = method.invoke(target, args); System.out.println(\"JDK 代理提交\"); return returnVal; &#125; &#125;); &#125;&#125;public class TeacherDaoImpl implements TeacherDao &#123; @Override public void teach() &#123; System.out.println(\" 老师授课中... \"); &#125; @Override public void sayHello(String name) &#123; System.out.println(\"hello \" + name); &#125;&#125; Cglib 代理模式静态代理和 JDK 代理模式都要求目标对象是实现一个接口,但是有时候目标对象只是一个单独的对象，并没有实现任何的接口，这个时候可使用目标对象子类来实现代理，这种模式就是 Cglib 代理。Cglib 代理也叫作子类代理，它是在内存中构建一个子类对象从而实现对目标对象功能扩展。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Client &#123; public static void main(String[] args) &#123; // 创建目标对象 TeacherDao target = new TeacherDao(); // 获取到代理对象，并且将目标对象传递给代理对象 TeacherDao proxyInstance = (TeacherDao)new ProxyFactory(target).getProxyInstance(); // 执行代理对象的方法，触发 intercept 方法，从而实现对目标对象的调用 String res = proxyInstance.teach(); System.out.println(\"res=\" + res); &#125;&#125;public class ProxyFactory implements MethodInterceptor &#123; // 维护一个目标对象 private Object target; // 构造器，传入一个被代理的对象 public ProxyFactory(Object target) &#123; this.target = target; &#125; // 返回一个代理对象: 是 target 对象的代理对象 public Object getProxyInstance() &#123; // 创建一个工具类 Enhancer enhancer = new Enhancer(); // 设置父类 enhancer.setSuperclass(target.getClass()); // 设置回调函数 enhancer.setCallback(this); // 创建子类对象，即代理对象 return enhancer.create(); &#125; // 重写 intercept 方法，会调用目标对象的方法 @Override public Object intercept(Object arg0, Method method, Object[] args, MethodProxy arg3) throws Throwable &#123; System.out.println(\"Cglib 代理模式开始\"); Object returnVal = method.invoke(target, args); System.out.println(\"Cglib 代理模式提交\"); return returnVal; &#125;&#125;public class TeacherDao &#123; public String teach() &#123; System.out.println(\" 老师授课中，cglib 代理不需要实现接口 \"); return \"hello\"; &#125;&#125; 常用变体 防火墙代理：内网通过代理穿透防火墙，实现对公网的访问; 缓存代理：比如：当请求图片文件等资源时，先到缓存代理取，如果取到资源则 ok，如果取不到资源，再到公网或者数据库取，然后缓存； 远程代理远程对象的本地代表，通过它可以把远程对象当本地对象来调用。远程代理通过网络和真正的远程对象沟通信息； 同步代理：主要使用在多线程编程中，完成多线程间同步工作。 行为型模式模板方法模式定义一个操作中的算法的骨架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的结构，就可以重定义该算法的某些特定步骤。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Client &#123; public static void main(String[] args) &#123; // 制作红豆豆浆 System.out.println(\"---制作红豆豆浆---\"); SoyaMilk redBeanSoyaMilk = new RedBeanSoyaMilk(); redBeanSoyaMilk.make(); System.out.println(\"---制作花生豆浆---\"); SoyaMilk peanutSoyaMilk = new PeanutSoyaMilk(); peanutSoyaMilk.make(); &#125;&#125;// 抽象类，表示豆浆public abstract class SoyaMilk &#123; // final 类型的模板方法：防止子类覆盖 final void make() &#123; select(); addCondiments(); soak(); beat(); &#125; // 选材料 void select() &#123; System.out.println(\"第一步：选择好的新鲜黄豆\"); &#125; // 添加不同的配料：抽象方法由子类具体实现 abstract void addCondiments(); // 浸泡 void soak() &#123; System.out.println(\"第三步：黄豆和配料开始浸泡（3小时）\"); &#125; void beat() &#123; System.out.println(\"第四步：黄豆和配料放到豆浆机去打碎\"); &#125;&#125;public class PeanutSoyaMilk extends SoyaMilk &#123; @Override void addCondiments() &#123; System.out.println(\" 加入上好的花生 \"); &#125;&#125;public class RedBeanSoyaMilk extends SoyaMilk &#123; @Override void addCondiments() &#123; System.out.println(\" 加入上好的红豆 \"); &#125;&#125; 钩子方法1234567891011121314151617181920212223242526272829//抽象类，表示豆浆public abstract class SoyaMilk &#123; final void make() &#123; select(); // 钩子 if(customerWantCondiments()) &#123; addCondiments(); &#125; soak(); beat(); &#125; // 选材料 void select() &#123; System.out.println(\"第一步：选择好的新鲜黄豆\"); &#125; // 钩子：添加不同的配料：抽象方法由子类具体实现 abstract void addCondiments(); // 浸泡 void soak() &#123; System.out.println(\"第三步：黄豆和配料开始浸泡（3小时）\"); &#125; void beat() &#123; System.out.println(\"第四步：黄豆和配料放到豆浆机去打碎\"); &#125; // 钩子方法：决定是否需要添加配料 boolean customerWantCondiments() &#123; return true; &#125;&#125; Spring 命令模式命令模式（Command Pattern）：在软件设计中，我们经常需要向某些对象发送请求，但是并不知道请求的接收者是谁，也不知道被请求的操作是哪个，只需在程序运行时指定具体的请求接收者即可，此时，可以使用命令模式来进行设计。 命名模式使得请求发送者与请求接收者消除彼此之间的耦合，让对象之间的调用关系更加灵活，实现解耦。请求发起者和请求执行者之间的解耦是通过命令对象实现的，命令对象起到了纽带桥梁的作用。 在命名模式中，会将一个请求封装为一个对象，以便使用不同参数来表示不同的请求(即命名)，同时命令模式也支持可撤销的操作。 空命令也是一种设计模式，它为我们省去了判空的操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class Client &#123; public static void main(String[] args) &#123; // 使用命令设计模式，完成通过遥控器对电灯进行操作 // 创建电灯的对象：接受者 LightReceiver lightReceiver = new LightReceiver(); // 创建电灯相关的开关命令 LightOnCommand lightOnCommand = new LightOnCommand(lightReceiver); LightOffCommand lightOffCommand = new LightOffCommand(lightReceiver); // 需要一个遥控器：调用者 RemoteController remoteController = new RemoteController(); // 给我们的遥控器设置命令, 比如 no = 0 是电灯的开和关的操作 remoteController.setCommand(0, lightOnCommand, lightOffCommand); System.out.println(\"---按下灯的开按钮---\"); remoteController.onButtonWasPushed(0); System.out.println(\"---按下灯的关按钮---\"); remoteController.offButtonWasPushed(0); System.out.println(\"---按下撤销按钮---\"); remoteController.undoButtonWasPushed(); &#125;&#125;// 创建命令接口public interface Command &#123; // 执行动作(操作) public void execute(); // 撤销动作(操作) public void undo();&#125;// 接收者public class LightReceiver &#123; public void on() &#123; System.out.println(\" 电灯打开了... \"); &#125; public void off() &#123; System.out.println(\" 电灯关闭了... \"); &#125;&#125;// 电灯操作命令具体实现public class LightOffCommand implements Command &#123; // 聚合 LightReceiver：接收者 LightReceiver light; // 构造器 public LightOffCommand(LightReceiver light) &#123; super(); this.light = light; &#125; @Override public void execute() &#123; // 调用接收者的方法 light.off(); &#125; @Override public void undo() &#123; // 调用接收者的方法 light.on(); &#125;&#125;public class LightOnCommand implements Command &#123; // 聚合 LightReceiver LightReceiver light; // 构造器 public LightOnCommand(LightReceiver light) &#123; super(); this.light = light; &#125; @Override public void execute() &#123; // 调用接收者的方法 light.on(); &#125; @Override public void undo() &#123; // 调用接收者的方法 light.off(); &#125;&#125;// 空命令模式public class NoCommand implements Command &#123; @Override public void execute() &#123; &#125; @Override public void undo() &#123; &#125;&#125;public class RemoteController &#123; // 开按钮的命令数组 Command[] onCommands; Command[] offCommands; // 执行撤销的命令 Command undoCommand; // 构造器，完成对按钮初始化 public RemoteController() &#123; onCommands = new Command[5]; offCommands = new Command[5]; for (int i = 0; i &lt; 5; i++) &#123; onCommands[i] = new NoCommand(); offCommands[i] = new NoCommand(); &#125; &#125; // 给我们的按钮设置你需要的命令 public void setCommand(int no, Command onCommand, Command offCommand) &#123; onCommands[no] = onCommand; offCommands[no] = offCommand; &#125; // 按下开按钮 public void onButtonWasPushed(int no) &#123; // no 0 // 找到你按下的开的按钮， 并调用对应方法 onCommands[no].execute(); // 记录这次的操作，用于撤销 undoCommand = onCommands[no]; &#125; // 按下开按钮 public void offButtonWasPushed(int no) &#123; // no 0 // 找到你按下的关的按钮， 并调用对应方法 offCommands[no].execute(); // 记录这次的操作，用于撤销 undoCommand = offCommands[no]; &#125; // 按下撤销按钮 public void undoButtonWasPushed() &#123; undoCommand.undo(); &#125;&#125; 访问者模式访问者模式（Visitor Pattern），封装一些作用于某种数据结构的各元素的操作，它可以在不改变数据结构的前提下定义作用于这些元素的新的操作。其做法主要将数据结构与数据操作分离，解决数据结构和操作耦合性问题，其基本原理是在被访问的类里面加一个对外提供接待访问者的接口。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class Client &#123; public static void main(String[] args) &#123; // 创建 ObjectStructure：枚举元素，用来允许访问者访问元素 ObjectStructure objectStructure = new ObjectStructure(); // 双分派：1.分派访问者 objectStructure.attach(new Man()); objectStructure.attach(new Woman()); // 成功 2.根据访问者分发任务 Success success = new Success(); objectStructure.display(success); System.out.println(\"=======\"); Fail fail = new Fail(); objectStructure.display(fail); System.out.println(\"===待定测评===\"); Wait wait = new Wait(); objectStructure.display(wait); &#125;&#125;// 数据结构，管理 Man Womanpublic class ObjectStructure &#123; // 维护了一个集合 private List&lt;Person&gt; persons = new LinkedList&lt;&gt;(); // 增加 Element public void attach(Person p) &#123; persons.add(p); &#125; // 移除 Element public void detach(Person p) &#123; persons.remove(p); &#125; // 显示测评情况 public void display(Action action) &#123; for(Person p: persons) &#123; p.accept(action); &#125; &#125;&#125;// Elementpublic abstract class Person &#123; public abstract void accept(Action action);&#125;public class Man extends Person &#123; @Override public void accept(Action action) &#123; action.getManResult(this); &#125;&#125;public class Woman extends Person&#123; @Override public void accept(Action action) &#123; action.getWomanResult(this); &#125;&#125;// Visitor：为 Element 提供具体操作public abstract class Action &#123; // 男性评价 public abstract void getManResult(Man man); // 女性评价 public abstract void getWomanResult(Woman woman);&#125;public class Success extends Action &#123; @Override public void getManResult(Man man) &#123; System.out.println(\" 男人给好评 !\"); &#125; @Override public void getWomanResult(Woman woman) &#123; System.out.println(\" 女人给好评 !\"); &#125;&#125;public class Fail extends Action &#123; @Override public void getManResult(Man man) &#123; System.out.println(\" 男人给差评 !\"); &#125; @Override public void getWomanResult(Woman woman) &#123; System.out.println(\" 女人给差评 !\"); &#125;&#125;public class Wait extends Action &#123; @Override public void getManResult(Man man) &#123; System.out.println(\" 男人给的评价是该歌手待定 \"); &#125; @Override public void getWomanResult(Woman woman) &#123; System.out.println(\" 女人给的评价是该歌手待定 \"); &#125;&#125; 迭代器模式迭代器模式，提供一种遍历集合元素的统一接口，用一致的方法遍历集合元素，不需要知道集合对象的底层表示，即：不暴露其内部的结构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163public class Client &#123; public static void main(String[] args) &#123; // 创建学院 List&lt;College&gt; collegeList = new ArrayList&lt;College&gt;(); ComputerCollege computerCollege = new ComputerCollege(); InfoCollege infoCollege = new InfoCollege(); collegeList.add(computerCollege); collegeList.add(infoCollege); OutPutImpl outPutImpl = new OutPutImpl(collegeList); outPutImpl.printCollege(); &#125;&#125;// 接口public interface College &#123; public String getName(); // 添加系 public void addDepartment(String name, String desc); // 返回一个迭代器 public Iterator createIterator();&#125;// 系public class Department &#123; private String name; private String desc; public Department(String name, String desc) &#123; super(); this.name = name; this.desc = desc; &#125; // Getter / Setter&#125;public class ComputerCollege implements College &#123; Department[] departments; int numOfDepartment = 0 ; // 保存当前数组的对象个数 public ComputerCollege() &#123; departments = new Department[5]; addDepartment(\"Java 专业\", \" Java 专业 \"); addDepartment(\"PHP 专业\", \" PHP 专业 \"); addDepartment(\"大数据专业\", \" 大数据专业 \"); &#125; @Override public String getName() &#123; return \"计算机学院\"; &#125; @Override public void addDepartment(String name, String desc) &#123; Department department = new Department(name, desc); departments[numOfDepartment] = department; numOfDepartment += 1; &#125; @Override public Iterator createIterator() &#123; // 实现了数据共享 return new ComputerCollegeIterator(departments); &#125;&#125;public class ComputerCollegeIterator implements Iterator &#123; Department[] departments; int position = 0; // 遍历的位置 public ComputerCollegeIterator(Department[] departments) &#123; this.departments = departments; &#125; //判断是否还有下一个元素 @Override public boolean hasNext() &#123; if(position &gt;= departments.length || departments[position] == null) &#123; return false; &#125;else &#123; return true; &#125; &#125; @Override public Object next() &#123; Department department = departments[position]; position += 1; return department; &#125; // 删除的方法，默认空实现 public void remove() &#123; &#125;&#125;public class InfoCollege implements College &#123; List&lt;Department&gt; departmentList; public InfoCollege() &#123; departmentList = new ArrayList&lt;Department&gt;(); addDepartment(\"信息安全专业\", \" 信息安全专业 \"); addDepartment(\"网络安全专业\", \" 网络安全专业 \"); addDepartment(\"服务器安全专业\", \" 服务器安全专业 \"); &#125; @Override public String getName() &#123; // TODO Auto-generated method stub return \"信息工程学院\"; &#125; @Override public void addDepartment(String name, String desc) &#123; // TODO Auto-generated method stub Department department = new Department(name, desc); departmentList.add(department); &#125; @Override public Iterator createIterator() &#123; return new InfoColleageIterator(departmentList); &#125;&#125;public class InfoColleageIterator implements Iterator &#123; List&lt;Department&gt; departmentList; // 信息工程学院是以 List 方式存放系 int index = -1; // 索引 public InfoColleageIterator(List&lt;Department&gt; departmentList) &#123; this.departmentList = departmentList; &#125; // 判断 list 中还有没有下一个元素 @Override public boolean hasNext() &#123; if(index &gt;= departmentList.size() - 1) &#123; return false; &#125; else &#123; index += 1; return true; &#125; &#125; @Override public Object next() &#123; return departmentList.get(index); &#125; // 默认 public void remove() &#123; &#125;&#125;public class OutPutImpl &#123; // 学院集合 List&lt;College&gt; collegeList; public OutPutImpl(List&lt;College&gt; collegeList) &#123; this.collegeList = collegeList; &#125; // 遍历所有学院,然后调用 printDepartment 输出各个学院的系 public void printCollege() &#123; // 从 collegeList 取出所有学院, Java 中的 List 已经实现 Iterator Iterator&lt;College&gt; iterator = collegeList.iterator(); while(iterator.hasNext()) &#123; // 取出一个学院 College college = iterator.next(); System.out.println(\"=== \"+college.getName() +\"====\" ); printDepartment(college.createIterator()); // 得到对应迭代器 &#125; &#125; // 输出学院，输出系 public void printDepartment(Iterator iterator) &#123; while(iterator.hasNext()) &#123; Department d = (Department)iterator.next(); System.out.println(d.getName()); &#125; &#125;&#125; JDK-ArrayList 提供了一种设计思想，就是一个类应该只有一个引起变化的原因（叫做单一责任原则）。在聚合类中，我们把迭代器分开，就是要把管理对象集合和遍历对象集合的责任分开，这样一来集合改变的话，只影响到聚合对象。而如果遍历方式改变的话，只影响到了迭代器。 观察者模式观察者模式：对象之间多对一依赖的一种设计方案，被依赖的对象为 Subject，依赖的对象为 Observer，Subject 通知 Observer 变化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127public class Client &#123; public static void main(String[] args) &#123; // 创建一个 WeatherData WeatherData weatherData = new WeatherData(); // 创建观察者 CurrentConditions currentConditions = new CurrentConditions(); BaiduSite baiduSite = new BaiduSite(); // 注册到 weatherData weatherData.registerObserver(currentConditions); weatherData.registerObserver(baiduSite); // 测试 System.out.println(\"通知各个注册的观察者, 看看信息\"); weatherData.setData(10f, 100f, 30.3f); weatherData.removeObserver(currentConditions); // 测试 System.out.println(); System.out.println(\"通知各个注册的观察者, 看看信息\"); weatherData.setData(10f, 100f, 30.3f); &#125;&#125;// 观察者接口public interface Observer &#123; public void update(float temperature, float pressure, float humidity);&#125;public class BaiduSite implements Observer &#123; // 温度，气压，湿度 private float temperature; private float pressure; private float humidity; // 更新天气情况：WeatherData 调用（推送模式） public void update(float temperature, float pressure, float humidity) &#123; this.temperature = temperature; this.pressure = pressure; this.humidity = humidity; display(); &#125; // 显示 public void display() &#123; System.out.println(\"===百度网站===\"); System.out.println(\"***百度网站 气温 : \" + temperature + \"***\"); System.out.println(\"***百度网站 气压: \" + pressure + \"***\"); System.out.println(\"***百度网站 湿度: \" + humidity + \"***\"); &#125;&#125;public class CurrentConditions implements Observer &#123; private float temperature; private float pressure; private float humidity; public void update(float temperature, float pressure, float humidity) &#123; this.temperature = temperature; this.pressure = pressure; this.humidity = humidity; display(); &#125; public void display() &#123; System.out.println(\"***Today Temperature: \" + temperature + \"***\"); System.out.println(\"***Today Pressure: \" + pressure + \"***\"); System.out.println(\"***Today Humidity: \" + humidity + \"***\"); &#125;&#125;// 接口, 让 WeatherData 来实现public interface Subject &#123; public void registerObserver(Observer o); public void removeObserver(Observer o); public void notifyObservers();&#125;/*** 核心类* 1. 包含最新的天气情况信息* 2. 含有观察者集合，使用 ArrayList 管理* 3. 当数据有更新时，就主动的调用 ArrayList, 通知所有的（接入方：观察者）就看到最新的信息*/public class WeatherData implements Subject &#123; private float temperatrue; private float pressure; private float humidity; // 观察者集合 private ArrayList&lt;Observer&gt; observers; // 加入新的第三方 public WeatherData() &#123; observers = new ArrayList&lt;Observer&gt;(); &#125; public float getTemperature() &#123; return temperatrue; &#125; public float getPressure() &#123; return pressure; &#125; public float getHumidity() &#123; return humidity; &#125; public void dataChange() &#123; // 调用接入方的 update notifyObservers(); &#125; // 当数据有更新时，就调用 setData public void setData(float temperature, float pressure, float humidity) &#123; this.temperatrue = temperature; this.pressure = pressure; this.humidity = humidity; // 调用 dataChange， 将最新的信息推送给接入方 currentConditions dataChange(); &#125; // 注册一个观察者 @Override public void registerObserver(Observer o) &#123; observers.add(o); &#125; // 移除一个观察者 @Override public void removeObserver(Observer o) &#123; if(observers.contains(o)) &#123; observers.remove(o); &#125; &#125; // 遍历所有的观察者，并通知 @Override public void notifyObservers() &#123; for(int i = 0; i &lt; observers.size(); i++) &#123; observers.get(i).update(this.temperatrue, this.pressure, this.humidity); &#125; &#125;&#125; 观察者模式设计后，会以集合的方式来管理用户（Observer），包括注册，移除和通知。这样，我们增加观察者（这里可以理解成一个新的公告板）就不需要去修改核心类 WeatherData 不会修改代码，遵守了 OCP 原则。 中介模式中介者模式（Mediator Pattern），用一个中介对象来封装一系列的对象交互。中介者使各个对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 比如 MVC 模式，C（Controller 控制器）是 M（Model 模型）和 V（View 视图）的中介者，在前后端交互时起到了中间人的作用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155public class ClientTest &#123; public static void main(String[] args) &#123; // 创建一个中介者对象 Mediator mediator = new ConcreteMediator(); // 创建 Alarm 并且加入到 ConcreteMediator 对象的 HashMap Alarm alarm = new Alarm(mediator, \"alarm\"); // 创建了 CoffeeMachine 对象，并 且加入到 ConcreteMediator 对象的 HashMap CoffeeMachine coffeeMachine = new CoffeeMachine(mediator, \"coffeeMachine\"); // 创建 Curtains , 并且加入到 ConcreteMediator 对象的 HashMap Curtains curtains = new Curtains(mediator, \"curtains\"); TV tV = new TV(mediator, \"TV\"); // 让闹钟发出消息 alarm.SendAlarm(0); coffeeMachine.FinishCoffee(); alarm.SendAlarm(1); &#125;&#125;// 同事抽象类public abstract class Colleague &#123; // 所有的同事都知道中介 private Mediator mediator; public String name; public Colleague(Mediator mediator, String name) &#123; this.mediator = mediator; this.name = name; &#125; public Mediator GetMediator() &#123; return this.mediator; &#125; public abstract void SendMessage(int stateChange);&#125;// 具体的同事类public class Alarm extends Colleague &#123; // 构造器 public Alarm(Mediator mediator, String name) &#123; super(mediator, name); // 在创建 Alarm 同事对象时，将自己注册到 ConcreteMediator 对象中[集合] mediator.Register(name, this); &#125; public void SendAlarm(int stateChange) &#123; SendMessage(stateChange); &#125; @Override public void SendMessage(int stateChange) &#123; // 调用的中介者对象的 getMessage this.GetMediator().GetMessage(stateChange, this.name); &#125;&#125;public class CoffeeMachine extends Colleague &#123; public CoffeeMachine(Mediator mediator, String name) &#123; super(mediator, name); mediator.Register(name, this); &#125; @Override public void SendMessage(int stateChange) &#123; this.GetMediator().GetMessage(stateChange, this.name); &#125; public void StartCoffee() &#123; System.out.println(\"It's time to startcoffee!\"); &#125; public void FinishCoffee() &#123; System.out.println(\"After 5 minutes!\"); System.out.println(\"Coffee is ok!\"); SendMessage(0); &#125;&#125;public class TV extends Colleague &#123; public TV(Mediator mediator, String name) &#123; super(mediator, name); mediator.Register(name, this); &#125; @Override public void SendMessage(int stateChange) &#123; this.GetMediator().GetMessage(stateChange, this.name); &#125; public void StartTv() &#123; System.out.println(\"It's time to StartTv!\"); &#125; public void StopTv() &#123; System.out.println(\"StopTv!\"); &#125;&#125;public class Curtains extends Colleague &#123; public Curtains(Mediator mediator, String name) &#123; super(mediator, name); mediator.Register(name, this); &#125; @Override public void SendMessage(int stateChange) &#123; this.GetMediator().GetMessage(stateChange, this.name); &#125; public void UpCurtains() &#123; System.out.println(\"I am holding Up Curtains!\"); &#125;&#125;public abstract class Mediator &#123; // 将给中介者对象加入到集合中 public abstract void Register(String colleagueName, Colleague colleague); // 接收消息：由具体的同事对象发出 public abstract void GetMessage(int stateChange, String colleagueName); public abstract void SendMessage();&#125;// 具体的中介者类public class ConcreteMediator extends Mediator &#123; // 集合：放入所有的同事对象 private HashMap&lt;String, Colleague&gt; colleagueMap; private HashMap&lt;String, String&gt; interMap; public ConcreteMediator() &#123; colleagueMap = new HashMap&lt;String, Colleague&gt;(); interMap = new HashMap&lt;String, String&gt;(); &#125; @Override public void Register(String colleagueName, Colleague colleague) &#123; colleagueMap.put(colleagueName, colleague); if (colleague instanceof Alarm) &#123; // 限定了每种类型机器的数量只能有一个：最新注册的机器 interMap.put(\"Alarm\", colleagueName); &#125; else if (colleague instanceof CoffeeMachine) &#123; interMap.put(\"CoffeeMachine\", colleagueName); &#125; else if (colleague instanceof TV) &#123; interMap.put(\"TV\", colleagueName); &#125; else if (colleague instanceof Curtains) &#123; interMap.put(\"Curtains\", colleagueName); &#125; &#125; // 具体中介者的核心方法 // 1. 根据得到消息，完成对应任务 // 2. 中介者在这个方法，协调各个具体的同事对象，完成任务 @Override public void GetMessage(int stateChange, String colleagueName) &#123; //处理闹钟发出的消息 if (colleagueMap.get(colleagueName) instanceof Alarm) &#123; if (stateChange == 0) &#123; ((CoffeeMachine) (colleagueMap.get(interMap.get(\"CoffeeMachine\")))).StartCoffee(); ((TV) (colleagueMap.get(interMap.get(\"TV\")))).StartTv(); &#125; else if (stateChange == 1) &#123; ((TV) (colleagueMap.get(interMap.get(\"TV\")))).StopTv(); &#125; &#125; else if (colleagueMap.get(colleagueName) instanceof CoffeeMachine) &#123; ((Curtains) (colleagueMap.get(interMap.get(\"Curtains\")))).UpCurtains(); &#125; else if (colleagueMap.get(colleagueName) instanceof TV) &#123; // 如果 TV 发现消息 &#125; else if (colleagueMap.get(colleagueName) instanceof Curtains) &#123; // 如果 Curtain 发现消息&#125; @Override public void SendMessage() &#123; &#125; &#125;&#125; 备忘录模式备忘录模式（Memento Pattern）在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，这样以后就可将该对象恢复到原先保存的状态。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Client &#123; public static void main(String[] args) &#123; // 创建游戏角色 GameRole gameRole = new GameRole(); gameRole.setVit(100); gameRole.setDef(100); System.out.println(\"和 Boss 大战前的状态\"); gameRole.display(); // 把当前状态保存 caretaker：看守者 Caretaker caretaker = new Caretaker(); caretaker.setMemento(gameRole.createMemento()); System.out.println(\"和 Boss 大战后\"); gameRole.setDef(30); gameRole.setVit(30); gameRole.display(); System.out.println(\"大战后，使用备忘录对象恢复到战斗前\"); gameRole.recoverGameRoleFromMemento(caretaker.getMemento()); System.out.println(\"恢复后的状态\"); gameRole.display(); &#125;&#125;// 守护者对象, 保存游戏角色的状态public class Caretaker &#123; // 如果只保存一次状态 private Memento memento; // 对 GameRole 保存多次状态 // private ArrayList&lt;Memento&gt; mementos; // 对多个游戏角色保存多个状态 // private HashMap&lt;String, ArrayList&lt;Memento&gt;&gt; rolesMementos; public Memento getMemento() &#123; return memento; &#125; public void setMemento(Memento memento) &#123; this.memento = memento; &#125;&#125;public class Memento &#123; private int vit; private int def; public Memento(int vit, int def) &#123; super(); this.vit = vit; this.def = def; &#125; // Getter / Setter&#125;public class GameRole &#123; private int vit; private int def; // 创建 Memento：即根据当前的状态得到 Memento public Memento createMemento() &#123; return new Memento(vit, def); &#125; // 备忘录对象恢复 GameRole 的状态 public void recoverGameRoleFromMemento(Memento memento) &#123; this.vit = memento.getVit(); this.def = memento.getDef(); &#125; // 显示当前游戏角色的状态 public void display() &#123; System.out.println(\"游戏角色当前的攻击力：\" + this.vit + \" 防御力: \" + this.def); &#125; // Getter / Setter&#125; 解释器模式给定一个语言（表达式），定义它的文法的一种表示，并定义一个解释器，使用该解释器来解释语言中的句子（表达式）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129public class ClientTest &#123; public static void main(String[] args) throws IOException &#123; String expStr = getExpStr(); // a+b HashMap&lt;String, Integer&gt; var = getValue(expStr); // var &#123;a=10, b=20&#125; Calculator calculator = new Calculator(expStr); System.out.println(\"运算结果：\" + expStr + \"=\" + calculator.run(var)); &#125; // 获得表达式 public static String getExpStr() throws IOException &#123; System.out.print(\"请输入表达式：\"); return (new BufferedReader(new InputStreamReader(System.in))).readLine(); &#125; // 获得值映射 public static HashMap&lt;String, Integer&gt; getValue(String expStr) throws IOException &#123; HashMap&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); for (char ch : expStr.toCharArray()) &#123; if (ch != '+' &amp;&amp; ch != '-') &#123; if (!map.containsKey(String.valueOf(ch))) &#123; System.out.print(\"请输入\" + String.valueOf(ch) + \"的值：\"); String in = (new BufferedReader(new InputStreamReader(System.in))).readLine(); map.put(String.valueOf(ch), Integer.valueOf(in)); &#125; &#125; &#125; return map; &#125;&#125;public class Calculator &#123; // 定义表达式 private Expression expression; // 构造函数传参，并解析 public Calculator(String expStr) &#123; // expStr = a+b // 安排运算先后顺序 Stack&lt;Expression&gt; stack = new Stack&lt;&gt;(); // 表达式拆分成字符数组 char[] charArray = expStr.toCharArray();// [a, +, b] Expression left = null; Expression right = null; // 遍历我们的字符数组， 即遍历 [a, +, b] // 针对不同的情况，做处理 for (int i = 0; i &lt; charArray.length; i++) &#123; switch (charArray[i]) &#123; case '+': left = stack.pop(); // 从 stack 取出 left =&gt; \"a\" right = new VarExpression(String.valueOf(charArray[++i])); // 取出右表达式 \"b\" stack.push(new AddExpression(left, right)); // 然后根据得到 left 和 right 构建 AddExpresson 加入 stack break; case '-': left = stack.pop(); right = new VarExpression(String.valueOf(charArray[++i])); stack.push(new SubExpression(left, right)); break; default: // 如果是一个 Var 就创建要给 VarExpression 对象，并 push 到 stack stack.push(new VarExpression(String.valueOf(charArray[i]))); break; &#125; &#125; // 当遍历完整个 charArray 数组后，stack 就得到最后 Expression this.expression = stack.pop(); &#125; public int run(HashMap&lt;String, Integer&gt; var) &#123; // 最后将表达式 a+b 和 var = &#123;a=10,b=20&#125; // 然后传递给 expression 的 interpreter 进行解释执行 return this.expression.interpreter(var); &#125;&#125;// 抽象类表达式，通过 HashMap 键值对, 可以获取到变量的值public abstract class Expression &#123; // a + b - c // 解释公式和数值, key 就是公式，参数[a,b,c], value 就是就是具体值 // HashMap &#123;a=10, b=20&#125; public abstract int interpreter(HashMap&lt;String, Integer&gt; var);&#125;// 变量的解释器public class VarExpression extends Expression &#123; private String key; // key=a,key=b,key=c public VarExpression(String key) &#123; this.key = key; &#125; // &#123;a=10, b=20&#125; // interpreter 根据变量名称，返回对应值 @Override public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; return var.get(this.key); &#125;&#125;// 抽象运算符号解析器 这里，每个运算符号，都只和自己左右两个数字有关系，// 但左右两个数字有可能也是一个解析的结果，无论何种类型，都是 Expression 类的实现类public class SymbolExpression extends Expression &#123; protected Expression left; protected Expression right; public SymbolExpression(Expression left, Expression right) &#123; this.left = left; this.right = right; &#125; // 因为 SymbolExpression 是让其子类来实现，因此 interpreter 是一个默认实现 @Override public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; return 0; &#125;&#125;public class SubExpression extends SymbolExpression &#123; public SubExpression(Expression left, Expression right) &#123; super(left, right); &#125; // 求出 left 和 right 表达式相减后的结果 public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; return super.left.interpreter(var) - super.right.interpreter(var); &#125;&#125;public class AddExpression extends SymbolExpression &#123; public AddExpression(Expression left, Expression right) &#123; super(left, right); &#125; // &#123;a=10,b=20&#125; public int interpreter(HashMap&lt;String, Integer&gt; var) &#123; //super.left.interpreter(var) ： 返回 left 表达式对应的值 a = 10 //super.right.interpreter(var): 返回 right 表达式对应值 b = 20 return super.left.interpreter(var) + super.right.interpreter(var); &#125;&#125; 状态模式主要用来解决对象在多种状态转换时，需要对外输出不同的行为的问题。状态和行为是相对应的，状态之间可以相互转换。当一个对象的内在状态改变时，允许改变其行为，这个对象看起来像是改变了其类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181public class ClientTest &#123; public static void main(String[] args) &#123; // 创建活动对象，奖品有 1 个奖品 RaffleActivity activity = new RaffleActivity(1); // 我们连续抽 300 次奖 for (int i = 0; i &lt; 30; i++) &#123; System.out.println(\"---第\" + (i + 1) + \"次抽奖---\"); // 参加抽奖，第一步点击扣除积分 activity.debuctMoney(); // 第二步抽奖 activity.raffle(); &#125; &#125;&#125;// 抽奖活动// 传入不同的状态进行调用public class RaffleActivity &#123; // state 表示活动当前的状态，是变化 State state = null; // 奖品数量 int count = 0; // 四个属性，表示活动的四种状态 State noRafflleState = new NoRaffleState(this); State canRaffleState = new CanRaffleState(this); State dispenseState = new DispenseState(this); State dispensOutState = new DispenseOutState(this); //构造器 // 1.初始化当前的状态为 noRafflleState（即不能抽奖的状态） // 2.初始化奖品的数量 public RaffleActivity(int count) &#123; this.state = getNoRafflleState(); this.count = count; &#125; // 扣分, 调用当前状态的 deductMoney public void debuctMoney()&#123; state.deductMoney(); &#125; // 抽奖 public void raffle()&#123; // 如果当前的状态是抽奖成功 if(state.raffle())&#123; // 领取奖品 state.dispensePrize(); &#125; &#125; // Getter / Setter // 每领取一次奖品，奖品数量减一 public int getCount() &#123; int curCount = count; count--; return curCount; &#125;&#125;// 抽奖人状态抽象类public abstract class State &#123; // 扣除积分 - 50 public abstract void deductMoney(); // 是否抽中奖品 public abstract boolean raffle(); // 发放奖品 public abstract void dispensePrize();&#125;// 可以抽奖的状态：重写各步骤public class CanRaffleState extends State &#123; RaffleActivity activity; public CanRaffleState(RaffleActivity activity) &#123; this.activity = activity; &#125; // 已经扣除了积分，不能再扣 @Override public void deductMoney() &#123; System.out.println(\"已经扣取过了积分\"); &#125; // 可以抽奖, 抽完奖后，根据实际情况，改成新的状态 @Override public boolean raffle() &#123; System.out.println(\"正在抽奖，请稍等！\"); Random r = new Random(); int num = r.nextInt(10); // 10% 中奖机会 if(num == 0)&#123; // 改变活动状态为发放奖品 context activity.setState(activity.getDispenseState()); return true; &#125;else&#123; System.out.println(\"很遗憾没有抽中奖品！\"); // 改变状态为不能抽奖 activity.setState(activity.getNoRafflleState()); return false; &#125; &#125; // 不能发放奖品 @Override public void dispensePrize() &#123; System.out.println(\"没中奖，不能发放奖品\"); &#125;&#125;// 奖品发送完毕状态public class DispenseOutState extends State &#123; // 初始化时传入活动引用 RaffleActivity activity; public DispenseOutState(RaffleActivity activity) &#123; this.activity = activity; &#125; @Override public void deductMoney() &#123; System.out.println(\"奖品发送完了，请下次再参加\"); &#125; @Override public boolean raffle() &#123; System.out.println(\"奖品发送完了，请下次再参加\"); return false; &#125; @Override public void dispensePrize() &#123; System.out.println(\"奖品发送完了，请下次再参加\"); &#125;&#125;// 发送奖品状态public class DispenseState extends State &#123; // 初始化时传入活动引用，发放奖品后改变其状态 RaffleActivity activity; public DispenseState(RaffleActivity activity) &#123; this.activity = activity; &#125; @Override public void deductMoney() &#123; System.out.println(\"不能扣除积分\"); &#125; @Override public boolean raffle() &#123; System.out.println(\"不能抽奖\"); return false; &#125; // 发放奖品 @Override public void dispensePrize() &#123; if(activity.getCount() &gt; 0)&#123; System.out.println(\"恭喜中奖了\"); // 改变状态为不能抽奖 activity.setState(activity.getNoRafflleState()); &#125;else&#123; System.out.println(\"很遗憾，奖品发送完了\"); // 改变状态为奖品发送完毕, 后面我们就不可以抽奖 activity.setState(activity.getDispensOutState()); // System.out.println(\"抽奖活动结束\"); // System.exit(0); &#125; &#125;&#125;// 不能抽奖的状态public class NoRaffleState extends State &#123; // 初始化时传入活动引用，扣除积分后改变其状态 RaffleActivity activity; public NoRaffleState(RaffleActivity activity) &#123; this.activity = activity; &#125; // 当前状态可以扣积分，扣除后，将状态设置成可以抽奖状态 @Override public void deductMoney() &#123; System.out.println(\"扣除 50 积分成功，您可以抽奖了\"); activity.setState(activity.getCanRaffleState()); &#125; // 当前状态不能抽奖 @Override public boolean raffle() &#123; System.out.println(\"扣了积分才能抽奖喔！\"); return false; &#125; // 当前状态不能发奖品 @Override public void dispensePrize() &#123; System.out.println(\"不能发放奖品\"); &#125;&#125; 策略模式分别封装行为接口，实现算法族，超类里放行为接口对象，在子类里具体设定行为对象。原则就是：分离变化部分，封装接口，基于接口编程各种功能。此模式让行为的变化独立于算法的使用者。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class Client &#123; public static void main(String[] args) &#123; WildDuck wildDuck = new WildDuck(); wildDuck.fly(); ToyDuck toyDuck = new ToyDuck(); toyDuck.fly(); PekingDuck pekingDuck = new PekingDuck(); pekingDuck.fly(); // 动态改变某个对象的行为, 北京鸭改为不能飞：改变策略 pekingDuck.setFlyBehavior(new NoFlyBehavior()); System.out.println(\"北京鸭的实际飞翔能力\"); pekingDuck.fly(); &#125;&#125;// 定义鸭的行为模式public abstract class Duck &#123; // 属性, 策略接口 FlyBehavior flyBehavior; // 其它属性 &lt;-&gt; 策略接口 QuackBehavior quackBehavior; public Duck() &#123; &#125; public abstract void display(); // 显示鸭子信息 public void quack() &#123; System.out.println(\"鸭子嘎嘎叫~~\"); &#125; public void swim() &#123; System.out.println(\"鸭子会游泳~~\"); &#125; public void fly() &#123; // 若改变策略 if(flyBehavior != null) &#123; flyBehavior.fly(); &#125; &#125; public void setFlyBehavior(FlyBehavior flyBehavior) &#123; this.flyBehavior = flyBehavior; &#125; public void setQuackBehavior(QuackBehavior quackBehavior) &#123; this.quackBehavior = quackBehavior; &#125;&#125;public class PekingDuck extends Duck &#123; // 假如北京鸭可以飞翔，但是飞翔技术一般 public PekingDuck() &#123; flyBehavior = new BadFlyBehavior(); &#125; @Override public void display() &#123; System.out.println(\"~~北京鸭~~~\"); &#125;&#125;public class ToyDuck extends Duck&#123; public ToyDuck() &#123; flyBehavior = new NoFlyBehavior(); &#125; @Override public void display() &#123; System.out.println(\"玩具鸭\"); &#125; // 需要重写父类的所有方法 public void quack() &#123; System.out.println(\"玩具鸭不能叫~~\"); &#125; public void swim() &#123; System.out.println(\"玩具鸭不会游泳~~\"); &#125;&#125;public class WildDuck extends Duck &#123; // 构造器，传入 FlyBehavor 的对象 public WildDuck() &#123; flyBehavior = new GoodFlyBehavior(); &#125; @Override public void display() &#123; System.out.println(\" 这是野鸭 \"); &#125;&#125;// 未实现策略public interface QuackBehavior &#123; void quack();//子类实现&#125;public interface FlyBehavior &#123; void fly(); // 子类具体实现&#125;public class GoodFlyBehavior implements FlyBehavior &#123; @Override public void fly() &#123; System.out.println(\"飞翔技术高超~~\"); &#125;&#125;public class NoFlyBehavior implements FlyBehavior&#123; @Override public void fly() &#123; System.out.println(\"不会飞翔...\"); &#125;&#125;public class BadFlyBehavior implements FlyBehavior &#123; @Override public void fly() &#123; System.out.println(\" 飞翔技术一般 \"); &#125;&#125; 策略模式的核心思想是：多用组合 / 聚合，少用继承；用行为类组合，而不是行为的继承，更有弹性。 责任链模式职责链模式（Chain of Responsibility Pattern）, 又叫责任链模式，为请求创建了一个接收者对象的链。这种模式对请求的发送者和接收者进行解耦 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112public class Client &#123; public static void main(String[] args) &#123; // 创建一个请求 PurchaseRequest purchaseRequest = new PurchaseRequest(1, 31000, 1); // 创建相关的审批人 DepartmentApprover departmentApprover = new DepartmentApprover(\"张主任\"); CollegeApprover collegeApprover = new CollegeApprover(\"李院长\"); ViceSchoolMasterApprover viceSchoolMasterApprover = new ViceSchoolMasterApprover(\"王副校\"); SchoolMasterApprover schoolMasterApprover = new SchoolMasterApprover(\"佟校长\"); // 需要将各个审批级别的下一个设置好 (处理人构成环形) // 各处理者处理的范围（任务类型）不同 departmentApprover.setApprover(collegeApprover); collegeApprover.setApprover(viceSchoolMasterApprover); viceSchoolMasterApprover.setApprover(schoolMasterApprover); schoolMasterApprover.setApprover(departmentApprover); // 分发任务 departmentApprover.processRequest(purchaseRequest); viceSchoolMasterApprover.processRequest(purchaseRequest); &#125;&#125;public abstract class Approver &#123; Approver approver; // 下一个处理者 String name; // 名字 public Approver(String name) &#123; this.name = name; &#125; // 下一个处理者 public void setApprover(Approver approver) &#123; this.approver = approver; &#125; // 处理审批请求的方法，得到一个请求, 处理是子类完成，因此该方法做成抽象 public abstract void processRequest(PurchaseRequest purchaseRequest);&#125;public class CollegeApprover extends Approver &#123; public CollegeApprover(String name) &#123; super(name); &#125; @Override public void processRequest(PurchaseRequest purchaseRequest) &#123; if(purchaseRequest.getPrice() &lt; 5000 &amp;&amp; purchaseRequest.getPrice() &lt;= 10000) &#123; System.out.println(\" 请求编号 id= \" + purchaseRequest.getId() + \" 被 \" + this.name + \" 处理\"); &#125;else &#123; approver.processRequest(purchaseRequest); &#125; &#125;&#125;public class DepartmentApprover extends Approver &#123; public DepartmentApprover(String name) &#123; super(name); &#125; @Override public void processRequest(PurchaseRequest purchaseRequest) &#123; if(purchaseRequest.getPrice() &lt;= 5000) &#123; System.out.println(\" 请求编号 id= \" + purchaseRequest.getId() + \" 被 \" + this.name + \" 处理\"); &#125;else &#123; approver.processRequest(purchaseRequest); &#125; &#125;&#125;public class SchoolMasterApprover extends Approver &#123; public SchoolMasterApprover(String name) &#123; super(name); &#125; @Override public void processRequest(PurchaseRequest purchaseRequest) &#123; if(purchaseRequest.getPrice() &gt; 30000) &#123; System.out.println(\" 请求编号 id= \" + purchaseRequest.getId() + \" 被 \" + this.name + \" 处理\"); &#125;else &#123; approver.processRequest(purchaseRequest); &#125; &#125;&#125;public class ViceSchoolMasterApprover extends Approver &#123; public ViceSchoolMasterApprover(String name) &#123; super(name); &#125; @Override public void processRequest(PurchaseRequest purchaseRequest) &#123; // TODO Auto-generated method stub if(purchaseRequest.getPrice() &lt; 10000 &amp;&amp; purchaseRequest.getPrice() &lt;= 30000) &#123; System.out.println(\" 请求编号 id= \" + purchaseRequest.getId() + \" 被 \" + this.name + \" 处理\"); &#125;else &#123; approver.processRequest(purchaseRequest); &#125; &#125;&#125;// 请求类public class PurchaseRequest &#123; private int type = 0; // 请求类型 private float price = 0.0f; // 请求金额 private int id = 0; // 构造器 public PurchaseRequest(int type, float price, int id) &#123; this.type = type; this.price = price; this.id = id; &#125; public int getType() &#123; return type; &#125; public float getPrice() &#123; return price; &#125; public int getId() &#123; return id; &#125;&#125;","categories":[{"name":"技术原理","slug":"技术原理","permalink":"http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Python 爬虫","slug":"Python/Python 爬虫","date":"2020-03-16T07:09:15.000Z","updated":"2020-07-10T04:19:26.026Z","comments":true,"path":"2020/03/16/Python/Python 爬虫/","link":"","permalink":"http://yoursite.com/2020/03/16/Python/Python%20%E7%88%AC%E8%99%AB/","excerpt":"Python 爬虫相关的知识。","text":"Python 爬虫相关的知识。 Urllib 库12345import urllib2 request = urllib2.Request(\"http://www.baidu.com\")response = urllib2.urlopen(request)print response.read() Post &amp; Get123456789import urllibimport urllib2 values = &#123;\"username\":\"1016903103@qq.com\",\"password\":\"XXXX\"&#125;data = urllib.urlencode(values) url = \"https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn\"request = urllib2.Request(url,data)response = urllib2.urlopen(request)print response.read() 123456789101112import urllibimport urllib2 values=&#123;&#125;values['username'] = \"1016903103@qq.com\"values['password']=\"XXXX\"data = urllib.urlencode(values) url = \"http://passport.csdn.net/account/login\"geturl = url + \"?\"+datarequest = urllib2.Request(geturl)response = urllib2.urlopen(request)print response.read() 构建消息头1234567891011import urllib import urllib2 url = 'http://www.server.com/login'user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' values = &#123;'username' : 'cqc', 'password' : 'XXXX' &#125; headers = &#123; 'User-Agent' : user_agent &#125; data = urllib.urlencode(values) request = urllib2.Request(url, data, headers) response = urllib2.urlopen(request) page = response.read() Proxy 设置12345678910import urllib2enable_proxy = Trueproxy_handler = urllib2.ProxyHandler(&#123;\"http\" : 'http://some-proxy.com:8080'&#125;)null_proxy_handler = urllib2.ProxyHandler(&#123;&#125;)if enable_proxy: opener = urllib2.build_opener(proxy_handler)else: opener = urllib2.build_opener(null_proxy_handler)urllib2.install_opener(opener) 爬取 TED 文章1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding: utf-8 -*import codecsimport bs4from bs4 import BeautifulSoupimport requestsimport re# 用户输入网址，例子：https://www.ted.com/talks/Xxx_Xxx_X...input_url = raw_input('Please Enter Article URL: ')url = input_url + '/transcript'# 获取文章标题url_str = url.split('/')title_ = url_str[4]title_str = title_.split('_')title = ''author = ''num = 0for str in title_str: num = num + 1 if(num &gt; 2): if(num == 3): str = str.capitalize() title = title + str + ' ' else: author = author + str.capitalize()print \"Title:\" + titleprint \"Author:\" + author# 获取文章html = requests.get(url).content soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')divs = soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\")article = title + '\\n'rude_t = re.compile(r'\\t')rude_n = re.compile(r'\\n')for div in divs: for string in div.stripped_strings: string = rude_t.sub('', string) string = rude_n.sub(' ', string) article = article + string article = article + '\\n'article = article + '\\n'file = codecs.open(\"TED.txt\",\"a\",\"utf-8\")file.write(article)file.close()print \"Finish!\"","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"Python 基础","slug":"Python/Python 基础","date":"2020-03-16T06:00:32.000Z","updated":"2020-07-12T08:32:02.783Z","comments":true,"path":"2020/03/16/Python/Python 基础/","link":"","permalink":"http://yoursite.com/2020/03/16/Python/Python%20%E5%9F%BA%E7%A1%80/","excerpt":"Python 入门。","text":"Python 入门。 Python 基础输入和输出12name = raw_input('please enter your name: ') # 输入为字符串形式 int()print 'hello,', name 字符串转义字符\\可以转义很多字符，比如\\n表示换行，\\t表示制表符，字符\\本身也要转义，所以\\\\表示的字符就是\\，如果字符串里面有很多字符都需要转义，就需要加很多\\，为了简化，Python还允许用r&#39;&#39;表示&#39;&#39;内部的字符串默认不转义。 如果字符串内部有很多换行，用\\n写在一行里不好阅读，为了简化，Python允许用&#39;&#39;&#39;...&#39;&#39;&#39;的格式表示多行内容。 12345678print '''line1... line2... line3'''## 输出结果# line1# line2# line3 占位符 占位符 %d 整数 %f 浮点数 %s 字符串 %x 十六进制整数 12'Hi, %s, you have $%d.' % ('Michael', 1000000)# 'Hi, Michael, you have $1000000.' 布尔值布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来。 布尔值可以用and、or和not运算。 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 变量变量在程序中就是用一个变量名表示了，变量名必须是大小写英文、数字和_的组合，且不能用数字开头。 1234567891011a = 'ABC' # 在内存中创建了一个 ABC 的字符串；在内存中创建了一个名为 a 的变量，并把它指向 ABC 字符串b = aa = 'XYZ'print b # ABC 字符串&gt;&gt;&gt; a = 'abc'&gt;&gt;&gt; b = a.replace('a', 'A')&gt;&gt;&gt; b'Abc'&gt;&gt;&gt; a'abc' list 和 tuplePython 内置的一种数据类型是列表：list。list 是一种有序的集合，可以随时添加和删除其中的元素。 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; len(classmates)3&gt;&gt;&gt; classmates[-1]'Tracy'&gt;&gt;&gt; classmates.append('Adam')&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy', 'Adam']&gt;&gt;&gt; classmates.insert(1, 'Jack')&gt;&gt;&gt; classmates['Michael', 'Jack', 'Bob', 'Tracy', 'Adam']&gt;&gt;&gt; classmates.pop()'Adam'&gt;&gt;&gt; classmates['Michael', 'Jack', 'Bob', 'Tracy']&gt;&gt;&gt; classmates.pop(1)'Jack'&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; p = ['asp', 'php']&gt;&gt;&gt; s = ['python', 'java', p, 'scheme'] tuple 另一种有序列表叫元组：tuple。tuple 和 list 非常类似，但是 tuple 一旦初始化就不能修改。 1&gt;&gt;&gt; classmates = ('Michael', 'Bob', 'Tracy') 错误用法 12345&gt;&gt;&gt; t = ('a', 'b', ['A', 'B'])&gt;&gt;&gt; t[2][0] = 'X'&gt;&gt;&gt; t[2][1] = 'Y'&gt;&gt;&gt; t('a', 'b', ['X', 'Y']) # list 是可以变的 条件判断和循环12345678if &lt;条件判断1&gt;: &lt;执行1&gt;elif &lt;条件判断2&gt;: &lt;执行2&gt;elif &lt;条件判断3&gt;: &lt;执行3&gt;else: &lt;执行4&gt; 1234sum = 0for x in range(101): sum = sum + xprint sum 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print sum dict 和 setPython 内置了字典：dict 的支持，dict 全称 dictionary，在其他语言中也称为 map，使用键-值（key-value）存储，具有极快的查找速度。 dict 可以用在需要高速查找的很多地方，在 Python 代码中几乎无处不在，正确使用 dict 非常重要，需要牢记的第一条就是 dict 的 key 必须是不可变对象。 123456789101112&gt;&gt;&gt; d['Adam'] = 67&gt;&gt;&gt; d['Adam']67&gt;&gt;&gt; 'Thomas' in dFalse&gt;&gt;&gt; d.get('Thomas')&gt;&gt;&gt; d.get('Thomas', -1)-1# pop('key') set 和 dict 类似，也是一组 key 的集合，但不存储 value。由于 key 不能重复，所以，在 set 中，没有重复的 key。 1234567891011121314&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; sset([1, 2, 3])&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; sset([1, 2, 3, 4])&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; sset([1, 2, 3, 4])&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; sset([1, 2, 3]) 函数内置常用函数在交互式命令行通过help(abs)查看abs函数的帮助信息。 12345678910111213&gt;&gt;&gt; abs(100)100&gt;&gt;&gt; abs(-20)20&gt;&gt;&gt; abs(12.34)12.34&gt;&gt;&gt; cmp(1, 2)-1&gt;&gt;&gt; cmp(2, 1)1&gt;&gt;&gt; cmp(3, 3)0 数据类型转换 1234567891011121314&gt;&gt;&gt; int('123')123&gt;&gt;&gt; int(12.34)12&gt;&gt;&gt; float('12.34')12.34&gt;&gt;&gt; str(1.23)'1.23'&gt;&gt;&gt; unicode(100)u'100'&gt;&gt;&gt; bool(1)True&gt;&gt;&gt; bool('')False 定义函数如果没有return语句，函数执行完毕后也会返回结果，只是结果为None。 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError('bad operand type') if x &gt;= 0: return x else: return -x 12def nop(): pass # 作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个 pass，让代码能运行起来 1234567891011import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print r(151.96152422706632, 70.0) # 返回值是一个tuple，tuple 可以省略括号，而多个变量可以同时接收一个 tuple，按位置赋给对应的值 函数的参数12345678910# 大多数学生注册时不需要提供年龄和城市，只提供必须的两个参数def enroll(name, gender, age=6, city='Beijing'): print 'name:', name print 'gender:', gender print 'age:', age print 'city:', city # 只有与默认参数不符的学生才需要提供额外的信息enroll('Bob', 'M', 7)enroll('Adam', 'M', city='Tianjin') *args是可变参数，args 接收的是一个 tuple； **kw是关键字参数，kw 接收的是一个 dict。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"开发常用工具类","slug":"后台技术/开发常用工具类","date":"2020-03-12T07:31:13.000Z","updated":"2020-07-10T02:57:49.009Z","comments":true,"path":"2020/03/12/后台技术/开发常用工具类/","link":"","permalink":"http://yoursite.com/2020/03/12/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/","excerpt":"记录一下开发中用到的各种小工具类。","text":"记录一下开发中用到的各种小工具类。 加密类Md512345678910111213141516171819202122232425262728293031323334353637383940414243import java.security.MessageDigest;import org.apache.commons.lang3.StringUtils;public class Md5Util &#123; private Md5Util() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** * md5 加密通用工具类 * * @param encode 需加密的字符串 * @return md5 加密后的字符串（32位字符串） */ public static String md5(String encode) &#123; // 检验参数是否是 null 或者 \"\" 或者 \" \" 等 if (StringUtils.isBlank(encode)) &#123; return \"\"; &#125; char[] hexDigits = &#123;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'&#125;; try &#123; // 设置字符集 byte[] strTemp = encode.getBytes(StandardCharsets.UTF_8); // 生成一个 md5 加密计算摘要 MessageDigest mdTemp = MessageDigest.getInstance(\"MD5\"); // 计算 md5 函数 mdTemp.update(strTemp); // digest() 最后确定返回 md5 hash 值，返回值为 8 位字符串。因为 md5 hash 值是 16 位的 hex 值，实际上就是 8 位的字符 byte[] md = mdTemp.digest(); int j = md.length; char[] str = new char[j * 2]; int k = 0; for (byte byte0 : md) &#123; // 不带符号右移四位(不管 byte0 的类型 位移处补 0)，&amp; 十六进制的 f 即 高四位清空 取低四位的值，&gt;&gt;&gt; 优先级高于 &amp; str[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; str[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(str); &#125; catch (Exception e) &#123; return \"\"; &#125; &#125;&#125; JsonFastJson12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class FastJsonConvertUtil &#123; private static final SerializerFeature[] featuresWithNullValue = &#123; SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullBooleanAsFalse, SerializerFeature.WriteNullListAsEmpty, SerializerFeature.WriteNullNumberAsZero, SerializerFeature.WriteNullStringAsEmpty &#125;; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSON字符串转换为实体对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSON字符串转换为实体对象&lt;BR&gt; * @param data JSON字符串 * @param clzss 转换对象 * @return T */ public static &lt;T&gt; T convertJSONToObject(String data, Class&lt;T&gt; clzss) &#123; try &#123; T t = JSON.parseObject(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSONObject对象转换为实体对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSONObject对象转换为实体对象&lt;BR&gt; * @param data JSONObject对象 * @param clzss 转换对象 * @return T */ public static &lt;T&gt; T convertJSONToObject(JSONObject data, Class&lt;T&gt; clzss) &#123; try &#123; T t = JSONObject.toJavaObject(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSON字符串数组转为List集合对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSON字符串数组转为List集合对象&lt;BR&gt; * @param data JSON字符串数组 * @param clzss 转换对象 * @return List&lt;T&gt;集合对象 */ public static &lt;T&gt; List&lt;T&gt; convertJSONToArray(String data, Class&lt;T&gt; clzss) &#123; try &#123; List&lt;T&gt; t = JSON.parseArray(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将List&lt;JSONObject&gt;转为List集合对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将List&lt;JSONObject&gt;转为List集合对象&lt;BR&gt; * @param data List&lt;JSONObject&gt; * @param clzss 转换对象 * @return List&lt;T&gt;集合对象 */ public static &lt;T&gt; List&lt;T&gt; convertJSONToArray(List&lt;JSONObject&gt; data, Class&lt;T&gt; clzss) &#123; try &#123; List&lt;T&gt; t = new ArrayList&lt;T&gt;(); for (JSONObject jsonObject : data) &#123; t.add(convertJSONToObject(jsonObject, clzss)); &#125; return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将对象转为JSON字符串&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将对象转为JSON字符串&lt;BR&gt; * @param obj 任意对象 * @return JSON字符串 */ public static String convertObjectToJSON(Object obj) &#123; try &#123; String text = JSON.toJSONString(obj); return text; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将对象转为JSONObject对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将对象转为JSONObject对象&lt;BR&gt; * @param obj 任意对象 * @return JSONObject对象 */ public static JSONObject convertObjectToJSONObject(Object obj)&#123; try &#123; JSONObject jsonObject = (JSONObject) JSONObject.toJSON(obj); return jsonObject; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;&lt;BR&gt; * @param obj * @return */ public static String convertObjectToJSONWithNullValue(Object obj) &#123; try &#123; String text = JSON.toJSONString(obj, featuresWithNullValue); return text; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"开发工具","slug":"开发工具","permalink":"http://yoursite.com/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}]},{"title":"用户管理系统","slug":"项目开发/用户管理系统","date":"2020-03-07T08:44:21.000Z","updated":"2020-07-10T03:53:02.284Z","comments":true,"path":"2020/03/07/项目开发/用户管理系统/","link":"","permalink":"http://yoursite.com/2020/03/07/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/","excerpt":"基于 Spring Boot 的用户管理系统。","text":"基于 Spring Boot 的用户管理系统。 准备工作功能模块分析： 数据库表分析： 数据库建表语句： 1234567891011CREATE TABLE `user_manage_demo` ( `user_id` bigint(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键id，用户id', `username` varchar(50) NOT NULL COMMENT '用户名', `password` varchar(100) NOT NULL COMMENT '用户密码', `type` char(1) unsigned NOT NULL COMMENT '用户类型（1：管理员；0：普通用户）', `gmt_create` datetime NOT NULL COMMENT '记录创建时间', `gmt_modified` datetime NOT NULL COMMENT '记录修改时间', `is_delete` tinyint(1) unsigned NOT NULL DEFAULT '0' COMMENT '记录是否逻辑删除（0：未删除；1：已删除）', PRIMARY KEY (`user_id`), UNIQUE KEY `uk_name_type` (`username`,`type`) USING BTREE COMMENT '用户名称、类型唯一索引') ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb4 COMMENT='用户管理表'; 在数据库表中初始化一条管理员数据： 1234INSERT INTO `springboot_user`.`user` (`user_id`, `username`, `password`, `type`, `gmt_create`, `gmt_modified`, `is_delete`) VALUES (1, 'admin', '123456', \"1\", '2019-05-19 20:26:22', '2019-05-19 20:26:27', 0); 接口的设计 为了项目代码更加简洁，此项目添加了 Lombok 支持依赖并且 IDEA 添加了相应的 Lombok 插件，通过在类注释@Data为相应的类添加Getter/Setter，toString()，equals() 等方法。 123456&lt;!-- 添加 Lombok 的支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 将前端传输过来的的数据封装在对应的 DTO 类中，用于展示层与服务层之间的数据传输对象。（View 👉 Action 👉 Srvice） DTO：Data Transfer Object（数据传输对象）DTO 是一组需要跨进程或网络边界传输的聚合数据的简单容器。它不应该包含业务逻辑，并将其行为限制为诸如内部一致性检查和基本验证之类的活动。 将返回前端的数据封装在对应的 VO 类中。 VO：View Object（视图对象）VO 的作用是把某个指定页面（或组件）的所有数据封装起来。 DTO 类的编写在 DTO 类中添加 JSR303 校验。 JSR303 是一项标准，JSR-349 是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如 @Null，@NotNull，@Pattern，在 javax.validation.constraints 包下，只提供规范不提供实现。 Hibernat Validation 是对这个规范的实践（不要将 Hibernate 和数据库 ORM 框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如@Email，@Length，@Range等等，在 org.hibernate.validator.constraints 包下。 Spring 为了给开发者提供便捷，对 Hibernate Validation 进行了二次封装，显示校验 Validated Bean 时，你可以使用 Spring validation 或者 Hibernate validation，而 Spring Validation 另一个特性，便是其在 Spring MVC 模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的 web 开发。 12345678910111213141516171819202122232425/*** DTO 类示例* @NotBlank 用于 String（会去掉空格）* @NotNull 用于 Integer* @Data 自动生成 Getter/Setter，toString()，equals() 等方法*/@Datapublic class LoginDTO implements Serializable &#123; private static final long serialVersionUID = 1L; /** 用户名 */ @NotBlank(message = \"用户名不能为空\") private String username; /** 用户密码 */ @NotBlank(message = \"用户密码不能为空\") private String password; /** 验证码 */ @NotBlank(message = \"验证码不能为空\") private String checkCode; /** 类型 */ @NotNull(message = \"用户类型不能为空\") private String type;&#125; VO 类的编写 12345678910111213141516171819/*** VO 类示例* 管理员用户列表页面的用户信息返回对象*/@Datapublic class ViewVO implements Serializable &#123; private static final long serialVersionUID = 1L; /** 用户标识 */ private Integer userId; /** 用户名 */ private String username; /** 用户类型 */ private String type; /** 用户创建时间 */ private Date gtmCreate;&#125; 静态常量类将项目中一些固定的字面量定义为不可变的静态常量public static final。 UrlConstant123456789101112131415161718192021222324/*** URL 静态常量类*/public final class UrlConstant &#123; /** 添加私有的构造函数，防止静态常量类构造其它的实例，然后改变静态常量的值 */ private UrlConstant() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 跳转首页 URL */ public static final String INDEX = \"/index\"; /** 跳转注册 URL */ public static final String REGISTER = \"/register\"; /** 跳转登录 URL */ public static final String LOGIN = \"/login\"; /** 跳转查看用户列表 URL */ public static final String VIEW = \"/view\"; /** 跳转新增用户 URL */ public static final String USER = \"/user\"; /** 跳转修改用户信息 URL */ public static final String MODIFY = \"/modify\";&#125; UserTypeConstant12345678910111213141516171819202122232425262728293031/*** 用户类型静态常量类*/public final class UserTypeConstant &#123; /** 添加私有的构造函数，防止静态常量类构造其它的实例，然后改变静态常量的值 */ private UserTypeConstant() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 管理员用户 */ public static final String ADMIN = 1; /** 普通用户 */ public static final String ORDINARY = 0; /** * 用户类型代码转描述 * * @param type 用户类型 * @return 用户类型中文描述 */ public static String typeToDesc(Sting type) &#123; if (UserTypeConstant.ADMIN.equals(type)) &#123; return \"管理员\"; &#125; else if (UserTypeConstant.ORDINARY.equals(type)) &#123; return \"普通用户\"; &#125; else &#123; return \"未知身份，请联系系统管理员\"; &#125; &#125; ImageCodeConstant123456789101112131415161718192021222324252627public final class ImageCodeConstant &#123; /** 添加私有的构造函数，防止静态常量类构造其它的实例，然后改变静态常量的值 */ private ImageCodeConstant() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 验证码生成的时间 */ public static final String CODE_TIME = \"codeTime\"; /** 验证码过期时间，单位：分钟 */ public static final int CAPTCHA_EXPIRY_TIME = 5; /** 秒/毫秒换算 */ public static final int MILLISECOND_CONVERSION = 1000; /** 分钟/秒换算 */ public static final int MINUTE_CONVERSION = 60; /** 生成验证码的变量名称 */ public static final String SIMPLE_CAPTCHA = \"simpleCaptcha\"; /** 生成验证码的变量值 */ public static final String CAPTCHA_VALUE = \"captchaValue\"; /** 生成的验证码图形的变量名 */ public static final String IMAGE = \"image\"; /** 生成的图形验证码文件格式化的名称 */ public static final String JPEG = \"JPEG\"; /** 图片字符涉及的罗马字体名 */ public static final String TIMES_NEW_ROMAN = \"Times New Roman\";&#125; RespoonseConstant12345678910111213141516171819202122public final class ResponseConstant &#123; /** 添加私有的构造函数，防止静态常量类构造其它的实例，然后改变静态常量的值 */ private ResponseConstant() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 处理成功时返回的 Code */ public static final String SUCCESS = \"200\"; /** 业务处理失败时返回的 Code */ public static final String FAIL = \"-1\"; /** 返回信息的 Key 常量值 */ public static final String RESPONSE_VO = \"responseVO\"; /** 返回状态码常量值 */ public static final String CODE = \"code\"; /** 返回状态信息常量值 */ public static final String MESSAGE = \"message\"&#125; UserConstant1234567891011121314public final class UserConstant &#123; /** 添加私有的构造函数，防止静态常量类构造其它的实例，然后改变静态常量的值 */ private UserConstant() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** Session 中的用户名常量的字段名称，多处使用统一在这里定义，避免 SonarLint 魔法值的告警 */ public static final String USERNAME = \"username\"; /** Session 中的用户类型常量的字段名称，多处使用统一在这里定义，避免 SonarLint 魔法值的告警 */ public static final String TYPE = \"type\"; /** Session 中的用户类型名称常量的字段名称，多处使用统一在这里定义，避免 SonarLint 魔法值的告警 */ public static final String TYPE_NAME = \"typeName\";&#125; 工具类MD5 密码加密1234567891011121314151617181920212223242526272829303132333435363738394041public class Md5Util &#123; private Md5Util() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** * md5 加密通用工具类 * * @param encode 需加密的字符串 * @return md5 加密后的字符串（32位字符串） */ public static String md5(String encode) &#123; // 检验参数是否是 null 或者 \"\" 或者 \" \" 等 if (StringUtils.isBlank(encode)) &#123; return \"\"; &#125; char[] hexDigits = &#123;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'&#125;; try &#123; // 设置字符集 byte[] strTemp = encode.getBytes(StandardCharsets.UTF_8); // 生成一个 md5 加密计算摘要 MessageDigest mdTemp = MessageDigest.getInstance(\"MD5\"); // 计算 md5 函数 mdTemp.update(strTemp); // digest() 最后确定返回 md5 hash 值，返回值为 8 位字符串。因为 md5 hash 值是 16 位的 hex 值，实际上就是 8 位的字符 byte[] md = mdTemp.digest(); int j = md.length; char[] str = new char[j * 2]; int k = 0; for (byte byte0 : md) &#123; // 不带符号右移四位(不管 byte0 的类型 位移处补 0)，&amp; 十六进制的 f 即 高四位清空 取低四位的值，&gt;&gt;&gt; 优先级高于 &amp; str[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; str[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(str); &#125; catch (Exception e) &#123; return \"\"; &#125; &#125;&#125; 验证码生成123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class ImageCodeUtil &#123; private ImageCodeUtil() &#123; throw new IllegalStateException(\"Utility class\"); &#125; /** 生成验证码图片的默认宽度 */ private static final int IMAGE_WIDTH = 60; /** 生成验证码图片的默认高度 */ private static final int IMAGE_HEIGHT = 20; /** 随机干扰线的条数，太稀疏或者太密织都达不到干扰的效果 */ private static final int RAND_COLOR = 168; /** 颜色数值的上限，三原色的范围都是 [0, 255] 范围 */ private static final int COLOR_MAX = 255; /** 可根据需要来修改，生成纯数字或者纯字母，或者默认这种混合的 */ private static char[] mapTable = &#123;'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'&#125;; /** * 生成图形验证码 * * @param width 生成的图形验证码的宽度 * @param height 生成的图形验证码的高度 * @param verifySize 需要生成的验证码位数 * @return 验证码的图片对象及验证码的值 */ public static Map&lt;String, Object&gt; getImageCode(int width, int height, int verifySize) throws NoSuchAlgorithmException &#123; Map&lt;String, Object&gt; returnMap = new HashMap&lt;&gt;(6); // 确保验证码的宽度和高度不能小于或等于零 width = width &lt;= 0 ? IMAGE_WIDTH : width; height = height &lt;= 0 ? IMAGE_HEIGHT : height; // BufferedImage 是 Image 的一个子类，Image 和 BufferedImage // 主要作用就是将一副图片加载到内存中，BufferedImage 生成的图片在内存里有一个图像缓冲区 // 利用这个缓冲区我们可以很方便的操作这个图片，通常用来做图片修改操作如大小变换、图片变灰、设置图片透明或不透明等 BufferedImage image = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); // 获取图形上下文 Graphics g = image.getGraphics(); // 生成随机类 Random random = SecureRandom.getInstanceStrong(); // 设定背景色 g.setColor(getRandColor(200, 250)); // 画长方形 g.fillRect(0, 0, width, height); // 设定字体 g.setFont(new Font(ImageCodeConstant.TIMES_NEW_ROMAN, Font.PLAIN, 18)); // 再次设置背景色，实测跟前一次设置背景色有叠加的效果，会让背景颜色看起来更复杂 g.setColor(getRandColor(160, 200)); // 随机产生 168 条干扰线，使图象中的认证码不易被其它程序探测到 for (int i = 0; i &lt; RAND_COLOR; i++) &#123; // nextInt(n) 会返回 [0, n) 之间的随机数 int x = random.nextInt(width); int y = random.nextInt(height); int xl = random.nextInt(12); int yl = random.nextInt(12); // x: 线段起点的横坐标；y: 线段起点的纵坐标；x + xl: 线段终点的横坐标；y + yl: 线段终点的纵坐标 g.drawLine(x, y, x + xl, y + yl); &#125; // 定义验证码的值 StringBuilder captchaValue = new StringBuilder(); // 4代表验证码位数，如果要生成更多位的认证码，则加大数值 for (int i = 0; i &lt; verifySize; ++i) &#123; captchaValue.append(mapTable[random.nextInt(mapTable.length)]); // 将认证码显示到图象中 g.setColor(new Color(20 + random.nextInt(110), 20 + random.nextInt(110), 20 + random.nextInt(110))); // 直接生成 String str = captchaValue.substring(i, i + 1); // 使用此图形上下文的当前字体和颜色绘制由指定 string 给定的文本 g.drawString(str, 13 * i + 6, 16); &#125; // 释放图形上下文资源 g.dispose(); // 组装图像结果及验证码值并返回 returnMap.put(ImageCodeConstant.IMAGE, image); returnMap.put(ImageCodeConstant.CAPTCHA_VALUE, captchaValue.toString()); return returnMap; &#125; /** 给定范围获得随机颜色 */ private static Color getRandColor(int fc, int bc) throws NoSuchAlgorithmException &#123; // 创建一个随机数生成对象的实例 Random random = SecureRandom.getInstanceStrong(); // 颜色值不能超过最大范围 fc = fc &gt; COLOR_MAX ? COLOR_MAX : fc; bc = bc &gt; COLOR_MAX ? COLOR_MAX : bc; // nextInt(n) 会返回 [0, n) 之间的随机数 int r = fc + random.nextInt(bc - fc); int g = fc + random.nextInt(bc - fc); int b = fc + random.nextInt(bc - fc); // 按随机生成的 r、g、b 值设置颜色并返回 return new Color(r, g, b); &#125;&#125; 实体类实体类的属性与数据库的字段相对应。 12345678910111213141516171819202122@Datapublic class UserMybatis &#123; private static final long serialVersionUID = 1L; /** 用户 id */ private Integer userId; /** 用户名 */ private String username; /** 用户密码 */ private String password; /** 用户类型 */ private String type; /** 记录创建时间 */ @DateTimeFormat(pattern = \"yyyy-MM-dd HH:mm:ss\") private Date gmtCreate; /** 记录修改时间 */ @DateTimeFormat(pattern = \"yyyy-MM-dd HH:mm:ss\") private Date gmtModified; /** 记录是否删除 */ private Boolean isDelete;&#125; 数据源配置数据源： 12345678# 数据源配置# serverTimezone=Asia/Shanghai 这种写法也可以# SSL 连接必须显示的进行配置是否使用，否则会报警告# 使用 MySQL80 服务 mysql-connector-java:8.0.19 版本的连接 Jar 包 DriverClass：com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/springboot_user?serverTimezone=GMT%2B8&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=falsespring.datasource.username=rootspring.datasource.password=123456spring.datasource.driverClassName=com.mysql.cj.jdbc.Driver 测试数据源是否配置成功： 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestclass JavaSpringWingoApplicationTests &#123; // 测试 DataSource 是否被成功注入 @Autowired DataSource dataSource; @Test void testDataSource() &#123; System.out.println(dataSource.getClass()); // 输出结果 class com.zaxxer.hikari.HikariDataSource Spring Boot 2 的默认连接池 Hikari &#125;&#125; Druid 连接池1234567891011121314&lt;!-- 支持阿里 Druid 连接池 --&gt;&lt;!-- 方式一：使用 Spring Boot 提供的启动器 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 方式二：此方式需要自行编写 Druid 配置类引入配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;!-- &lt;version&gt;1.1.16&lt;/version&gt; 此版本找不到依赖 --&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt; Druid 配置123456789101112131415161718192021222324252627282930313233343536373839404142## Druid连接池配置spring.datasource.type=com.alibaba.druid.pool.DruidDataSource## Druid 连接池配置spring.datasource.type=com.alibaba.druid.pool.DruidDataSource# 初始化大小、最小、最大spring.datasource.initialSize=1spring.datasource.minIdle=3spring.datasource.maxActive=20# 获取连接等待超时时间spring.datasource.maxWait=60000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.timeBetweenEvictionRunsMillis=30000# 指定获取连接时连接校验的sql查询语句spring.datasource.validationQuery=select 'x'# 获取连接后，确实是否要进行连接空闲时间的检查spring.datasource.testWhileIdle=true# 获取连接检测spring.datasource.testOnBorrow=false# 归还连接检测spring.datasource.testOnReturn=false# 指定连接校验查询的超时时间spring.datasource.validationQueryTimeout=600000# 配置一个连接在池总最小生存的时间spring.datasource.minEvictableIdleTimeMillis=300000# 打开 PSCache，并且指定每个连接上 Cache 的大小spring.datasource.poolPreparedStatements=truespring.datasource.maxPoolPreparedStatementPerConnectionSize=20# 配置监控统计拦截的 filter，去掉后监控界面 sql 无法统计，wall 用于防火墙，stat 用于监控统计spring.datasource.filters=stat,wall,slf4j# 通过 connectProperties 属性来打开 mergeSql 功能，慢 sql 记录等spring.datasource.connectionProperties=druid.stat.mergeSql=true;druid.stat.logSlowSql=true;druid.stat.slowSqlMillis=5000## druid连接池监控 访问 URL：locahost:8080/druid # 需要账号密码才能访问控制台，默认为rootspring.datasource.druid.stat-view-servlet.login-username=adminspring.datasource.druid.stat-view-servlet.login-password=123# 访问路径为/druid时，跳转到StatViewServletspring.datasource.druid.stat-view-servlet.url-pattern=/druid/*# 是否能够重置数据spring.datasource.druid.stat-view-servlet.reset-enable=false# 排除一些静态资源，以提高效率spring.datasource.druid.web-stat-filter.exclusions=*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/* 再次测试控制台输出的 DataSource 的类型为 com.alibaba.druid.pool.DruidDataSource。 整合 Mybatis123456&lt;!-- 支持 MyBatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 配置 Mapper123456## MyBatis 配置# 配置别名 parameterType 直接使用类名mybatis.typeAliasesPackage=com.wingo.user.entitymybatis.mapperLocations=classpath:mapping/*.xml# 打印 MyBatis 执行的 SQL 打印出来mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl Mybatis 编写数据库操作的方法对应的 XxxMapper.java 接口类，以及对应的 XxxMapper.xml 配置类，配置类用于编写具体的 SQL 语句以便 Mybatis 进行动态实现。 Mapper 接口类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// 也可通过在 Spring Boot 运行类上添加 @MapperScan() 注解扫描@Mapper // 扫描此类为 Mybatis 的 mapper 类public interface UserMybatisMapper &#123; /** * 用户注册 / 添加用户：在数据库中添加用户信息 * * @param userMybatis 用户 * @return */ int insert(UserMybatis userMybatis); /** * 用户登录：通过用户名、用户密码和用户类型查询用户信息 * * @param username 用户名 * @param password 用户密码 * @param type 用户类型 * @return */ UserMybatis findUserMybatisByUsernameAndPasswordAndType(String username, String password, String type); /** * 管理员用户管理页面：通过用户类型和用户状态查询用户信息 * * @param type 用户类型 * @param isDelete 用户状态 * @return */ List&lt;UserMybatis&gt; findAllByTypeAndStatus(String type, Integer isDelete); /** * 用户修改密码：通过用户名和用户类修改用户密码 * * @param password 用户密码 * @param username 用户名 * @param type 用户类型 * @param gmtModified 用户修改时间 * @return */ int updatePasswordByUsernameAndType(String password, String username, String type, Date gmtModified); /** * 更新用户信息 * * @param userMybatis 用户 * @return */ int update(UserMybatis userMybatis); /** *管理员删除用户：通过用户标识更新用户状态 * * @param userId 用户标识 * @param gmtModified 用户修改时间 * @return */ int updateStatusByUserId(Integer userId, Date gmtModified); /** * 通过用户名和用户类型查找用户 * * @param username 用户名 * @param type 用户类型 * @return */ UserMybatis findUserMybatisByUsernameAndType(String username, String type);&#125; Mapper 配置实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wingo.user.mapper.UserMybatisMapper\"&gt; &lt;!-- 编写字段与属性的映射关系 --&gt; &lt;resultMap id=\"BaseResultMap\" type=\"com.wingo.user.entity.UserMybatis\"&gt; &lt;result column=\"user_id\" jdbcType=\"INTEGER\" property=\"userId\" /&gt; &lt;result column=\"username\" jdbcType=\"VARCHAR\" property=\"username\" /&gt; &lt;result column=\"password\" jdbcType=\"VARCHAR\" property=\"password\" /&gt; &lt;result column=\"type\" jdbcType=\"CHAR\" property=\"type\" /&gt; &lt;result column=\"gmt_create\" jdbcType=\"TIMESTAMP\" property=\"gmtCreate\" /&gt; &lt;result column=\"gmt_modified\" jdbcType=\"TIMESTAMP\" property=\"gmtModified\" /&gt; &lt;result column=\"is_delete\" jdbcType=\"BIT\" property=\"isDelete\" /&gt; &lt;/resultMap&gt; &lt;insert id=\"insert\" parameterType=\"UserMybatis\" keyProperty=\"userId\" useGeneratedKeys=\"true\"&gt; INSERT INTO user (user_id, username, password,type, gmt_create, gmt_modified, is_delete) VALUES (#&#123;userId&#125;, #&#123;username&#125;, #&#123;password&#125;, #&#123;type&#125;, #&#123;gmtCreate&#125;, #&#123;gmtModified&#125;, #&#123;isDelete&#125;) &lt;/insert&gt; &lt;select id=\"findUserMybatisByUsernameAndPasswordAndType\" resultMap=\"BaseResultMap\"&gt; SELECT user_id, username, password, type, gmt_create, gmt_modified, is_delete FROM user WHERE username = #&#123;username&#125; AND password = #&#123;password&#125; AND type = #&#123;type&#125; &lt;/select&gt; &lt;select id=\"findAllByTypeAndStatus\" resultMap=\"BaseResultMap\"&gt; SELECT user_id, username, password, type, gmt_create, gmt_modified, is_delete FROM user WHERE type = #&#123;type&#125; AND is_delete = #&#123;isDelete&#125; &lt;/select&gt; &lt;select id=\"findUserMybatisByUsernameAndType\" resultMap=\"BaseResultMap\"&gt; SELECT user_id, username, password, type, gmt_create, gmt_modified, is_delete FROM user WHERE username = #&#123;username&#125; AND type = #&#123;type&#125; &lt;/select&gt; &lt;update id=\"update\" parameterType=\"UserMybatis\"&gt; UPDATE user &lt;set&gt; &lt;if test=\"username != null\"&gt; username = #&#123;username&#125;, &lt;/if&gt; &lt;if test=\"password != null\"&gt; password = #&#123;password&#125;, &lt;/if&gt; &lt;if test=\"type != null\"&gt; type = #&#123;type&#125;, &lt;/if&gt; &lt;if test=\"gmtCreate != null\"&gt; gmt_create = #&#123;gmtCreate&#125;, &lt;/if&gt; &lt;if test=\"gmtModified != null\"&gt; gmt_modified = #&#123;gmtModified&#125;, &lt;/if&gt; &lt;if test=\"isDelete != null\"&gt; is_delete = #&#123;isDelete&#125; &lt;/if&gt; &lt;/set&gt; WHERE user_id = #&#123;userId&#125; &lt;/update&gt; &lt;update id=\"updatePasswordByUsernameAndType\"&gt; UPDATE user SET password = #&#123;password&#125;, gmt_modified = #&#123;gmtModified&#125; where username =#&#123;username&#125; and type = #&#123;type&#125; &lt;/update&gt; &lt;update id=\"updateStatusByUserId\"&gt; UPDATE user SET is_delete = true, gmt_modified = #&#123;gmtModified&#125; WHERE user_id = #&#123;userId&#125; &lt;/update&gt;&lt;/mapper&gt; 编写测试方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546@RunWith(SpringRunner.class)@SpringBootTestpublic class JavaSpringWingoApplicationTests &#123; @Autowired UserMybatisMapper userMybatisMapper; @Test @Rollback public void testInsert() &#123; Date date = new Date(); UserMybatis userMybatis = new UserMybatis(); userMybatis.setUsername(\"张三\"); userMybatis.setPassword(\"123456\"); userMybatis.setType(\"0\"); userMybatis.setGmtCreate(date); userMybatis.setGmtModified(date); userMybatis.setIsDelete(false); userMybatisMapper.insert(userMybatis); &#125; /** * UserMybatis userMybatis = userMybatisMapper. * findUserMybatisByUsernameAndPasswordAndType(\"张三\",\"123456\",\"0\"); * * List&lt;UserMybatis&gt; list = new ArrayList&lt;&gt;(); * list = userMybatisMapper.findAllByTypeAndStatus(\"0\",0); * Iterator&lt;UserMybatis&gt; iterable = list.iterator(); * while (iterable.hasNext())&#123; * UserMybatis userMybatis = iterable.next(); * System.out.println(userMybatis); * &#125; * * UserMybatis userMybatis = userMybatisMapper.findUserMybatisByUsernameAndType(\"张三\",\"0\"); * * UserMybatis userMybatis = new UserMybatis(); * userMybatis.setUserId(6); * userMybatis.setUsername(\"李四\"); * int result = userMybatisMapper.update(userMybatis); * * Date date = new Date(); * userMybatisMapper.updatePasswordByUsernameAndType(\"654321\",\"李四\",\"0\",date); * * Date date = new Date(); * userMybatisMapper.updateStatusByUserId(6,date); */&#125; 业务逻辑层Mybatis 版本编写项目场景功能的业务逻辑：接收前端传来的信息所封住的 DTO，然后调用 DAO 进行业务逻辑的编写。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173@Service(\"userServiceMybatisImpl\")public class UserServiceMybatisImpl implements UserService &#123; private final UserMybatisMapper userMybatisMapper; public UserServiceMybatisImpl(UserMybatisMapper userMybatisMapper) &#123; this.userMybatisMapper = userMybatisMapper; &#125; /** * 检查此用户是否已经存在 * * @param username 用户名 * @return */ private boolean checkUserDuplicate(String username)&#123; // 定义默认返回 false boolean isDuplicate = false; UserMybatis userMybatis = userMybatisMapper.findUserMybatisByUsernameAndType(username, UserTypeConstant.ORDINARY); // 若此用户存在，则返回 true if(userMybatis!=null)&#123; isDuplicate = true; &#125; return isDuplicate; &#125; private UserMybatis buildUserMybatis(String username, String password)&#123; Date data = new Date(); // 创建用户实体对象并进行赋值 UserMybatis userMybatis = new UserMybatis(); userMybatis.setUsername(username); userMybatis.setPassword(Md5Util.md5(password)); userMybatis.setType(UserTypeConstant.ORDINARY); userMybatis.setGmtCreate(data); userMybatis.setGmtModified(data); userMybatis.setIsDelete(false); return userMybatis; &#125; /** 用户注册 */ @Override @Transactional(rollbackFor = Exception.class) public ResponseVO register(RegisterDTO registerDTO) &#123; // 定义返回对象 ResponseVO responseVO = new ResponseVO(); // 检查此用户是否是否已经存在 if(checkUserDuplicate(registerDTO.getUsername()))&#123; // 组装错误返回对象 responseVO.fail(\"用户名重复，请重新注册！\"); // 返回错误代码及错误信息 return responseVO; &#125; // 创建用户实体对象并进行赋值 UserMybatis userMybatis = buildUserMybatis(registerDTO.getUsername(), registerDTO.getPassword()); // 保存数据到数据库 userMybatisMapper.insert(userMybatis); // 组装成功返回对象 responseVO.success(\"恭喜您，注册成功！请登录\"); return responseVO; &#125; /** 用户登录 */ @Override public ResponseVO login(LoginDTO loginDTO) &#123; ResponseVO responseVO = new ResponseVO(); UserMybatis userMybatis = userMybatisMapper.findUserMybatisByUsernameAndPasswordAndType (loginDTO.getUsername(), Md5Util.md5(loginDTO.getPassword()), loginDTO.getType()); if(userMybatis == null)&#123; responseVO.fail(\"登录失败，用户不存在、已删除或者用户名、密码或类型选择错误！\"); return responseVO; &#125; responseVO.success(\"登录成功！\"); // 构建返回的用户信息 UserVO userVO = new UserVO(); userVO.setUsername(loginDTO.getUsername()); userVO.setType(loginDTO.getType()); responseVO.setData(\"userVO\", userVO); return responseVO; &#125; /** 管理员添加用户 */ @Override @Transactional(rollbackFor = Exception.class) public ResponseVO add(AddDTO addDTO) &#123; // 定义返回对象 ResponseVO responseVO = new ResponseVO(); // 检查此用户是否是否已经存在 if(checkUserDuplicate(addDTO.getUsername()))&#123; // 组装错误返回对象 responseVO.fail(\"用户名重复，请重新添加！\"); // 返回错误代码及错误信息 return responseVO; &#125; // 创建用户实体对象并进行赋值 UserMybatis userMybatis = buildUserMybatis(addDTO.getUsername(), addDTO.getPassword()); // 保存数据到数据库 userMybatisMapper.insert(userMybatis); // 组装成功返回对象 responseVO.success(\"添加成功！\"); return responseVO; &#125; /** 管理员查看用户列表 */ @Override public ResponseVO view() &#123; ResponseVO responseVO = new ResponseVO(); // 定义返回的结果集对象，这种定义方式是对象形式的集合 List&lt;ViewVO&gt; userList = new ArrayList&lt;&gt;(); List&lt;UserMybatis&gt; userMybatisList = userMybatisMapper.findAllByTypeAndStatus(UserTypeConstant.ORDINARY, false); if(userMybatisList == null)&#123; responseVO.fail(\"请求用户列表失败！\"); return responseVO; &#125; for(UserMybatis userMybatis : userMybatisList)&#123; // 创建一个新对象来存储查询出的一条条记录 ViewVO viewVO = new ViewVO(); // 对对象进行赋值 viewVO.setUserId(userMybatis.getUserId()); viewVO.setUsername(userMybatis.getUsername()); viewVO.setType(userMybatis.getType()); viewVO.setGtmCreate(userMybatis.getGmtCreate()); // 将对象放入集合中 userList.add(viewVO); &#125; responseVO.success(\"请求用户列表成功！\"); responseVO.setData(\"viewVO\", userList); return responseVO; &#125; /** 用户修改密码 */ @Override @Transactional(rollbackFor = Exception.class) public ResponseVO modify(ModifyDTO modifyDTO) &#123; ResponseVO responseVO = new ResponseVO(); // 判断是否有新密码且新旧密码是否一致 if(modifyDTO.getNewPassword() != null &amp;&amp; modifyDTO.getConfirmNewPassword() != null &amp;&amp; !modifyDTO.getConfirmNewPassword().equals(modifyDTO.getNewPassword()))&#123; responseVO.fail(\"新密码和确认新密码不一致，请检查！\"); return responseVO; &#125; // 判断用户的原密码是否正确 UserMybatis userMybatis = userMybatisMapper.findUserMybatisByUsernameAndPasswordAndType( modifyDTO.getUsername(), Md5Util.md5(modifyDTO.getPassword()), modifyDTO.getType() ); if(userMybatis == null)&#123; responseVO.fail(\"用户原密码不正确，请检查！\"); return responseVO; &#125; Date date = new Date(); // 确认满足修改条件后对用户信息进行更新 userMybatis.setPassword(Md5Util.md5(modifyDTO.getNewPassword())); userMybatis.setGmtModified(date); userMybatisMapper.update(userMybatis); responseVO.success(\"密码修改成功！\"); return responseVO; &#125; /** 删除用户 */ @Override @Transactional(rollbackFor = Exception.class) public ResponseVO delete(DeleteDTO deleteDTO) &#123; ResponseVO responseVO = new ResponseVO(); Date date = new Date(); int deleteResult = userMybatisMapper.updateStatusByUserId(deleteDTO.getUerId(),date); if(deleteResult != 1)&#123; responseVO.fail(\"删除失败，没有更新到或者更新了多条记录！\"); return responseVO; &#125; responseVO.success(\"用户删除成功！\"); return responseVO; &#125;&#125; 测试用例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@RunWith(SpringRunner.class)@SpringBootTestpublic class JavaSpringWingoApplicationTests &#123; @Resource(name = \"userServiceMybatisImpl\") UserService userService; /** Md5Util 输出 */ @Test public void testMd5()&#123; System.out.println(Md5Util.md5(\"e10adc3949ba59abbe56e057f20f883e\")); &#125; /** 测试注册接口 */ @Test @Rollback public void testRegister() &#123; // 构造注册的测试数据 RegisterDTO registerDTO = new RegisterDTO(); registerDTO.setUsername(\"Jay\"); registerDTO.setPassword(\"123456\"); // 处理业务 ResponseVO responseVO = userService.register(registerDTO); // 测试断言 assertEquals(ResponseConstant.SUCCESS, responseVO.get(\"code\")); &#125; /** 测试登录接口 */ @Test public void testLogin()&#123; LoginDTO loginDTO = new LoginDTO(); loginDTO.setUsername(\"Tom\"); loginDTO.setPassword(\"123456\"); loginDTO.setType(0); ResponseVO responseVO = userService.login(loginDTO); assertEquals(ResponseConstant.SUCCESS, responseVO.get(\"code\")); &#125; /** 测试添加接口 */ @Test @Rollback public void testAdd()&#123; AddDTO addDTO = new AddDTO(); addDTO.setUsername(\"Tim\"); addDTO.setPassword(\"123456\"); ResponseVO responseVO = userService.add(addDTO); assertEquals(ResponseConstant.SUCCESS, responseVO.get(\"code\")); &#125; /** 测试用户列表接口 */ @Test public void testView()&#123; ResponseVO responseVO = userService.view(); System.out.println(responseVO); assertEquals(ResponseConstant.SUCCESS, responseVO.get(\"code\")); &#125; /** 测试修改密码接口*/ @Test @Rollback public void testModify()&#123; ModifyDTO modifyDTO = new ModifyDTO(); modifyDTO.setUsername(\"Jay\"); modifyDTO.setType(0); modifyDTO.setPassword(\"123456\"); modifyDTO.setNewPassword(\"654321\"); modifyDTO.setConfirmNewPassword(\"654321\"); ResponseVO responseVO = userService.modify(modifyDTO); assertEquals(ResponseConstant.SUCCESS, responseVO.get(\"code\")); &#125;&#125; 控制层Swagger 整合用于浏览器请求测试。 也可采用 Spring Boot 提供的 swagger 的 stater，利用启动器的方式添加依赖，可以直在 Spring Boot 的配置文件中配置 swagger 的配置信息。 1234567891011&lt;!-- 添加 Swagger 的支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 配置类 1234567891011121314151617@Configuration@EnableSwagger2public class SwaggerConfig &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(\"com.wingo.user.controller\")) .paths(PathSelectors.any()).build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder().title(\"Swagger RESTful APIs\") .description(\"Swagger API 服务\") .build(); &#125;&#125; 代码编写 Model model：Spring MVC 提供了一个Model用于暴露后端传递给页面的信息，通过addAttribute(K, V)方法添加数据，并且在前端的模板引擎页面通过${K}取得对应的V； HttpServletResponse httpServletResponse：encodeRedirectURL(Url)方法进行页面跳转，跳转的同时携带域中的信息； HttpSession httpSession：setAttribute(K, V)方法在 Session 域中添加信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201@Controller@Slf4jpublic class UserController &#123; /** 注入 UserService */ @Resource(name = \"userServiceMybatisImpl\") private UserService userService; /** 处理用户注册的请求 */ @ApiOperation(\"用户注册接口\") @PostMapping(value = \"/register\") public String register(Model model, @Valid @ModelAttribute(value = \"registerDTO\") RegisterDTO registerDTO, HttpServletResponse httpServletResponse) &#123; ResponseVO responseVO = userService.register(registerDTO); model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); if (ResponseConstant.SUCCESS.equals(responseVO.get(ResponseConstant.CODE))) &#123; // 注册成功，重定向到前端登录页面 login.html，并且携带数据过去 return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; // 注册失败，跳转到注册页面，并将失败的信息返回 return httpServletResponse.encodeRedirectURL(UrlConstant.REGISTER); &#125; /** * 生成图形验证码 * * @throws NoSuchAlgorithmException */ @ApiIgnore @GetMapping(value = \"/imageCode\") public String imageCode(HttpServletRequest request, HttpServletResponse response) throws IOException, NoSuchAlgorithmException &#123; OutputStream os = response.getOutputStream(); // 生成验证码 Map&lt;String, Object&gt; map = ImageCodeUtil.getImageCode(60, 20, 4); // 放入客户端的 Session 对象中 request.getSession().setAttribute(ImageCodeConstant.SIMPLE_CAPTCHA, map.get(ImageCodeConstant.CAPTCHA_VALUE).toString().toLowerCase()); // 设置验证码的生成时间，并放入客户端 Session 对象中 request.getSession().setAttribute(ImageCodeConstant.CODE_TIME, System.currentTimeMillis()); // 将生成的验证码图片写到图片输出流 try &#123; ImageIO.write((BufferedImage) map.get(ImageCodeConstant.IMAGE), ImageCodeConstant.JPEG, os); &#125; catch (IOException e) &#123; log.error(\"图形验证码生成异常\"); &#125; return null; &#125; /** 处理用户的登录请求 */ @ApiIgnore @PostMapping(value = \"/login\") public String login(Model model, @Valid @ModelAttribute(value = \"loginDTO\") LoginDTO loginDTO, HttpServletResponse httpServletResponse, HttpSession httpSession) &#123; ResponseVO responseVO = new ResponseVO(); // 取得用户填写的验证码 String checkCode = loginDTO.getCheckCode(); // 从 Session 中取得存入的验证码信息 Object sessionImageCode = httpSession.getAttribute(ImageCodeConstant.SIMPLE_CAPTCHA); // 获取验证码信息失败 if(sessionImageCode == null)&#123; responseVO.fail(\"请填写验证码\"); model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; String code = sessionImageCode.toString(); // 检测验证码 Date date = new Date(); Long codeTime = Long.valueOf(httpSession.getAttribute(ImageCodeConstant.CODE_TIME) + \"\"); // 验证码是否填写正确 if (StringUtils.isBlank(checkCode) || StringUtils.isBlank(code) || !(checkCode.equalsIgnoreCase(code))) &#123; responseVO.fail(\"验证码不一致\"); model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; // 验证码是否过期（有效期为 5 分钟） else if ((date.getTime() - codeTime) / ImageCodeConstant.MILLISECOND_CONVERSION / ImageCodeConstant.MINUTE_CONVERSION &gt; ImageCodeConstant.CAPTCHA_EXPIRY_TIME) &#123; responseVO.fail(\"验证码已过期\"); model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; // 处理用户登录业务 responseVO = userService.login(loginDTO); // 将结果放入 model 中，在模板中可以取到 model 中的值 // 这里就是交互的一个重要地方，我们可以在模板中通过这些属性值访问到数据 model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); // 设置登录 session，模版引擎使用 httpSession.setAttribute(UserConstant.USERNAME, loginDTO.getUsername()); httpSession.setAttribute(UserConstant.TYPE, loginDTO.getType()); // 将类型码转换为中文说明 httpSession.setAttribute(UserConstant.TYPE_NAME, UserTypeConstant.typeToDesc(loginDTO.getType())); // 组装结果返回 if (responseVO == null || !ResponseConstant.SUCCESS.equals(responseVO.get(ResponseConstant.CODE))) &#123; // 登录失败则跳转到前端登录页面 login.html return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; // 登录成功重定向到前端首页 index.html，并且携带数据过去 return httpServletResponse.encodeRedirectURL(UrlConstant.INDEX); &#125; /** 处理查看用户列表的请求 */ @ApiIgnore @GetMapping(value = \"/users\") public String users(Model model, HttpServletResponse httpServletResponse) &#123; // 处理用户列表查看业务 ResponseVO responseVO = userService.view(); model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); // 开始重定向前端用户列表也 view.html，携带数据过去了。 return httpServletResponse.encodeRedirectURL(UrlConstant.VIEW); &#125; /** 处理新增用户的请求 */ @ApiOperation(\"用户新增接口\") @PostMapping(value = \"/user\") public String user(Model model, @Valid @ModelAttribute(value = \"addDTO\") AddDTO addDTO, HttpServletResponse httpServletResponse) &#123; // 处理新增用户业务 ResponseVO responseVO = userService.add(addDTO); // 将处理结果写入到返回对象中 model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); // 用户添加失败，重定向到前端页面 user.html if (responseVO == null || !ResponseConstant.SUCCESS.equals(responseVO.get(ResponseConstant.CODE)))&#123; return httpServletResponse.encodeRedirectURL(UrlConstant.USER); &#125; // 处理成功 responseVO = userService.view(); model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); // 开始重定向，携带数据过去 return httpServletResponse.encodeRedirectURL(UrlConstant.VIEW); &#125; /** 删除用户 */ @ApiOperation(\"删除用户接口\") @GetMapping(value = \"/delete\") public String delete(Model model, @Valid @ModelAttribute(value = \"deleteDTO\") DeleteDTO deleteDTO, HttpServletResponse httpServletResponse) &#123; // 处理逻辑删除用户业务 ResponseVO responseVO = userService.delete(deleteDTO); // 删除失败返回错误信息 if(responseVO.get(ResponseConstant.CODE).equals(ResponseConstant.FAIL))&#123; model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); return httpServletResponse.encodeRedirectURL(UrlConstant.VIEW); &#125; // 删除成功后再查询用户列表数据，返回删除后的最新用户列表数据 responseVO = userService.view(); if (responseVO != null) &#123; model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); &#125; // 逻辑删除用户后，跳转到用户前端列表页 view.jsp return httpServletResponse.encodeRedirectURL(UrlConstant.VIEW); &#125; /** 处理用户信息修改的请求 */ @ApiIgnore @PostMapping(value = \"/modify\") public String toModify(Model model, @Valid @ModelAttribute(value = \"modifyDTO\") ModifyDTO modifyDTO, HttpServletResponse httpServletResponse, HttpSession httpSession) &#123; // 从 Session 中取得用户名字及类型 String strUsername = httpSession.getAttribute(UserConstant.USERNAME).toString(); String strType = httpSession.getAttribute(UserConstant.TYPE).toString(); modifyDTO.setUsername(strUsername); modifyDTO.setType(strType); // 处理用户信息修改业务 ResponseVO responseVO = userService.modify(modifyDTO); // 将结果放入 Model 中，在模板中可以取到 Model 中的值 model.addAttribute(ResponseConstant.RESPONSE_VO, responseVO); if (responseVO == null || !ResponseConstant.SUCCESS.equals(responseVO.get(ResponseConstant.CODE))) &#123; // 修改失败时，继续重定向到前端用户信息修改页面 modify.html return httpServletResponse.encodeRedirectURL(UrlConstant.MODIFY); &#125; // 密码修改成功，从 Session 中删除 user 属性，用户退出登录 httpSession.removeAttribute(UserConstant.USERNAME); httpSession.removeAttribute(UserConstant.TYPE); httpSession.removeAttribute(UserConstant.TYPE_NAME); // 开始重定向前端登录页面 login.html，并且携带数据过去 return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; /** 注销登录 */ @ApiIgnore @GetMapping(value = \"/loginOut\") public String loginOut(HttpSession httpSession, HttpServletResponse httpServletResponse) &#123; // 从 Session 中删除 user 属性，用户退出登录 httpSession.removeAttribute(UserConstant.USERNAME); httpSession.removeAttribute(UserConstant.TYPE); httpSession.removeAttribute(UserConstant.TYPE_NAME); // 开始重定向到前端页面 login.html return httpServletResponse.encodeRedirectURL(UrlConstant.LOGIN); &#125; /** 处理 Get:/index 请求 */ @ApiIgnore @GetMapping(value=\"/index\") public String index(HttpServletResponse httpServletResponse)&#123; // 重定向到前端页面 index.html return httpServletResponse.encodeRedirectURL(UrlConstant.INDEX); &#125; // 省略余下页面跳方法&#125; Thymeleaf引入依赖： 12345&lt;!-- 支持 Thymeleaf --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 配置 Thymeleaf： 1234567## Thymeleaf 配置spring.thymeleaf.prefix=classpath:/templatesspring.thymeleaf.suffix=.htmlspring.thymeleaf.mode=HTMLspring.thymeleaf.servlet.content-type=text/htmlspring.thymeleaf.cache=falsespring.thymeleaf.enabled=true login.html1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;title&gt;登录&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p th:if=\"$&#123;responseVO != null &amp;&amp; responseVO.get('message') != null&#125;\" th:text=\"$&#123;responseVO.get('message')&#125;\"&gt;&lt;/p&gt; &lt;form th:action=\"@&#123;/login&#125;\" th:object=\"$&#123;loginDTO&#125;\" method=\"post\"&gt; &lt;div style=\"height: 27px\"&gt; &lt;label for=\"username\"&gt;用户名: &lt;/label&gt; &lt;input type=\"text\" name=\"username\" id=\"username\" placeholder=\"用户名\" th:value=\"$&#123;registerDTO !=null &amp;&amp; registerDTO.username != null&#125; ? $&#123;registerDTO.username&#125; : ''\" /&gt; &lt;/div&gt; &lt;div style=\"height: 27px\"&gt; &lt;label for=\"password\"&gt;密码: &lt;/label&gt; &lt;input type=\"password\" name=\"password\" id=\"password\" placeholder=\"密码\"/&gt; &lt;/div&gt; &lt;div style=\"height: 27px\"&gt; &lt;label for=\"checkCode\" class=\"green\"&gt;验证码: &lt;/label&gt; &lt;input id=\"checkCode\" name=\"checkCode\" class=\"checkCode\" type=\"text\" placeholder=\"验证码\" /&gt; &lt;img style=\"cursor: pointer; margin-left: 3px;\" th:src=\"@&#123;/imageCode&#125;\" onclick=\"this.src=this.src + '?' + new Date().valueOf();\" id=\"validateImg\" alt=\"验证码\" class=\"codePic\" title=\"验证码。点击此处更新验证码。\" /&gt; &lt;/div&gt; &lt;div style=\"height: 27px\"&gt; &lt;label for=\"type\"&gt;用户类型: &lt;/label&gt; &lt;select id=\"type\" name=\"type\"&gt; &lt;option type=\"number\" value=\"0\" &gt;普通用户&lt;/option&gt; &lt;option type=\"number\" value=\"1\" &gt;管理员用户&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;input type=\"submit\" value=\"登录\"&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; index.html1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;登录后的首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h4&gt; 欢迎您：&lt;span style=\"color: red\" th:text=\"$&#123;session.username&#125;?$&#123;session.username&#125;:'（您未登录）'\"&gt;&lt;/span&gt;, 用户身份是：&lt;span style=\"color: red\" th:text=\"$&#123;session.typeName&#125;?$&#123;session.typeName&#125;:'（您未登录）'\" &gt;&lt;/span&gt; &lt;/h4&gt; &lt;th:block th:if=\"$&#123;session.username != null&#125;\"&gt; &lt;a th:href=\"@&#123;/users&#125;\" th:if=\"$&#123;session.typeName eq '管理员'&#125;\"&gt;查看用户列表&lt;/a&gt; &lt;a th:href=\"@&#123;/user&#125;\" th:if=\"$&#123;session.typeName eq '管理员'&#125; \"&gt;添加用户&lt;/a&gt; &lt;a th:href=\"@&#123;/modify&#125;\"&gt;修改密码&lt;/a&gt; &lt;a th:href=\"@&#123;/loginOut&#125;\"&gt;注销登录&lt;/a&gt; &lt;/th:block&gt;&lt;/body&gt;&lt;/html&gt; modify.html123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;修改密码&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p th:if=\"$&#123;responseVO != null &amp;&amp; responseVO.get('message') != null&#125;\" th:text=\"$&#123;responseVO.get('message')&#125;\"&gt;&lt;/p&gt; &lt;th:block th:if=\"$&#123;session.username != null&#125;\"&gt; &lt;form th:action=\"@&#123;/modify&#125;\" th:object=\"$&#123;modifyDTO&#125;\" method=\"post\"&gt; &lt;label for=\"password\"&gt;原密码:&lt;/label&gt; &lt;input type=\"password\" name=\"password\" id=\"password\" /&gt; &lt;br /&gt; &lt;label for=\"newPassword\"&gt;新密码:&lt;/label&gt; &lt;input type=\"password\" name=\"newPassword\" id=\"newPassword\" /&gt; &lt;br /&gt; &lt;label for=\"confirmNewPassword\"&gt;确认新密码:&lt;/label&gt; &lt;input type=\"password\" name=\"confirmNewPassword\" id=\"confirmNewPassword\" /&gt; &lt;br /&gt; &lt;input type=\"submit\" value=\"确认修改\"&gt; &lt;/form&gt; &lt;/th:block&gt;&lt;/body&gt;&lt;/html&gt; register.html12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;用户注册&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p th:if=\"$&#123;responseVO != null &amp;&amp; responseVO.get('message') != null&#125;\" th:text=\"$&#123;responseVO.get('message')&#125;\"&gt;&lt;/p&gt; &lt;form th:action=\"@&#123;/register&#125;\" th:object=\"$&#123;registerDTO&#125;\" method=\"post\"&gt; &lt;label for=\"username\"&gt;用户名:&lt;/label&gt; &lt;input type=\"text\" name=\"username\" id=\"username\" /&gt; &lt;br /&gt; &lt;label for=\"password\"&gt;密码:&lt;/label&gt; &lt;input type=\"password\" name=\"password\" id=\"password\" /&gt; &lt;br /&gt; &lt;input type=\"submit\" value=\"注册\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; user.html123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;添加用户&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p th:if=\"$&#123;responseVO != null &amp;&amp; responseVO.get('message') != null&#125;\" th:text=\"$&#123;responseVO.get('message')&#125;\"&gt;&lt;/p&gt; &lt;th:block th:if=\"$&#123;session.username != null &amp;&amp; session.typeName == '管理员'&#125;\"&gt; &lt;form th:action=\"@&#123;/user&#125;\" th:object=\"$&#123;addDTO&#125;\" method=\"post\"&gt; &lt;label for=\"username\"&gt;用户名:&lt;/label&gt; &lt;input type=\"text\" name=\"username\" id=\"username\" /&gt; &lt;br /&gt; &lt;label for=\"password\"&gt;密码:&lt;/label&gt; &lt;input type=\"password\" name=\"password\" id=\"password\" /&gt; &lt;br /&gt; &lt;input type=\"submit\" value=\"添加\"&gt; &lt;/form&gt; &lt;/th:block&gt;&lt;/body&gt;&lt;/html&gt; view.html123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;查看用户列表&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;table border=\"1\"&gt; &lt;tr&gt; &lt;td&gt;ID&lt;/td&gt; &lt;td&gt;名字&lt;/td&gt; &lt;td&gt;用户类型&lt;/td&gt; &lt;td&gt;注册时间&lt;/td&gt; &lt;td&gt;操作类型&lt;/td&gt; &lt;/tr&gt; &lt;th:block th:if=\"$&#123;session.username != null &amp;&amp; session.typeName == '管理员'&#125;\"&gt; &lt;!-- 循环展示普通用户清单 --&gt; &lt;th:block th:if=\"$&#123;responseVO.get('data').viewVO != null&#125;\"&gt; &lt;tr th:each=\"ViewVO : $&#123;responseVO.get('data').viewVO&#125;\"&gt; &lt;td th:text=\"$&#123;ViewVO.userId&#125;\"&gt;&lt;/td&gt; &lt;td th:text=\"$&#123;ViewVO.username&#125;\"&gt;&lt;/td&gt; &lt;td th:text=\"$&#123;ViewVO.type&#125;\"&gt;&lt;/td&gt; &lt;td th:text=\"$&#123;#dates.format(ViewVO.gmtCreate, 'yyyy-MM-dd')&#125;\"&gt;&lt;/td&gt; &lt;td&gt;&lt;a th:href=\"@&#123;/delete(uerId=$&#123;ViewVO.userId&#125;)&#125;\"&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/th:block&gt; &lt;/th:block&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 过滤器对某些特定页面的访问需要有一定的前提条件：比如用户是否登录，用户是否拥有访问此页面的权限等。要实现这样的功能可以使用 Filter。 过滤器执行顺序：(1) 通过实例化 FilterRegistrationBean 类自定义 Filter，setOrder() 方法可以定义执行顺序。（小的优先）(2) 通过限定类名限定 Filter 的顺序。（默认初始化过滤器时进行了类名的排序） 123456789101112131415161718192021222324252627282930313233@Component@WebFilter(filterName = \"/Filter0Session\", urlPatterns = &#123;\"/index\", \"/add\", \"/modify\", \"/view\", \"/delete\", \"/loginOut\"&#125;)public class Filter0Session implements Filter &#123; private Filter0Session() &#123;&#125; /** 过滤器初始化方法，web 服务器根据配置创建 Filter 实例 */ @Override public void init(FilterConfig fConfig) throws ServletException &#123; // Do nothing. &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; HttpSession httpSession = httpServletRequest.getSession(false); if (httpSession != null &amp;&amp; httpSession.getAttribute(UserConstant.USERNAME) != null) &#123; // 用户已登录则放行 chain.doFilter(request, response); &#125; else &#123; // 检测到用户未登录则跳转重定向到登陆页面 httpServletResponse.sendRedirect(\"/login\"); &#125; &#125; /** 过滤器销毁方法，可以释放过滤器使用的资源 */ @Override public void destroy() &#123; // Do nothing. &#125;&#125; 12345678910111213141516171819202122232425262728293031323334@Component@WebFilter(filterName = \"/Filter1Admin\", urlPatterns = &#123;\"/add\", \"/view\", \"/delete\"&#125;)public class Filter1Admin implements Filter &#123; private Filter1Admin() &#123;&#125; @Override public void init(FilterConfig fConfig) throws ServletException &#123; // Do nothing. &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; HttpSession httpSession = httpServletRequest.getSession(false); // 检测用户是否登录，并且用户类型是否为管理员 if (httpSession != null &amp;&amp; httpSession.getAttribute(UserConstant.USERNAME) != null &amp;&amp; httpSession.getAttribute(UserConstant.TYPE) != null &amp;&amp; UserTypeConstant.ADMIN.equals(httpSession.getAttribute(UserConstant.TYPE))) &#123; chain.doFilter(request, response); &#125; else &#123; httpServletResponse.sendRedirect( \"/login\"); &#125; &#125; @Override public void destroy() &#123; // Do nothing. &#125;&#125; ！！！@ServletComponentScan(value = “com.wingo.user.filter”) 全局异常处理对于前端传输的数据，在 DTO 类中添加 JSR303 校验，在测试的时候发现此校验的抛出异常时，Spring Boot 会自动跳转到默认的 error 页面。原因在于项目中未对此校验索抛出的异常进行处理。先看看校验后所抛出的异常信息。 Spring Boot 默认的错误提示页面： 控制台抛出的异常BindException: org.springframework.validation.BeanPropertyBindingResult: 3 errors 可以在 controller 类中添加 try{ }catch{ } 异常处理块捕捉异常抛出，并进行处理（如返回 Json）。但这样代码很不美观，推荐使用的方式是用注解 @ControllerAdvice + @ExceptionHandler 全局处理 Controller 层异常。 编写全局处理类 12345678910111213141516171819202122232425262728@Slf4j@ControllerAdvice // 声明全局的异常处理类@ResponseBodypublic class GlobalExceptionHandler &#123; // 定义捕捉的异常 @ExceptionHandler(BindException.class) @ResponseStatus(HttpStatus.BAD_REQUEST) public ResponseVO resolveMethodArgumentNotValidException(BindException exception) &#123; ResponseVO response = new ResponseVO(); // 定义字段及校验异常信息的 key / value 结构 Map&lt;String, String&gt; fieldAndMessage = new HashMap&lt;&gt;(16); StringBuilder sb = new StringBuilder(); // 组装字段及校验异常信息 for (FieldError fieldError : exception.getBindingResult().getFieldErrors()) &#123; fieldAndMessage.put(fieldError.getField(), fieldError.getDefaultMessage()); sb.append(fieldError.getField()).append(\":\").append(fieldError.getDefaultMessage()).append(\";\"); &#125; String strField = sb.toString(); // 组装异常信息：从自定义的异常信息枚举类中取得信息 response.setCode(ResponseCodeTypeEnum.PARAMETER_ERROR.getCode()); response.setMessage(ResponseCodeTypeEnum.PARAMETER_ERROR.getMessage()); // 方便调试：将异常统一打印出来 log.warn(\"BindException.class 异常信息: &#123;&#125;\", strField); // 返回 Json return response; &#125;&#125; 运行结果：此时已不显示默认的错误页面，而是显示由用户定义的 Json 字符串。 控制台输出：方便开发调试。 自定义 SQL 日志打印当开启 Mybatis 的默认日志打印，控制台的 SQL 默认打印样式： 自定义拦截器： [Mybatis 插件拦截器]([https://welab-wingo.cn/2020/03/29/Software/Mybatis/Mybatis%20%E6%8F%92%E4%BB%B6%E4%B9%8B%E6%8B%A6%E6%88%AA%E5%99%A8/](https://welab-wingo.cn/2020/03/29/Software/Mybatis/Mybatis 插件之拦截器/)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170@Slf4j@Component@Intercepts(&#123;@Signature(type = Executor.class, method = \"update\", args = &#123;MappedStatement.class, Object.class&#125;), @Signature(type = Executor.class, method = \"query\", args = &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125;)&#125;)public class SqlInterceptor implements Interceptor &#123; private Properties properties; @Override public Object intercept(Invocation invocation) throws Throwable &#123; // MappedStatement 维护了一条 mapper.xml 文件里面 select 、update、delete、insert 节点的封装 MappedStatement mappedStatement = (MappedStatement) invocation.getArgs()[0]; Object parameter = null; if (invocation.getArgs().length &gt; 1) &#123; parameter = invocation.getArgs()[1]; &#125; // String sqlId = mappedStatement.getId(); // 获取动态生成的 SQL 语句以及相应的参数信息对象 BoundSql boundSql = mappedStatement.getBoundSql(parameter); // MyBatis 所有的配置信息都维持在 Configuration 对象之中，包括 id、Statement 类型、SQL 类型、mapper 的 xml 文件的绝对地址、是否使用缓存等配置 Configuration configuration = mappedStatement.getConfiguration(); Object returnValue = null; long start = System.currentTimeMillis(); // 继续执行原目标方法 returnValue = invocation.proceed(); // 获取实体类 Class&lt;?&gt; parameterType = getParameterType(mappedStatement); /** 判断实体类上是否包含 @SqlPrint 注解，不包含的话直接返回 */ if (!hasSqlPrintAnnotation(parameterType)) &#123; return returnValue; &#125; long end = System.currentTimeMillis(); long time = (end - start); /** 当 SQL 执行大于 1 毫秒时打印 SQL，所以也可以用于慢 SQL 的打印及监控 */ if (time &gt; 1) &#123; String sql = getSql(configuration, boundSql, sqlId, time); log.info(sql); &#125; // 返回执行后的结果 return returnValue; &#125; /** * 作用：包装目标对象，包装的意思就是为目标对象创建一个代理对象 */ @Override public Object plugin(Object target) &#123; /** 判断下需要拦截的目标对象类型，需要拦截时才拦截，否则直接返回本身，减少被代理的次数 */ if (target instanceof Executor) &#123; // 借助 Plugin 的 wrap 方法来使用当前的拦截器包装我们的目标对象，返回的就是为当前 target 创建的动态代理 // 使用 JDK 的动态代理，给 target 对象创建一个 delegate 代理对象，以此来实现方法拦截和增强功能，它会回调 intercept() 方法。 return Plugin.wrap(target, this); &#125; else &#123; // 无需拦截则直接返回目标对象本身 return target; &#125; &#125; /** * 将插件注册时的 property 属性设置进来 */ @Override public void setProperties(Properties properties) &#123; this.properties = properties; &#125; /** * 组装需要打印的信息，包括 SQL 语句、特殊标记、SQL 执行时间等 * * @param configuration MyBatis 所有的配置信息都维持在 Configuration 对象之中 * @param boundSql 表示动态生成的 SQL 语句以及相应的参数信息对象 * @param sqlId 拦截的执行 SQL 的id（可以理解为方法） * @param time SQL 执行时间 * @return */ private static String getSql(Configuration configuration, BoundSql boundSql, String sqlId, long time) &#123; String sql = showSql(configuration, boundSql); StringBuilder str = new StringBuilder(100); // 拦截的执行 SQL 的id（可以理解为方法），例如：com.gorge4j.user.mapper.UserManageDemoMyBatisMapper.update str.append(sqlId); str.append(\" : \"); // 具体执行的 SQL 语句 str.append(sql); str.append(\" &gt;&gt;&gt;&gt;&gt;&gt; \"); // SQL 执行的时间 str.append(\"SQL执行耗时: \"); str.append(time); str.append(\"ms\"); return str.toString(); &#125; /** 按类型格式化 SQL 里的参数 */ private static String getFormatParameterValue(Object obj) &#123; String value = null; if (obj instanceof String) &#123; value = \"'\" + obj.toString() + \"'\"; &#125; else if (obj instanceof Date) &#123; DateFormat formatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT, DateFormat.DEFAULT, Locale.CHINA); value = \"'\" + formatter.format(new Date()) + \"'\"; &#125; else &#123; if (obj != null) &#123; value = obj.toString(); &#125; else &#123; value = \"\"; &#125; &#125; return value; &#125; /** 组装需要打印的 SQL */ private static String showSql(Configuration configuration, BoundSql boundSql) &#123; // 取得参数的对象，实际上就是一个多个参数的 map 结构 // 示例：parameterObject:MapperMethod&#123;password=123456,name=xxxxxx,type=O,param3=O,param1=xxxxxx,param2=123456&#125; Object parameterObject = boundSql.getParameterObject(); // 查询 SQL 中的参数 List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); // 将 SQL 中一个或多个回车换行符号替换成一个空格 String sql = boundSql.getSql().replaceAll(\"[\\\\s]+\", \" \"); if (!parameterMappings.isEmpty() &amp;&amp; parameterObject != null) &#123; // Mybatis 在启动时就会通过 TypeHandlerRegistry 进行注册，即建立 JdbcType, JavaType, TypeHandler 三者之间的关系。 // 因此，这意味着在 Mybatis 启动时我们也需要通过 TypeHandlerRegistry 将我们的所有的枚举类型（JavaType）与自定义的枚举 // TypeHandler（EnumTypeHandler）建立联系 TypeHandlerRegistry typeHandlerRegistry = configuration.getTypeHandlerRegistry(); // 自定义 TypeHandler 时会走这个逻辑，建立 JavaType 和 JdbcType 之间的联系 if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123; // 替换 SQL 中的占位符 “?” 为具体的参数，replaceFirst 作为是替换匹配到的第一个占位符 “?” sql = sql.replaceFirst(\"\\\\?\", getFormatParameterValue(parameterObject)); &#125; // 没有自定义的 TypeHandler，走通用逻辑 else &#123; // 拿到 target（拦截对象）的元数据 MetaObject metaObject = configuration.newMetaObject(parameterObject); // 参数是按顺序存储的，下面逻辑按顺序来替换 for (ParameterMapping parameterMapping : parameterMappings) &#123; // 获取参数的名称 String propertyName = parameterMapping.getProperty(); // 如果元数据对象中存在该参数，则替换相应的占位符 “?” if (metaObject.hasGetter(propertyName)) &#123; Object obj = metaObject.getValue(propertyName); sql = sql.replaceFirst(\"\\\\?\", getFormatParameterValue(obj)); &#125; // 如果有额外的参数，走下面的逻辑 else if (boundSql.hasAdditionalParameter(propertyName)) &#123; Object obj = boundSql.getAdditionalParameter(propertyName); sql = sql.replaceFirst(\"\\\\?\", getFormatParameterValue(obj)); &#125; &#125; &#125; &#125; // 返回组装好的 SQL return sql; &#125; /** 获取实体对象类：class com.gorge4j.user.entity.UserManageDemoMyBatis */ private Class&lt;?&gt; getParameterType(MappedStatement statement) &#123; if (statement.getParameterMap() == null || statement.getParameterMap().getType() == null) &#123; return null; &#125; return statement.getParameterMap().getType(); &#125; /** 判断实体类上是否包含自定义的 SQl 打印的注解 @SqlPrint */ private static boolean hasSqlPrintAnnotation(Class&lt;?&gt; classType) &#123; return classType == null ? Boolean.FALSE : classType.isAnnotationPresent(SqlPrint.class); &#125;&#125; 注意：需要在实体类上添加自定义注解 @SqlPrint 自定义注解类： 12345@Retention(RUNTIME)@Target(TYPE)public @interface SqlPrint &#123;&#125; 自定义的 SQL 日志打印样式： 跨域 跨域，是指浏览器不能执行其他网站的脚本。它是由浏览器的同源策略造成的，是浏览器对 JavaScript 实施的安全策略。 123456789101112131415161718192021222324@Configurationpublic class CorsConfig extends WebMvcConfigurationSupport &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping(\"/**\") .allowedOrigins(\"*\") .allowCredentials(true) .allowedHeaders(\"*\") .allowedMethods(\"GET\", \"POST\", \"DELETE\", \"PUT\", \"OPTIONS\") .maxAge(3600); &#125; // 配置 Swagger 的资源映射 @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(\"swagger-ui.html\") .addResourceLocations(\"classpath:/META-INF/resources/\"); registry.addResourceHandler(\"/webjars/**\") .addResourceLocations(\"classpath:/META-INF/resources/webjars/\"); &#125;&#125; 注意12345/** * @see EnableWebMvc 用了此注解默认配置的 MVC 失效，需要用户重新配置 * @see WebMvcConfigurer */public class WebMvcConfigurationSupport ... 自定义校验 除了使用默认提供的 Validator，用户还可以提供具有自定义校验规则的 Validator。 注解类123456789@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = UsernameValidator.class) // 引入校验规则类进行校验public @interface Username &#123; // 错误提示消息 @Username(message = \"\") String message() default \"&#123;com.wingo.user.annotaion.Username.message&#125;\"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 12345678@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = PasswordValidator.class)public @interface Password &#123; String message() default \"&#123;com.wingo.user.annotaion.Password.message&#125;\"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 校验类123456789101112131415public class UsernameValidator implements ConstraintValidator&lt;Username, String&gt; &#123; @Override public void initialize(Username constraintAnnotation) &#123; // do nothing &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; if (StringUtils.isBlank(value)) &#123; return false; &#125; return Pattern.matches(RegexConstant.REGEX_USERNAME, value); &#125;&#125; 123456789101112131415public class PasswordValidator implements ConstraintValidator&lt;Password, String&gt; &#123; @Override public void initialize(Password constraintAnnotation) &#123; // do nothing &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; if (StringUtils.isBlank(value)) &#123; return false; &#125; // 正则匹配 return Pattern.matches(RegexConstant.REGEX_PASS, value); &#125;&#125; 请求响应打印 切面：方法前打印请求信息参数；方法后打印响应信息。 1234567891011121314151617181920212223242526272829303132333435363738394041@Slf4j@Component@Aspectpublic class ReqAndRespLogAspect &#123; /** * 请求参数的日志打印 * * @param joinPoint 切点 */ @Before(\"within(com.wingo.user.controller.*)\") public void before(JoinPoint joinPoint) &#123; // 获取传入目标方法的参数对象 Object[] args = joinPoint.getArgs(); // 获取封装了署名信息的对象,在该对象中可以获取到目标方法名,所属类的Class等信息 MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); // 获取方法对象 Method method = methodSignature.getMethod(); // 分割组装参数 String strArgs = StringUtils.join(args, \",\"); log.info(\"&#123;&#125;.&#123;&#125; - 请求参数: &#123;&#125;\", method.getDeclaringClass().getName(), method.getName(), strArgs); &#125; /** * 响应结果的日志打印 * * @param joinPoint 切点 * @param res 响应结果对象 */ @AfterReturning(value = \"within(com.wingo.user.controller.*)\", returning = \"res\") public void after(JoinPoint joinPoint, Object res) &#123; // 获取封装了署名信息的对象,在该对象中可以获取到目标方法名,所属类的 Class 等信息 MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); // 获取方法对象 Method method = methodSignature.getMethod(); // 将返回对象转换成 Json 字符串，项目的返回如果不是 Json 的数据格式，那么返回参数就只有 URL // String strRes = JSONUtils.toJSONString(res); String strRes = res.toString(); log.info(\"&#123;&#125;.&#123;&#125; - 返回结果: &#123;&#125;\", method.getDeclaringClass().getName(), method.getName(), strRes); &#125;&#125;","categories":[{"name":"项目开发","slug":"项目开发","permalink":"http://yoursite.com/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"用户管理系统","slug":"用户管理系统","permalink":"http://yoursite.com/tags/%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"}]},{"title":"Spring Boot 数据访问","slug":"后台技术/Spring Boot/Spring Boot 数据访问","date":"2020-03-05T07:42:42.000Z","updated":"2020-07-10T04:13:37.067Z","comments":true,"path":"2020/03/05/后台技术/Spring Boot/Spring Boot 数据访问/","link":"","permalink":"http://yoursite.com/2020/03/05/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/","excerpt":"Spring Boot 数据访问的相关知识。","text":"Spring Boot 数据访问的相关知识。 JDBC123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 1234567spring: datasource: username: root password: 123456 url: jdbc:mysql://localhosthost:3306/jdbc driver-class-name: com.mysql.jdbc.Driver # Spring Boot 2.1.x 默认使用了 MySQL 8.0：com.mysql.cj.jdbc.Driver Sping Boot 默认使用 org.apache.tomcat.jdbc.pool.DataSource 作为数据源，相关配置都在 DataSourceProperties 里面。 默认建表配置规则 1schema-*.sql 指定建表文件 sql 文件的文件名 1234spring: datasource: schema: - classpath:user.sql Sping Boot 自动配置了 JdbcTemplate 操作数据库 JdbcTempale创建一个 User 表： 1234CREATE TABLE `User` ( `name` varchar(100) COLLATE utf8mb4_general_ci NOT NULL, `age` int NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci 编写实体对象： 12345678@Data // Getter / Setter@NoArgsConstructor // 无参构造器public class User &#123; private String name; private Integer age;&#125; 数据访问对象接口： 123456789101112131415161718public interface UserService &#123; // 新增一个用户 int create(String name, Integer age); // 根据 name 查询用户 List&lt;User&gt; getByName(String name); // 根据 name 删除用户 int deleteByName(String name); // 获取用户总量 int getAllUsers(); // 删除所有用户 int deleteAllUsers();&#125; 数据访问操作的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private JdbcTemplate jdbcTemplate; UserServiceImpl(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; &#125; @Override public int create(String name, Integer age) &#123; return jdbcTemplate.update(\"insert into USER(NAME, AGE) values(?, ?)\", name, age); &#125; @Override public List&lt;User&gt; getByName(String name) &#123; List&lt;User&gt; users = jdbcTemplate.query (\"select NAME, AGE from USER where NAME = ?\", (resultSet, i) -&gt; &#123; User user = new User(); user.setName(resultSet.getString(\"NAME\")); user.setAge(resultSet.getInt(\"AGE\")); return user; &#125;, name); return users; &#125; @Override public int deleteByName(String name) &#123; return jdbcTemplate.update(\"delete from USER where NAME = ?\", name); &#125; @Override public int getAllUsers() &#123; return jdbcTemplate.queryForObject(\"select count(1) from USER\", Integer.class); &#125; @Override public int deleteAllUsers() &#123; return jdbcTemplate.update(\"delete from USER\"); &#125;&#125; 整合 Druid 数据源123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--数据库连接池--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/i-auth?useSSL=false# 可以不配置 Druid 可以根据 url 自动识别数据库驱动# spring.datasource.driver-class-name=com.mysql.jdbc.Driver# 初始化连接数spring.datasource.druid.initial-size=5# 最小连接数spring.datasource.druid.min-idle=10# 最大连接数spring.datasource.druid.max-active=10# 获取连接最长等待时间 单位毫秒spring.datasource.druid.max-wait=10000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.druid.timeBetweenEvictionRunsMillis=30000# 指定获取连接时连接校验的sql查询语句spring.datasource.druid.validationQuery=select 'x'# 获取连接后，确实是否要进行连接空闲时间的检查spring.datasource.druid.testWhileIdle=true# 获取连接检测spring.datasource.druid.testOnBorrow=false# 归还连接检测spring.datasource.druid.testOnReturn=false# 指定连接校验查询的超时时间spring.datasource.druid.validationQueryTimeout=600000# 配置一个连接在池总最小生存的时间spring.datasource.druid.minEvictableIdleTimeMillis=300000# 打开 PSCache，并且指定每个连接上 Cache 的大小spring.datasource.druid.poolPreparedStatements=truespring.datasource.druid.maxPoolPreparedStatementPerConnectionSize=20# 配置监控统计拦截的filter，去掉后监控界面sql无法统计，'wall'用于防火墙spring.datasource.druid.filters=stat,wall,slf4j# 通过connectProperties属性来打开mergeSql功能，慢sql记录等spring.datasource.druid.connectionProperties=druid.stat.mergeSql=true;druid.stat.logSlowSql=true;druid.stat.slowSqlMillis=5000## druid连接池监控# 需要账号密码才能访问控制台，默认为rootspring.datasource.druid.stat-view-servlet.login-username=adminspring.datasource.druid.stat-view-servlet.login-password=123# 访问路径为/druid时，跳转到StatViewServletspring.datasource.druid.stat-view-servlet.url-pattern=/druid/*# 是否能够重置数据spring.datasource.druid.stat-view-servlet.reset-enable=false# 排除一些静态资源，以提高效率spring.datasource.druid.web-stat-filter.exclusions=*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/* 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 导入 Druid 数据源@Configurationpublic class DruidConfig &#123; // 一个自定义的 DataSource 组件 @ConfigurationProperties(prefix = \"spring.datasource\") @Bean public DataSource druid()&#123; return new DruidDataSource(); &#125; // 配置 Druid 的监控 // 配置一个管理后台的 Servlet @Bean public ServletRegistrationBean statViewServlet()&#123; ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), \"/druid/*\"); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(\"loginUsername\",\"admin\"); initParams.put(\"loginPassword\",\"123456\"); initParams.put(\"allow\",\"\"); // 默认就是允许所有访问 initParams.put(\"deny\",\"192.168.15.21\"); bean.setInitParameters(initParams); return bean; &#125; // 配置一个 Web 监控的 Filter @Bean public FilterRegistrationBean webStatFilter()&#123; FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(\"exclusions\",\"*.js,*.css,/druid/*\"); bean.setInitParameters(initParams); bean.setUrlPatterns(Arrays.asList(\"/*\")); return bean; &#125;&#125; 整合 MyBatis可直接使用插件：Mybatis-Plus 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 注解版 1234567891011121314151617// 指定这是一个操作数据库的 Mapper@Mapperpublic interface DepartmentMapper &#123; @Select(\"select * from department where id=#&#123;id&#125;\") public Department getDeptById(Integer id); @Delete(\"delete from department where id=#&#123;id&#125;\") public int deleteDeptById(Integer id); @Options(useGeneratedKeys = true,keyProperty = \"id\") @Insert(\"insert into department(departmentName) values(#&#123;departmentName&#125;)\") public int insertDept(Department department); @Update(\"update department set departmentName=#&#123;departmentName&#125; where id=#&#123;id&#125;\") public int updateDept(Department department);&#125; 123456789// 使用 MapperScan 批量扫描所有的 Mapper 接口，或在 Mapper 类使用 Mapper@MapperScan(value = \"com.wingo.springboot.mapper\")@SpringBootApplicationpublic class SpringBootDataMybatisApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootDataMybatisApplication.class, args); &#125;&#125; 自定义 MyBatis 配置规则：驼峰命名法。 1234567891011121314@Configuration public class MyBatisConfig &#123; @Bean public ConfigurationCustomizer configurationCustomizer()&#123; return new ConfigurationCustomizer()&#123; @Override public void customize(Configuration configuration) &#123; configuration.setMapUnderscoreToCamelCase(true); &#125; &#125;; &#125; &#125; 配置文件版 12345mybatis: # 指定全局配置文件的位置 config-location: classpath:mybatis/mybatis-config.xml # 指定 Sql 映射文件的位置 mapper-locations: classpath:mybatis/mapper/*.xml 整合 SpringData JPA12345678910111213&lt;!-- JPA --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 编写一个实体类 Bean 和数据表进行映射，并且配置好映射关系： 1234567891011121314// 使用 JPA 注解配置映射关系@Entity // 告诉 JPA 这是一个实体类（和数据表映射的类）@Table(name = \"tbl_user\") // @Table 来指定和哪个数据表对应，如果省略默认表名就是 userpublic class User &#123; @Id // 这是一个主键 @GeneratedValue(strategy = GenerationType.IDENTITY) // 自增主键 private Integer id; @Column(name = \"last_name\",length = 50) // 这是和数据表对应的一个列 private String lastName; @Column // 省略默认列名就是属性名 private String email;&#125; 编写一个 Dao 接口来操作实体类对应的数据表： 123// 继承 JpaRepository 来完成对数据库的操作，传参：实体类型 主键类型public interface UserRepository extends JpaRepository&lt;User,Integer&gt; &#123;&#125; JpaProperties 基本配置 1234567spring: jpa: hibernate: # 更新或者创建数据表结构 ddl-auto: update # 控制台显示SQL show-sql: true","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://yoursite.com/tags/MyBatis/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"},{"name":"JPA","slug":"JPA","permalink":"http://yoursite.com/tags/JPA/"},{"name":"Druid","slug":"Druid","permalink":"http://yoursite.com/tags/Druid/"},{"name":"JDBC","slug":"JDBC","permalink":"http://yoursite.com/tags/JDBC/"}]},{"title":"Docker的使用","slug":"开发杂项/Docker 的使用","date":"2020-03-04T09:40:46.000Z","updated":"2020-07-10T03:02:10.457Z","comments":true,"path":"2020/03/04/开发杂项/Docker 的使用/","link":"","permalink":"http://yoursite.com/2020/03/04/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/Docker%20%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"Docker 是一个开源的应用容器引擎，是一个轻量级容器技术。","text":"Docker 是一个开源的应用容器引擎，是一个轻量级容器技术。 环境搭建CentOS 7 下载：阿里云镜像 VMware 创建虚拟机运行 CentOS 7 光盘映像文件。 选择 &gt; SYSTEM &gt; INSTALLATION DESTINATION，进入磁盘分区界面； 选择 &gt; I will configure partitioning &gt; Done &gt; partitioning scheme &gt; Standard Partition &gt; + &gt;： 挂载点：swap，期望容量：4096；挂载点：/，期望容量：留空。选择 &gt; SOFTWARE &gt; Minimal Install。Begin Install &gt; 设置密码 &gt; Reboot &gt; 初始化完成。 网络配置 CentOS 7.x 默认安装好之后是没有自动开启网络连接的，需要自行配置。 ！！！注意！！！VMWare 网络选择桥接模式。 1234cat /etc/sysconfig/network-scripts/ifcfg-ens33 # 默认的网络配置文件mv ifcfg-ens33 ifcfg-eth0 # 修改习惯性命名ip address # 查看 MACvi /etc/sysconfig/network-scripts/ifcfg-eth0 # 修改配置 注意：默认网关一致；DHCP 动态分配改为 STATIC 静态模式；静态 IP 地址填写与主机相同的网段内；MAC 地址随机分配，不可重复，复制主机的时候记得修改。一下是需要修改的配置： 修改主机名：hostnamectl set-hostname [new_hostname] --static Docker 安装1yum -y install docker 核心概念Docker 主机 Host：安装了 Docker 程序的机器（Docker 直接安装在操作系统之上）； Docker 客户端 Client：连接 Docker 主机进行操作； Docker 仓库 Registry：用来保存各种打包好的软件镜像； Docker 镜像 Images：软件打包好的镜像,放在 Docker 仓库中； Docker 容器 Container：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用。 Docker 使用 虚拟机记得设置桥接网络并选择对应的网卡接入网线。 1234# 设置好网络以后使用命令重启虚拟机的网络service network restart# 查看 Linux 的 IP 地址ip addr Linux 环境下 Docker 的安装： 12345678910111213# 检查内核版本，Docker 要求内核版本必须是 3.10 及以上uname -sr# 安装 Dockeryum install docker# 输入 y 确认安装# 启动 Dockersystemctl start docker# 查看版本docker -v# 开机启动 Dockersystemctl enable docker# 停止 Dockersystemctl stop docker Docker 常用命令操作 操作 命令 说明 检索 docker search [name] 我们经常去 docker hub 上检索镜像的详细信息，如镜像的 tag 拉取 docker pull [name]:tag :tag 是可选的，tag 表示标签，多为软件的版本，默认是 latest 列表 docker images 查看所有本地镜像 删除 docker rm image-id 删除指定的本地镜像 123456789101112131415161718192021222324252627# 搜索镜像docker search tomcat# 拉取镜像docker pull tomcat# 根据镜像启动容器，没有做映射无法正常访问docker run --name mytomcat -d tomcat:latest# 查看运行中的容器docker ps # 停止运行中的容器docker stop cantainerId# 查看所有的容器，运行中和已经退出的docker ps -a# 启动容器docker start cantainerId# 删除一个容器docker rm cantainerId# 启动一个做了端口映射的 tomcatdocker run -d -p 8888:8080 tomcat# -d：后台运行# -p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口# 查看防火墙状态service firewalld status# 关闭防火墙service firewalld stop# 查看容器的日志docker logs container-name/container-id# 具体镜像的启动操作可以参考每一个镜像的文档 常用镜像安装MySQL1234567891011# 安装并启动 MySQL 镜像docker pull mysqldocker run --name mysql01 -d mysqldocker ps -a# 发现容器异常退出了，查看错误日志docker logs containerId# 发现是因为没有指定账号密码# 正确启动并指定端口映射docker run -p 3306:3306 --name mysql01 -e MYSQL_ROOT_PASSWORD=123456 -d mysql# docker run --name mysql03 -v [自定义 MySQL 配置文件的目录]:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag# docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci RedisDocker Hub 查询 Redis 源。 123docker pull redisdocker images # 查看镜像名称docker run -d -p 6379:6379 --name myredis [镜像名称] # redis 客户端测试连接 RabbitMQ1docker run -d -p 5672:5672 -p 15672:15672 --hostname my_rabbitmq --name rabbitmq rabbitmq:management 常见错误1WARNING: IPv4 forwarding is disabled. Networking will not work. systemctl restart docker重启一下 Docker 服务即可。","categories":[{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Spring Boot Web 开发","slug":"后台技术/Spring Boot/Spring Boot Web 开发","date":"2020-03-03T06:08:47.000Z","updated":"2020-07-10T04:13:45.688Z","comments":true,"path":"2020/03/03/后台技术/Spring Boot/Spring Boot Web 开发/","link":"","permalink":"http://yoursite.com/2020/03/03/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20Web%20%E5%BC%80%E5%8F%91/","excerpt":"Spring Boot Web 方面的开发介绍。","text":"Spring Boot Web 方面的开发介绍。 自动配置原理： xxxAutoConfiguration：帮我们给容器中自动配置组件； xxxProperties：配置类封装配置文件的内容。 静态资源映射以 JAR 包的方引入静态资源： 123456&lt;!-- 引入 jquery-webjar 在访问的时候只需要写 webjars 下面资源的名称即可 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt; 访问 jquery.js：localhost:8080/webjars/jquery/3.3.1/jquery.js 访问当前项目的任何资源，都去静态资源文件夹找映射。 12345&quot;classpath:&#x2F;META-INF&#x2F;resources&#x2F;&quot;, &quot;classpath:&#x2F;resources&#x2F;&quot;,&quot;classpath:&#x2F;static&#x2F;&quot;, &quot;classpath:&#x2F;public&#x2F;&quot; &quot;&#x2F;&quot;：当前项目的根路径 Thymeleaf 模板引擎123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 默认是 2.1.6 版本 --&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 切换 Thymeleaf 版本 --&gt;&lt;properties&gt; &lt;thymeleaf.version&gt;3.0.9.RELEASE&lt;/thymeleaf.version&gt; &lt;!-- 布局功能的支持程序 Thymeleaf3 主程序 Layout2 以上版本 --&gt; &lt;!-- Thymeleaf2 layout1--&gt; &lt;thymeleaf-layout-dialect.version&gt;2.2.2&lt;/thymeleaf-layout-dialect.version&gt;&lt;/properties&gt; 自动配置Spriing Boot 对于 Thymeleaf 的自动配置 12345678910# Enable template cachingspring thymeleaf cache=true# Template encodingspring thymeleaf encoding=UTF-8# Template mode to be applied to templates. See also StandardTemplateModeHandlersspring thymeleaf mode=HTML5# Prefix that gets prepended to view names when building a URLspring thymeleaf prefix=classpath: /templates/# Suffix that gets appended to view names when building a URLspring thymeleaf suffix=.html 12345678910111213@ConfigurationProperties(prefix = \"spring.thymeleaf\")public class ThymeleafProperties &#123; private static final Charset DEFAULT_ENCODING = Charset.forName(\"UTF-8\"); private static final MimeType DEFAULT_CONTENT_TYPE = MimeType.valueOf(\"text/html\"); public static final String DEFAULT_PREFIX = \"classpath:/templates/\"; public static final String DEFAULT_SUFFIX = \".html\"; // ...&#125; 从自动配置类中可以看出只要我们把 HTML 页面放在classpath:/templates/目录下 Thymeleaf 就能自动渲染。 Thymeleaf 的使用123456789101112&lt;!DOCTYPE html&gt;&lt;!-- 导入 Thymeleaf 的名称空间 --&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- th:text 设置 div 里面的文本内容 --&gt; &lt;div th:text=\"$&#123;hello&#125;\"&gt;这是显示欢迎信息&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 常用标签 常用表达式 123456789101112131415161718192021行内写法：[[]] -&gt; th:text 会转义特殊字符[()] -&gt; th:utext 不会转义特殊字符内置工具类：#execInfo : information about the template being processed.#messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #&#123;…&#125; syntax.#uris : methods for escaping parts of URLs&#x2F;URIs#conversions : methods for executing the configured conversion service (if any).#dates : methods for java.util.Date objects: formatting, component extraction, etc.#calendars : analogous to #dates , but for java.util.Calendar objects.#numbers : methods for formatting numeric objects.#strings : methods for String objects: contains, startsWith, prepending&#x2F;appending, etc.#objects : methods for objects in general.#bools : methods for boolean evaluation.#arrays : methods for arrays.#lists : methods for lists.#sets : methods for sets.#maps : methods for maps.#aggregates : methods for creating aggregates on arrays or collections.#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). Spring MVC 自动配置Sping Boot 对 Spring MVC 的默认配置：WebMvcAutoConfiguration 12345678910//使用 WebMvcConfigurerAdapter 可以来扩展 SpringMVC 的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // 浏览器发送 /wingo 请求来到 success registry.addViewController(\"/wigno\").setViewName(\"success\"); &#125;&#125; 不能标注@EnableWebMvc，标注后 Spring MVC 的默认自动装配将不会进行，即开发者全面接管 Spring MVC Restful CRUD默认访问首页： 1234567891011121314151617// 使用 WebMvcConfigurerAdapter 可以来扩展 Spring MVC 的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; // 所有的 WebMvcConfigurerAdapter 组件都会一起起作用 @Bean // 将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(\"/\").setViewName(\"login\"); registry.addViewController(\"/index.html\").setViewName(\"login\"); &#125; &#125;; return adapter; &#125;&#125; 国际化IDEA 会自动识别国际化目录。 IDEA 进行优化后的便于编辑的视图：（点击左下角的 Resource Bundle） Spring Boot 中的消息自动配置： 12345678public class MessageSourceAutoConfiguration &#123; // ... // 默认配置为 spring.messages.basename=messages 即国际化资源默认放在类路径下的 messages.properties String basename = context.getEnvironment().getProperty(\"spring.messages.basename\", \"messages\"); //... &#125; 12# 自定义国际化文件的位置spring.message.basename=i18n.login 页面获取国际化的信息： 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"&gt; &lt;meta name=\"description\" content=\"\"&gt; &lt;meta name=\"author\" content=\"\"&gt; &lt;title&gt;Signin Template for Bootstrap&lt;/title&gt; &lt;!-- webjars 形式的静态资源映射 --&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href=\"asserts/css/bootstrap.min.css\" th:href=\"@&#123;/webjars/bootstrap/4.0.0/css/bootstrap.css&#125;\" rel=\"stylesheet\"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href=\"asserts/css/signin.css\" th:href=\"@&#123;/asserts/css/signin.css&#125;\" rel=\"stylesheet\"&gt; &lt;/head&gt; &lt;body class=\"text-center\"&gt; &lt;form class=\"form-signin\" action=\"dashboard.html\"&gt; &lt;img class=\"mb-4\" th:src=\"@&#123;/asserts/img/bootstrap-solid.svg&#125;\" src=\"asserts/img/bootstrap-solid.svg\" alt=\"\" width=\"72\" height=\"72\"&gt; &lt;!-- Thymeleaf 用 #&#123;&#125; 表达式取出国际化信息 --&gt; &lt;h1 class=\"h3 mb-3 font-weight-normal\" th:text=\"#&#123;login.tip&#125;\"&gt;Please sign in&lt;/h1&gt; &lt;label class=\"sr-only\" th:text=\"#&#123;login.username&#125;\"&gt;Username&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" placeholder=\"Username\" th:placeholder=\"#&#123;login.username&#125;\" required=\"\" autofocus=\"\"&gt; &lt;label class=\"sr-only\" th:text=\"#&#123;login.password&#125;\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" placeholder=\"Password\" th:placeholder=\"#&#123;login.password&#125;\" required=\"\"&gt; &lt;div class=\"checkbox mb-3\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"remember-me\"/&gt; [[#&#123;login.remember&#125;]] &lt;/label&gt; &lt;/div&gt; &lt;button class=\"btn btn-lg btn-primary btn-block\" type=\"submit\" th:text=\"#&#123;login.btn&#125;\"&gt;Sign in&lt;/button&gt; &lt;p class=\"mt-5 mb-3 text-muted\"&gt;© 2017-2018&lt;/p&gt; &lt;a class=\"btn btn-sm\"&gt;中文&lt;/a&gt; &lt;a class=\"btn btn-sm\"&gt;English&lt;/a&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 根据浏览器请求中所携带的国际化 Locale 信息进行语言的转换。 123456789101112131415@Bean@ConditionalOnMissingBean@ConditionalOnProperty( prefix = \"spring.mvc\", name = &#123;\"locale\"&#125;)public LocaleResolver localeResolver() &#123; if (this.mvcProperties.getLocaleResolver() == org.springframework.boot.autoconfigure.web.servlet.WebMvcProperties.LocaleResolver.FIXED) &#123; return new FixedLocaleResolver(this.mvcProperties.getLocale()); &#125; else &#123; AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver(); localeResolver.setDefaultLocale(this.mvcProperties.getLocale()); return localeResolver; &#125;&#125; 可以在切换语言的的链接上携带区域信息，进行语言切换。 12345678910111213141516171819202122public class MyLocaleResolver implements LocaleResolver &#123; @Override public Locale resolveLocale(HttpServletRequest request) &#123; String myLocale = request.getParameter(\"locale\"); Locale locale = Locale.getDefault(); if(!StringUtils.isEmpty(myLocale))&#123; String[] split = myLocate.split(\"_\"); //国家 语言 locale = new Locale(split[0],split[1]); &#125; return locale; &#125; @Override public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) &#123;&#125; @Bean public LocaleResolver localeResolver()&#123; return new MyLocaleResolver(); &#125;&#125; 用户登录12# 禁用缓存spring.thymeleaf.cache&#x3D;false 修改了页面后 Ctrl + F9 进行重新编译 123&lt;!-- 登录错误消息显示 --&gt;&lt;!-- 使用 Thymeleaf 内置工具类 #Srting, 取传递变量的值 $&#123;&#125; 表达式--&gt;&lt;p style=\"color: red\" th:text=\"$&#123;msg&#125;\" th:if=\"$&#123;not #strings.isEmpty(msg)&#125;\"&gt;&lt;/p&gt; 拦截器拦截器的编写： 12345678910111213141516171819public class LoginHandlerInterceptor implements HandlerInterceptor &#123; // 目标方法执行之前 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 用户登录后在 Session 中保存用户标志 Object user = request.getSession().getAttribute(\"loginUser\"); if(user == null)&#123; // 未登陆，返回登陆页面 request.setAttribute(\"msg\",\"没有权限请先登陆\"); request.getRequestDispatcher(\"/index.html\").forward(request,response); return false; &#125;else&#123; // 已登陆，放行请求 return true; &#125; &#125; // 省略其它不需要编写的抽象方法&#125; 注册拦截器 123456789101112131415161718192021// 所有的 WebMvcConfigurerAdapter 组件都会一起起作用@Bean // 将组件注册在容器public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(\"/\").setViewName(\"login\"); registry.addViewController(\"/index.html\").setViewName(\"login\"); &#125; // 注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; // super.addInterceptors(registry); // 静态资源 *.css , *.js SpringBoot 已经做好了映射 // 除了访问登录页面的 URL 其余的 URL 全部进行拦截认证 registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns(\"/**\") .excludePathPatterns(\"/index.html\",\"/\",\"/user/login\"); &#125; &#125;; return adapter;&#125; 员工列表 普通CRUD（URI 来区分操作） RestfulCRUD 查询 getEmp emp—GET 添加 addEmp?xxx emp—POST 修改 updateEmp?id=xxx&amp;xxx=xx emp/{id}—PUT 删除 deleteEmp?id=1 emp/{id}—DELETE 本次实验的请求架构： 实验功能 请求URI 请求方式 查询所有员工 emps GET 查询某个员工（来到修改页面） emp/{id} GET 来到添加页面 emp GET 添加员工 emp POST 来到修改页面（查出员工进行信息回显） emp/{id} GET 修改员工 emp PUT 删除员工 emp/{id} DELETE 员工列表，Thymeleaf 公共页面元素抽取。 123456789101112131415161718192021222324252627282930&lt;!-- common/footer.html 抽取公共片段 --&gt;&lt;footer th:fragment=\"copy\"&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;!-- 引入公共片段 --&gt;&lt;div th:insert=\"footer::copy\"&gt;&lt;/div&gt;&lt;div th:replace=\"footer::copy\"&gt;&lt;/div&gt;&lt;div th:include=\"footer::copy\"&gt;&lt;/div&gt;&lt;!-- ~&#123;templatename::selector&#125;：模板名::选择器~&#123;templatename::fragmentname&#125;:模板名::片段名insert 的公共片段在 div 标签中，如果使用 th:insert 等属性进行引入，可以不用写 ~&#123;&#125;；；行内写法可以加上：[[~&#123;&#125;]] [(~&#123;&#125;)]--&gt;&lt;!-- 效果 --&gt;&lt;div&gt; &lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery &lt;/footer&gt;&lt;/div&gt;&lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;div&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt; 引入片段时传递参数： 12345678&lt;!--引入侧边栏并传入参数；此处用的 id 选择器来引入片段 --&gt;&lt;div th:replace=\"commons/bar::#sidebar(activeUri='emps')\"&gt;&lt;/div&gt;&lt;!-- 应用场景：点击侧边栏的选项，被点击的选项样式发生改变，变为高亮 --&gt;&lt;!-- 解决方式：页面引用时所传递的参数进行样式的修改，生成的侧边栏样式根据传入的参数改变样式 --&gt;&lt;a class=\"nav-link active\" th:class=\"$&#123;activeUri=='main.html'?'nav-link active':'nav-link'&#125;\" href=\"#\" th:href=\"@&#123;/main.html&#125;\"&gt; 添加及修改123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!-- 需要区分是员工修改还是添加 --&gt;&lt;form th:action=\"@&#123;/emp&#125;\" method=\"post\"&gt;&lt;!-- 发送 put 请求修改员工数据--&gt; &lt;!-- Spring MVC 中配置 HiddenHttpMethodFilter；（SpringBoot自动配置好的） 页面创建一个post表单； 创建一个 input 项，name=\"_method\"；value 就是我们指定的请求方式。 --&gt; &lt;!-- 若 emp 携带信息，则此表单提交的是 put 请求 --&gt; &lt;input type=\"hidden\" name=\"_method\" value=\"put\" th:if=\"$&#123;emp!=null&#125;\"/&gt; &lt;!-- 修改需要携带员工 id 用于保存修改 --&gt; &lt;input type=\"hidden\" name=\"id\" th:if=\"$&#123;emp!=null&#125;\" th:value=\"$&#123;emp.id&#125;\"&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;LastName&lt;/label&gt; &lt;input name=\"lastName\" type=\"text\" class=\"form-control\" placeholder=\"zhangsan\" th:value=\"$&#123;emp!=null&#125;?$&#123;emp.lastName&#125;\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input name=\"email\" type=\"email\" class=\"form-control\" placeholder=\"zhangsan@atguigu.com\" th:value=\"$&#123;emp!=null&#125;?$&#123;emp.email&#125;\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;Gender&lt;/label&gt;&lt;br/&gt; &lt;div class=\"form-check form-check-inline\"&gt; &lt;input class=\"form-check-input\" type=\"radio\" name=\"gender\" value=\"1\" th:checked=\"$&#123;emp!=null&#125;?$&#123;emp.gender==1&#125;\"&gt; &lt;label class=\"form-check-label\"&gt;男&lt;/label&gt; &lt;/div&gt; &lt;div class=\"form-check form-check-inline\"&gt; &lt;input class=\"form-check-input\" type=\"radio\" name=\"gender\" value=\"0\" th:checked=\"$&#123;emp!=null&#125;?$&#123;emp.gender==0&#125;\"&gt; &lt;label class=\"form-check-label\"&gt;女&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;department&lt;/label&gt; &lt;!--提交的是部门的id--&gt; &lt;select class=\"form-control\" name=\"department.id\"&gt; &lt;option th:selected=\"$&#123;emp!=null&#125;?$&#123;dept.id == emp.department.id&#125;\" th:value=\"$&#123;dept.id&#125;\" th:each=\"dept:$&#123;depts&#125;\" th:text=\"$&#123;dept.departmentName&#125;\"&gt;1&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;Birth&lt;/label&gt; &lt;!-- #dates 工具类进行时间格式化 --&gt; &lt;input name=\"birth\" type=\"text\" class=\"form-control\" placeholder=\"zhangsan\" th:value=\"$&#123;emp!=null&#125;?$&#123;#dates.format(emp.birth, 'yyyy-MM-dd HH:mm')&#125;\"&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-primary\" th:text=\"$&#123;emp!=null&#125;?'修改':'添加'\"&gt;添加&lt;/button&gt;&lt;/form&gt; 删除12345678910111213141516171819202122&lt;tr th:each=\"emp:$&#123;emps&#125;\"&gt; &lt;td th:text=\"$&#123;emp.id&#125;\"&gt;&lt;/td&gt; &lt;td&gt;[[$&#123;emp.lastName&#125;]]&lt;/td&gt; &lt;td th:text=\"$&#123;emp.email&#125;\"&gt;&lt;/td&gt; &lt;td th:text=\"$&#123;emp.gender&#125;==0?'女':'男'\"&gt;&lt;/td&gt; &lt;td th:text=\"$&#123;emp.department.departmentName&#125;\"&gt;&lt;/td&gt; &lt;td th:text=\"$&#123;#dates.format(emp.birth, 'yyyy-MM-dd HH:mm')&#125;\"&gt;&lt;/td&gt; &lt;td&gt; &lt;a class=\"btn btn-sm btn-primary\" th:href=\"@&#123;/emp/&#125;+$&#123;emp.id&#125;\"&gt;编辑&lt;/a&gt; &lt;!-- th:attr 标签自定义属性及值 --&gt; &lt;button th:attr=\"del_uri=@&#123;/emp/&#125;+$&#123;emp.id&#125;\" class=\"btn btn-sm btn-danger deleteBtn\"&gt;删除&lt;/button&gt; &lt;/td&gt;&lt;/tr&gt;&lt;script&gt; $(\".deleteBtn\").click(function()&#123; // 删除当前员工 $(\"#deleteEmpForm\").attr(\"action\",$(this).attr(\"del_uri\")).submit(); return false; &#125;);&lt;/script&gt; 错误处理机制Spring Boot 默认的错误处理机制。 默认效果：对于浏览器，Spring Boot 返回一个默认的错误页面；对于其它客户端，默认响应一个 Json 数据。 1234567891011// 帮我们在页面共享信息@Overridepublic Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;String, Object&gt;(); errorAttributes.put(\"timestamp\", new Date()); addStatus(errorAttributes, requestAttributes); addErrorDetails(errorAttributes, requestAttributes, includeStackTrace); addPath(errorAttributes, requestAttributes); return errorAttributes;&#125; 1234567891011121314151617181920212223242526// 处理默认 /error 请求@Controller@RequestMapping(\"$&#123;server.error.path:$&#123;error.path:/error&#125;&#125;\") // 默认为错误页面路径 /errorpublic class BasicErrorController extends AbstractErrorController &#123; @RequestMapping(produces = \"text/html\") // 产生 HTML 类型的数据；浏览器发送的请求来到这个方法处理 public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) &#123; HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); // 去哪个页面作为错误页面，包含页面地址和页面内容 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView == null ? new ModelAndView(\"error\", model) : modelAndView); &#125; @RequestMapping @ResponseBody // 产生 Json 数据，其他客户端来到这个方法处理 public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(body, status); &#125; 1234567891011121314151617181920212223@Overridepublic ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) &#123; ModelAndView modelAndView = resolve(String.valueOf(status), model); if (modelAndView == null &amp;&amp; SERIES_VIEWS.containsKey(status.series())) &#123; modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); &#125; return modelAndView;&#125;private ModelAndView resolve(String viewName, Map&lt;String, Object&gt; model) &#123; // 默认 Spring Boot 可以去找到一个页面 error/404 String errorViewName = \"error/\" + viewName; // 模板引擎可以解析这个页面地址就用模板引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); if (provider != null) &#123; // 模板引擎可用的情况下返回到 errorViewName 指定的视图地址 return new ModelAndView(errorViewName, model); &#125; //模板引擎不可用，就在静态资源文件夹下找 errorViewName 对应的页面 error/404.html return resolveResource(errorViewName, model);&#125; 一但系统出现 4xx 或者 5xx 之类的错误，ErrorPageCustomizer 就会生效（定制错误的响应规则），就会来到 /error 请求，就会被BasicErrorController处理。 错误页面的定制有模板引擎的情况下：error/状态码（将错误页面命名为 错误状态码 .html 放在模板引擎文件夹里面的 error 文件夹下）发生此状态码的错误就会来到对应的页面。 可以使用 4xx 和 5xx 作为错误页面的文件名来匹配这种类型的所有错误，精确优先。（优先寻找精确的 [状态码].html） 1234567页面能获取的信息： timestamp：时间戳 status：状态码 error：错误提示 exception：异常对象 message：异常消息 errors：JSR303 数据校验的错误都在这里 没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹下找，以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面。 12345678910111213141516// 自定义异常处理 &amp; 返回定制 Json 数据@ControllerAdvicepublic class MyExceptionHandler &#123; @ResponseBody @ExceptionHandler(UserNotExistException.class) public Map&lt;String,Object&gt; handleException(Exception e)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 传入我们自己的错误状态码 4xx 5xx，否则就不会进入定制错误页面的解析流程 request.setAttribute(\"javax.servlet.error.status_code\",500); map.put(\"code\",\"user.notexist\"); map.put(\"message\",e.getMessage()); // 用于错误信息的读取 request.setAttribute(\"userNotExist\",map); return \"forward:/error\"; &#125;&#125; 12345678910111213141516// 给容器中加入我们自己定义的 ErrorAttributes@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes &#123; @Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; // 这个 map 就是页面和 Json 能获取的所有字段 Map&lt;String, Object&gt; map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put(\"company\", \"Welab\"); // 异常处理器携带的数据 Map&lt;String, Object&gt; map userNotExist = (Map&lt;String, Object&gt;)requestAttributes.getAttribute(\"userNotExist\",0); map.put(\"userNotExist\", userNotExist) return map; &#125;&#125; 嵌入式 Servlet 容器由于 Spring Boot 默认是以 Jar 包的方式启动嵌入式的 Servlet 容器来启动 Spring Boot 的 web 应用，没有 web.xml 文件。 Spring Boot 修改和 server 有关的配置： 123456# 方式一：propertiesserver.port=8081server.context-path=/crudserver.tomcat.uri-encoding=UTF-8# 通用的 Servlet 容器设置 server.xxx# Tomcat 的设置 server.tomcat.xxx 1234567891011// 编写嵌入式的 Servlet 容器的定制器来修改 Servlet 容器的配置@Bean //一定要将这个定制器加入到容器中public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer()&#123; return new EmbeddedServletContainerCustomizer() &#123; //定制嵌入式的Servlet容器相关的规则 @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.setPort(8083); &#125; &#125;;&#125; Servlet 三大组件的注册123456@Beanpublic ServletRegistrationBean myServlet()&#123; ServletRegistrationBean registrationBean = new ServletRegistrationBean( new MyServlet(),\"/myServlet\"); return registrationBean;&#125; 1234567@Beanpublic FilterRegistrationBean myFilter()&#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(new MyFilter()); registrationBean.setUrlPatterns(Arrays.asList(\"/hello\",\"/myServlet\")); return registrationBean;&#125; 123456@Beanpublic ServletListenerRegistrationBean myListener()&#123; ServletListenerRegistrationBean&lt;MyListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new MyListener()); return registrationBean;&#125; 替换 Servlet 容器 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 引入 web 模块默认就是使用嵌入式的 Tomcat 作为 Servlet 容器 --&gt;&lt;/dependency&gt; 123456789101112131415161718&lt;!-- 引入web模块 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--引入其他的Servlet容器--&gt;&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;!-- spring-boot-starter-undertow --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;/dependency&gt; 外置 Servlet 容器！！！创建的必须是一个 war 项目（利用 IDEA 创建好目录结构） 123456&lt;!-- 将嵌入式的 Tomcat 指定为 provided --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 必须要编写一个 SpringBootServletInitializer 的子类，并调用 configure() 方法。 123456789public class ServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; // 传入 SpringBoot 应用的主程序 return application.sources(SpringBootMyApplication.class); &#125;&#125; REST 架构风格","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"},{"name":"Web","slug":"Web","permalink":"http://yoursite.com/tags/Web/"}]},{"title":"Spring Boot 日志","slug":"后台技术/Spring Boot/Spring Boot 日志框架","date":"2020-03-02T02:27:24.000Z","updated":"2020-07-10T04:13:32.475Z","comments":true,"path":"2020/03/02/后台技术/Spring Boot/Spring Boot 日志框架/","link":"","permalink":"http://yoursite.com/2020/03/02/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/","excerpt":"Spring Boot 日志框架的使用。","text":"Spring Boot 日志框架的使用。 日志框架市面上常见的日志框架： 日志门面 （日志的抽象层） 日志实现 JCL: Jakarta Commons Logging;SLF4j: Simple Logging Facade for Java;Jboss-Logging Log4JJUL: java.util.logging;Log4J2 Logback Spring Boot 选用 SLF4J 和 Logback Q：Spring Boot 整合了很多框架，若整合的框架默认使用的日志框架与 Spring Boot 默认使用的日志框架不一样时，要怎么做？ A：排除整合框架的默认日志框架，用中间包来替代原有的日志框架。 SLF4J在开发的中，日志记录方法的调用不应该来直接调用日志的实现类，而是调用日志抽象层里面的方法。 12345678910// 给系统里面导入 SLF4J 的 JAR 和 Logback 的实现 JARimport org.slf4j.Logger;import org.slf4j.LoggerFactory;public class HelloWorld &#123; public static void main(String[] args) &#123; Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(\"Hello World\"); &#125;&#125; SLF4J 做为不同的日志实现框架的门面的配置结构： 统一日志记录： 将系统中其他日志框架先排除出去； 用中间包来替换原有的日志框架； 导入 SLF4J 的其他的实现。 Spring Boot 日志关系1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot 内配置的排除 Spring 的 Commons-Logging： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 日志的使用日志的级别： 12345678// 由低到高 trace &lt; debug &lt; info &lt; warn &lt; error// 可以调整输出的日志级别；日志就只会在这个级别以以后的高级别生效logger.trace(\"这是trace日志...\");logger.debug(\"这是debug日志...\");// SpringBoot 默认给我们使用的是 info 级别（root 级别）logger.info(\"这是info日志...\");logger.warn(\"这是warn日志...\");logger.error(\"这是error日志...\"); 日志输出格式： 1234567日志输出格式： %d表示日期时间; %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %thread表示线程名; %-5level：级别从左显示5个字符宽度; %logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割; %msg：日志消息; %n是换行符。 Spring Boot 修改日志的默认设置 1234567891011121314logging.level.com.wingo=trace # 指定某个包路径下的日志输出等级# logging.path=# 不指定路径在当前项目下生成 springboot.log 日志# 可以指定完整的路径；# logging.file=G:/springboot.log# 在当前磁盘的根路径下创建 spring 文件夹和里面的 log 文件夹；使用 spring.log 作为默认文件# logging.path=/spring/log# 在控制台输出的日志的格式logging.pattern.console=%d&#123;yyyy-MM-dd&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n# 指定文件中日志输出的格式logging.pattern.file=%d&#123;yyyy-MM-dd&#125; === [%thread] === %-5level === %logger&#123;50&#125; ==== %msg%n 指定配置 给类路径下放上每个日志框架自己的配置文件即可，SpringBoot 就不使用他默认配置的了。 Logging System Customization Logback logback-spring.xml, logback-spring.groovy, logback.xml or logback.groovy Log4J2 log4j2-spring.xml or log4j2.xml JDK (Java Util Logging) logging.properties logback.xml：直接就被日志框架识别了； logback-spring.xml：日志框架就不直接加载日志的配置项，由 SpringBoot 解析日志配置，可以使用 SpringBoot 的高级 Profile 功能。 SpringBoot 的高级 Profile 功能： 123&lt;springProfile name=\"staging\"&gt; &lt;!-- configuration to be enabled when the \"staging\" profile is active --&gt;&lt;/springProfile&gt; 配置文件12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt; &lt;!-- 先定义所有的 appender --&gt; &lt;appenders&gt; &lt;!-- 输出控制台的配置 --&gt; &lt;console name=\"Console target=SYSTEM_OUT\"&gt; &lt;!-- 输出日志的格式 --&gt; &lt;patternlayout pattern=\"[%p] &gt;&gt;&gt; %m%n\"/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;!-- 定义各种包下的日志的输出级别 --&gt; &lt;root level=\"DEBUG\"&gt; &lt;!-- 输出到控制台 --&gt; &lt;appender-ref ref=\"Console\"/&gt; &lt;/root&gt; &lt;logger name=\"org.springframework\" level=\"ERROR\"/&gt; &lt;logger name=\"com.baomidou\" level=\"ERROR\"/&gt; &lt;logger name=\"org.hibernate\" level=\"ERROR\"/&gt; &lt;logger name=\"com. alibaba druid\" level=\"ERROR\"/&gt; &lt;/loggers&gt;&lt;/configuration&gt; 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration scan=\"true\" scanPeriod=\"60 seconds\" debug=\"false\"&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;property name=\"log.path\" value=\"C:\\Users\\wingo\\Documents\\Recent\\log\" /&gt; &lt;!--输出到控制台--&gt; &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!-- &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt;--&gt; &lt;encoder&gt; &lt;pattern&gt;[%p] &gt;&gt;&gt; %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--输出到文件--&gt; &lt;appender name=\"file\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/logback.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %contextName [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=\"info\"&gt; &lt;appender-ref ref=\"console\" /&gt; &lt;appender-ref ref=\"file\" /&gt; &lt;/root&gt; &lt;!-- logback为java中的包 --&gt; &lt;logger name=\"com.wingo.controller\"/&gt; &lt;!--logback.LogbackDemo：类的全路径 --&gt; &lt;logger name=\"com.wingo.controller.LearnController\" level=\"WARN\" additivity=\"false\"&gt; &lt;appender-ref ref=\"console\"/&gt; &lt;/logger&gt;&lt;/configuration&gt;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"},{"name":"Log","slug":"Log","permalink":"http://yoursite.com/tags/Log/"}]},{"title":"Spring Boot 入门配置","slug":"后台技术/Spring Boot/Spring Boot 入门配置","date":"2020-02-29T02:48:07.000Z","updated":"2020-07-10T04:13:31.158Z","comments":true,"path":"2020/02/29/后台技术/Spring Boot/Spring Boot 入门配置/","link":"","permalink":"http://yoursite.com/2020/02/29/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E5%85%A5%E9%97%A8%E9%85%8D%E7%BD%AE/","excerpt":"Spring Boot 2.x 的 Hello World 以及配置详解。","text":"Spring Boot 2.x 的 Hello World 以及配置详解。 背景：J2EE 笨重的开发、繁多的配置、底下的开发效率吧、复杂的部署流程、第三方技术集成难度大。 解决：Spring 全家桶 👉 Spring Boot J2EE 一站式解决方案 👉 Spring Cloud 分布式整体解决方案 Spring Boot 简介 简化 Spring 应用开发的一个框架； 整合 Spring 技术栈的一个大集合； J2EE 一站式解决方案。 微服务：一种架构风格，即一个应用应该是一组小型服务，这些小服务之间可以通过 HTTP 的方式进行通信，每一个功能元素最终都是一个可独立替换和独立升级的软件单元。 Spring Boot HelloWorld 功能：浏览器发送 hello 请求，服务器接受请求并处理，响应 Hello World 字符串 项目生成项目生成地址：Spring Initiizr 填写项目的基本信息后点击 Generate 下载生成的项目。 将项目解压到相应目录，打开 IDEA 导入项目。（File 👉 New 👉 Project from Existing Sources） 选择以 Maven 项目的方式导入。 设置 JDK ，点击 Environment settings 配置 Maven。 项目导入成功后的项目目录。 resources文件夹中目录结构 static：保存所有的静态资源； js css images 等； templates：保存所有的模板页面；（Spring Boot 默认 jar 包使用嵌入式的 Tomcat，默认不支持 JSP 页面）；可以使用模板引擎（freemarker、thymeleaf）； application.properties：Spring Boot 应用的配置文件；可以修改一些默认设置； 代码编写在 pom.xml 中加入 Spring Boot 的相关依赖。 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 功能场景抽取成 Starter 来导入相关依赖 --&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启动 Spring Boot 应用的主程序 1234567891011/** * @author wingo */@SpringBootApplicationpublic class JavaSpringbootWingoApplication &#123; public static void main(String[] args) &#123; run(JavaSpringbootWingoApplication.class, args); &#125;&#125; 编写相关的 Controller 123456789@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping(\"/hello\") public String hello()&#123; return \"Hello World!\"; &#125;&#125; 运行测试控制台打印中可看到 Tomcat 在其默认端口 8080 被开启。 访问：localhost:8080/hello 访问成功，此 Spring Boot 项目可正常运行。 简化部署123456789&lt;!-- 这个插件可以将应用打包成一个可执行的jar包，在 cmd 直接用 java -jar 命令执行 --&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Hello World 探究项目依赖spring-boot-starter-Xxx：启动器 Spring Boot 场景启动器，用于自动导入了场景运行所依赖的组件。 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- Starter 启动器的父项目 --&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt; 点击进入 spring-boot-starter-parent，它的父项目是 spring-boot-dependencies。 1234567&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 定义了各种依赖的版本号 --&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt; 主程序类（入口类）Spring Boot 用@SpringBootApplication来标注一个主程序类，说明这是一个 Spring Boot 应用。 123456@SpringBootConfiguration // 这是一个 Spring Boot 配置类@EnableAutoConfiguration // 开启自动配置功能@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;&#125; Spring Boot 在启动的时候从类路径下的 META-INF/spring.factories 中获取 EnableAutoConfiguration 指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作。 按住 Ctrl 点击一个自动装配类： 发现有大量的@Conditional派生类，必须是在@Conditional指定的条件成立的情况下，才给容器中添加组件，配置里面的所有内容才生效。 可以通过 debug=true 属性让控制台打印自动配置报告。 Spring Boot 配置文件 application.properties application.yml YAML以数据为中心。 123server: port: 8081 path: /hello 基本语法： k:(空格)v 表示一对键值对（大小写敏感）； 以空格的缩进来控制层级关系；只要是左对齐的一列数据，都是同一个层级的。 字符串字符串默认不用加上单引号或者双引号： 单引号：会转义特殊字符，特殊字符最终只是一个普通的字符串数据； 双引号：不会转义字符串里面的特殊字符，特殊字符会作为本身想表示的意思。 对象、Map： 123456friends: lastName: Wen age: 23# 行内写法friends: &#123;lastName: Wen,age: 23&#125; 数组、List、Set： 123456pets: - cat - dog # 行内写法friends: [cat,dog] 配置文件值的注入配置文件： 123456789101112person: lastName: hello age: 18 boss: false birth: 2017/12/12 maps: &#123;k1: v1,k2: 12&#125; lists: - lisi - zhaoliu dog: name: 小狗 age: 12 Java Bean 将配置文件中配置的每一个属性的值，映射到这个组件中。 只有这个组件是容器中的组件，才能使用容器提供的@ConfigurationProperties功能 @ConfigurationProperties告诉 SpringBoot 将本类中的所有属性和配置文件中相关的配置进行绑定； prefix = &quot;person&quot;配置文件中哪个下面的所有属性进行一一映射。 123456789101112131415161718192021/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = \"person\"：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能使用容器提供的@ConfigurationProperties功能； * */@Component@ConfigurationProperties(prefix = \"person\")public class Person &#123; private String lastName; private Integer age; private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog;&#125; 导入配置文件处理器，这样编写配置时会有提示。 123456&lt;!-- 导入配置文件处理器，配置文件进行绑定就会有提示 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; properties 配置文件在 IDEA 中默认 UTF-8 可能会乱码，勾选 Setting - File Encodings - Transparent native-to-ascii conversion。 @Value获取值和@ConfigurationProperties获取值比较 @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 不支持 SpEL 不支持 支持 JSR303数据校验 支持 不支持 复杂类型封装 支持 不支持 配置文件 yml 还是 properties 它们都能获取到值： 只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value； 专门编写了一个 JavaBean 来和配置文件进行映射，我们就直接使用@ConfigurationProperties； 配置文件注入校验数据。 12345678//...@Validated // 开启校验public class Person &#123; //... @Email // 必须为邮件格式 private String lastName; //...&#125; 配置文件的加载@PropertySource：加载指定的配置文件。 1@PropertySource(value = &#123;\"classpath:person.properties\"&#125;) @ImportResource：让 Spring 的配置文件生效，加载进来。Spring Boot 里面没有 Spring 的配置文件，我们自己编写的配置文件，也不能自动识别。 1@ImportResource(locations = &#123;\"classpath:beans.xml\"&#125;) Spring Boot 推荐使用全注解的方式。 12345678910@Configuration // 指明当前类是一个 Spring 配置类public class MyAppConfig &#123; //将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名 @Bean public HelloService helloService()&#123; System.out.println(\"配置类 @Bean 给容器中添加组件了...\"); return new HelloService(); &#125;&#125; 配置文件占位符随机数： 12$&#123;random.value&#125;、$&#123;random.int&#125;、$&#123;random.long&#125;$&#123;random.int(10)&#125;、$&#123;random.int[1024,65536]&#125; 占位符获取之前配置过的值，可用:指定默认值 123456789person.last-name=张三$&#123;random.uuid&#125;person.age=$&#123;random.int&#125;person.birth=2017/12/15person.boss=falseperson.maps.k1=v1person.maps.k2=14person.lists=a,b,cperson.dog.name=$&#123;person.hello:hello&#125;_dog // 指定默认值person.dog.age=15 Profile多 Profile 文件：我们在主配置文件编写的时候，文件名可以是 application-{profile}.properties / yml，默认使用 application.properties 的配置。 Yaml 支持多文档块的方式： 12345678910111213141516171819server: port: 8081spring: profiles: active: prod # 指定激活哪个环境--- # 文档块分隔符server: port: 8083spring: profiles: dev # 属于 dev 环境---server: port: 8084spring: profiles: prod # 属于 prod 环境 激活指定的 Profile 在配置文件中指定spring.profiles.active=dev 命令行运行java -jar [项目 JAR 包] --spring.profiles.active=dev; 虚拟机参数-Dspring.profiles.active=dev 配置文件加载位置springboot 启动会扫描以下位置的 application.properties 或者 application.yml 文件作为 Spring boot 的默认配置文件 file:./config/ file:./ classpath:/config/ classpath:/ 优先级由高到底，高优先级的配置会覆盖低优先级的配置；SpringBoot 会从这四个位置加载全部配置文件形成互补配置。 项目打包好后，若要改变配置文件，可以使用命令行参数的形式 java -jar [项目 JAR 包] --spring.config.location=D:/application.properties 或者直接使用 --配置项=值的方式改变某一项配置 java -jar [项目 JAR 包] --server.port=8087 --server.context-path=/abc 优先级：外部 &gt; 内部、带 profile &gt; 不带 profile 启动原理解析","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"}]},{"title":"Netty 常用配置","slug":"后台技术/Netty/Netty 常用配置","date":"2020-02-27T08:35:34.000Z","updated":"2020-07-10T04:03:45.673Z","comments":true,"path":"2020/02/27/后台技术/Netty/Netty 常用配置/","link":"","permalink":"http://yoursite.com/2020/02/27/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"在使用 Netty 的过程中会对 serverBootStrap 的 option 和 childOption 进行自定义的一些配置，具体就是使用ChannelOption 里面声明的常量进行配置，在此介绍一下这些常量。","text":"在使用 Netty 的过程中会对 serverBootStrap 的 option 和 childOption 进行自定义的一些配置，具体就是使用ChannelOption 里面声明的常量进行配置，在此介绍一下这些常量。 ChannelOption 服务端： option() 用于设置 ServerChannel 的选项，负责监听和接收连接； childOption()：用于设置 SocketChannel，负责处理 I/O 操作。 客户端： 没有 childOption()。 ChannelOption.SO_BACKLOG SO_BACKLOG 对应的是 TCP/IP 协议 listen() 函数中的 backlog 参数，函数 listen(int socketfd, int backlog) 用来初始化服务端可连接队列，服务端处理客户端链接是顺序处理的，所以同一时间只能处理一个链接，多客户端请求过来时，服务端会将未处理的请求放入请求队列，backlogb就是制定了队列的大小。 ChannelOption.SO_REUSEADDR SO_REUSEADDR 对应的是 Socket 选项中 SO_REUSEADDR，这个参数表示允许重复使用本地地址和端口，例如，某个服务占用了 TCP 的 8080 端口，其他服务再对这个端口进行监听就会报错，SO_REUSEADDR 这个参数就是用来解决这个问题的，该参数允许服务公用一个端口，这个在服务器程序中比较常用，例如某个进程非正常退出，对一个端口的占用可能不会立即释放，这时候如果不设置这个参数，其他进程就不能立即使用这个端口。 ChannelOption.SO_KEEPALIVE SO_KEEPALIVE 这个参数对应 Socket 中的 SO_KEEPALIVE，当设置这个参数为 true 后，TCP 连接会测试这个连接的状态，如果该连接长时间没有数据交流，TCP 会自动发送一个活动探测数据报文，来检测链接是否存活。 ChannelOption.TCP_NODELAY TCP_NODELAY 对应于 socket 选项中的 TCP_NODELAY，该参数的使用和 Nagle 算法有关，Nagle 算法是将小的数据包组装为更大的帧进行发送，而不会来一个数据包发送一次，目的是为了提高每次发送的效率，因此在数据包没有组成足够大的帧时，就会延迟该数据包的发送，虽然提高了网络负载却造成了延时，TCP_NODELAY 参数设置为 true，就可以禁用 Nagle 算法，即使用小数据包即时传输。与 TCP_NODELAY 对应的就是 TCP_CORK，该选项会等到发送的数据量最大的时候，一次性发送，适合进行文件传输。 ChannelOption.SO_SNDBUF 和 ChannelOption.SO_RCVBUF SO_SNDBUF 和 SO_RCVBUF 对应 Socket 中的 SO_SNDBUF 和 SO_RCVBUF 参数，即设置发送缓冲区和接收缓冲区的大小，发送缓冲区用于保存发送数据，直到发送成功，接收缓冲区用于保存网络协议站内收到的数据，直到程序读取成功。 CodecHttp 服务HttpServerCodec 将二进制报文转为 HTTP 文本报文。 HttpObjectAggregator(512*1024) 合并分批次过来的 HTTP 报文，合并后 Handler 拿到的将是一个完整的 HTTP 报文。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Java 8 新特性","slug":"基础知识/Java 8 新特性","date":"2020-02-18T07:14:54.000Z","updated":"2020-07-10T03:12:41.959Z","comments":true,"path":"2020/02/18/基础知识/Java 8 新特性/","link":"","permalink":"http://yoursite.com/2020/02/18/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Java%208%20%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"LambdJava SE 8 的一个大亮点是引入了 Lambda 表达式，使用它设计的代码会更加简洁。 当开发者在编写 Lambda 表达式时，也会随之被编译成一个函数式接口，Lambda 允许把函数作为一个方法的参数传递进方法中。Lambda 表达式本质是匿名函数（或者叫匿名方法），也可称之为闭包，像方法一样， Lambda 表达式具有带类型的参数、主体和返回类型。可使用 Lambda 语法来代替匿名的内部类，代码不仅简洁而且还可读。 当然，不足之处可能就是可读性稍差及调试稍麻烦一些。 Lambda 语法格式能够用于 Lambda 表达式的只能是 interface，且 interface 中只有一个方法。 Lambda 表达式的标准结构： (parameters) -&gt; {statements;} 123456789101112&#x2F;&#x2F; 标准结构的各种变形&#x2F;&#x2F; 当方法体中只有一条语句时，return 和 &#123;&#125; 都可省略(parameters) -&gt; expression&#x2F;&#x2F; 当仅有一个参数时，() 可省略parameters -&gt; expression&#x2F;&#x2F; 当没有参数时，可直接使用 ()() -&gt; expression&#x2F;&#x2F; 绝大多数情况下，编译器可以从上下文环境中推断出 Lambda 表达式的参数类型，所以参数的类型声明也可以省掉 Lambda 方法引用Lambda 表达式简洁写法：s -&gt; s.toLowerCase()可省略为 String::toLowerCase 方法引用包含的情况 静态方法引用Class::static_method 构造方法引用Class::new或Class&lt;T&gt;::new 类成员方法引用Class::method 对象方法引用instance::method Supplier 接口：不接受任何参数，返回一个结果（只有一个get()方法）。 123456public class SupplierTest &#123; public static void main(String n args)&#123; Supplier&lt;String&gt; supplier = () -&gt; \"hello world\"; System.out.println(supplier.get()); // 控制台打印 hello world &#125;&#125; StreamJava 8 中的Stream是对集合（Collection）对象功能的增强，适用于对集合对象进行各种便利、高效的过滤、映射、排序及聚合等操作，可以独立也可以组合成复杂的操作。 Stream还同时支持串行和并行两种操作模式，并行模式能够充分利用目前主流的多核处理器的优势，使用 Fork / Join 并行方式来拆分和加速任务的处理过程。Stream API 无需编写多线程代码，通过并发模式很容易就支持任务的并行处理。 Stream API 使用一种类似 SQL 语句从数据库查询数据的直观方式来操作 Java 集合数据，可以写出高效率、干净、简洁的代码，极大的提高集合操作的生产力。可以将要处理的集合元素看作一种流，流在管道中传输，并且可以在管道传输的各个节点上进行筛选、排序及聚合等操作。流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作（terminal operation）得到前面处理的结果。 Stream 示例12345678910111213141516171819202122232425262728293031public class StreamDemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; peekList = Stream.of(\"one\", \"tow\", \"three\", \"four\", \"five\", \"six\", \"seven\") .filter(e -&gt; e.length() &gt; 3) // 过滤出字符串长度大于 3 的元素 .peek(e -&gt; System.out.println(\"Filtered value: \" + e)) .map(String::toUpperCase) // 将元素转为大写 .peek(e -&gt; System.out.println(\"Mapped value: \" + e)) .collect(Collectors.toList()); // 将输出 Stream 转换为 List System.out.println(\"集合打印：\"); for (String string : peekList) &#123; System.out.println(string); &#125; &#125;&#125;// 运行结果Filtered value: threeMapped value: THREEFiltered value: fourMapped value: FOURFiltered value: fiveMapped value: FIVEFiltered value: sevenMapped value: SEVEN集合打印：THREEFOURFIVESEVEN Stream 基础特性 基本流程：数据源（存储） → 元素队列（Stream） → 操作（filter、map、reduce、find、match、sorted 等） Pipelining：中间操作都会返回流对象本身。这样多个操作可以串联成一个管道， 如同流式风格（fluent style）。 这样做可以对操作进行优化， 比如延迟执行（laziness）和短路（short-circuiting）； 内部迭代：以前对集合遍历都是通过 Iterator 或者 For-Each 的方式, 显式的在集合外部进行迭代， 这叫做外部迭代。 Java Stream 提供了内部迭代的方式， 通过访问者模式（Visitor）实现。 Stream 串 / 并行","categories":[{"name":"基础知识","slug":"基础知识","permalink":"http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"WebSocket","slug":"后台技术/Netty/WebSocket","date":"2020-02-17T04:12:30.000Z","updated":"2020-07-10T04:04:07.888Z","comments":true,"path":"2020/02/17/后台技术/Netty/WebSocket/","link":"","permalink":"http://yoursite.com/2020/02/17/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/WebSocket/","excerpt":"WebSocket 入门介绍以及 Netty WebSocket 协议开发。","text":"WebSocket 入门介绍以及 Netty WebSocket 协议开发。 HTTP 请求 / 响应模式：客户端加载一个网页，然后直到用户点击下一页之前，什么都不会发生。 Ajax：网络开始变得更加动态了，但所有的 HTTP 通信仍然由客户端控制，这就需要用户进行互动或定期轮询，以便从服务器加载新数据。 长期以来存在着各种技术让服务器得知有新数据可用时，立即将数据发送到客户端这些技术种类繁多。最常用的一种黑客手段是对服务器发起链接创建假象，被称为长轮询。利用长轮询，客户端可以打开指向服务器的 HTTP 连接，而服务器会一直保持连接打开，直到发送响应。服务器只要实际拥有新数据，就会发送响应。 问题：由于 HTTP 协议的开销，导致它们不适用于低延迟应用。 WebSocket：将网络套接字引入到了客户端和服务端来解决这一问题，浏览器和服务器之间可以通过套接字建立持久的连接，双方随时都可以互发数据给对方，而不是之前由客户端控制的一请求一应答模式。 WebSocket 入门在 WebSocket API 中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道，两者就可以直接互相传送数据了。WebSocket 基于 TCP 双向全双工进行消息传递，在同一时刻，既可以发送消息，也可以接收消息，相比于 HTTP 的半双工协议，性能得到很大提升。 WebSocket 连接建立 WebSocket 连接时，需要通过客户端或者浏览器发出握手请求。 握手请求消息如下： 为了建立一个 WebSocket 连接，客户端浏览器首先要向服务器发起一个 HTTP 请求，这个请求和通常的 HTTP 请求不同，包含了一些附加头信息，其中附加头信息 Upgrade: WebSocket表明这是一个申请协议升级的 HTTP 请求。服务器端解析这些附加的头信息，然后生成应答信息返回给客户端，客户端和服务器端的 WebSocket 连接就建立起来了，双方可以通过这个连接通道自由地传递信息，并且这个连接会持续存在直到客户端或者服务器端的某一方主动关闭连接。 握手应答消息如下： 请求消息中的Sec-WebSocket-Key是随机的,服务器端会用这些数据来构造出一个 SHA-1 的信息摘要。使用 SHA-1 加密,然后进行 BASE-64 编码，将结果做为Sec- WebSocket- Accept头的值，返回给客户端。 WebSocket 生命周期握手成功之后,服务端和客户端就可以通过 messages 的方式进行通信了，一个消息由一个或者多个帧组成，WebSocket的消息并不一定对应一个特定网络层的帧，它可以被分割成多个帧或者被合并。 WebSocket 的握手关闭消息带有一个状态码和一个可选的关闭原因，它必须按照协议要求发送一个 Close 控制帧，当对端接收到关闭控制帧指令时，需要主动关闭 WebSocket连接。 Netty WebSocket 协议开发Netty 基于 HTTP 协议栈开发了 WebSocket 协议栈,利用 Netty 的 WebSocket 协议栈可以非常方便地开发出 WebSocket 客户端和服务端。 Netty 服务端实例功能介绍 支持 WebSocket 的浏览器通过 WebSocket 协议发送请求消息给服务端，服务端对请求消息进行判断； 如果是合法的 WebSocket 请求,则获取请求消息体（文本）并在后面追加字符串“欢迎使用 Netty WebSocket服务,现在时刻:[系统时间]”； 客户端 HTML 通过内嵌的 JS 脚本创建 WebSocket 连接，握手成功 / 失败，在文本框中打印“打开 Web Socket服务正常,浏览器支持 Web Socket!” / “抱歉，您的浏览器不支持 Web Socket协议!”。 功能开发12345678&lt;!-- pom.xml --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// WebSocket 服务端启动类public class WebSocketServer&#123; public void run(int port) throws Exception&#123; EventLoopGroup bossGroup = new NioEventLoopGroup (); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b= new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline (); // 将请求和应答消息编码 / 解码为 HTTP 消息 pipeline.addLast(\"http-codec\", new HttpServerCodec()); // 将 HTTP 消息的多个部分组合成一条完整的 HTTP 消息 pipeline.addLast (\"aggregator\", new HttpObjectAggregator(65536)); // 主要用于支持浏览器和服务端进行 WebSocket 通信 ch.pipeline().addLast(\"http-chunked\", new ChunkedWriteHandler()); // 增加 WebSocket 服务端 handler pipeline.addLast(\"handler\", new WebSocketServerHandler()); &#125; &#125;); Channel ch = b.bind(port).sync().channel(); System.out.println(\"Web socket server started at port\" + port +'.'); System.out.println (\"Open our browser and navigate to http://localhost:\" + port + '/'); ch.closeFuture().sync(); &#125; finally &#123; bossGroup. shutdownGracefully(); workerGroup. shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws Exception &#123; int port = 8080; if (args.length &gt;0) &#123; try&#123; port = Integer.parseInt(args[0]); &#125; catch(NumberFormatException e) &#123; e.printStackTrace(); &#125; &#125; new WebSocketServer().run(port); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// Websocket 服务端处理类public class WebSocketServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; private static final Logger logger = Logger.getLogger(WebSocketServerHandler.class.getName()); private WebSocketServerHandshaker handshaker; @Override protected void messageReceived(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 传统的 HTTP 接入 if (msg instanceof FullHttpRequest)&#123; // 判断请求消息有中是否包含 Upgrade: websocket handleHttpRequest(ctx,(FullHttpRequest)msg); &#125;// WebSocket 接入 else if (msg instanceof WebSocketFrame)&#123; handleWebSocketFrame(ctx,(WebSocketFrame)msg); &#125; &#125; private void handleHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) throws Exception&#123; // 如果 HTTP 解码失败，返回 HTTP 400 响应异常 if (!req.getDecoderResult().isSuccess() || (!\"websocket\".equals(req.headers().get(\"Upgrade\"))))&#123; sendHttpResponse(ctx, req,new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.BAD_REQUEST)); return; &#125; // 构造握手工厂，本机测试 WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory(\"ws://localhost:8080/websocket\",null,false); // 创建握手处理类 handshaker = wsFactory.newHandshaker(req); if (handshaker == null)&#123; WebSocketServerHandshakerFactory.sendUnsupportedWebSocketVersionResponse(ctx.channel()); &#125;else &#123; // 构造握手响应消息返回给客户端 // 将 WebSocket 相关的编码和解码类动态添加到 ChannelPipeline 中用于 WebSocket 消息的编解码 handshaker.handshake(ctx.channel(),req); &#125; &#125; // 链路建立成功之后分别对控制帧进行判断 private void handleWebSocketFrame(ChannelHandlerContext ctx, WebSocketFrame frame) &#123; // 判断是否关闭链路指令 if (frame instanceof CloseWebSocketFrame)&#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) frame.retain()); return; &#125; // 判断是否是维持链路的 Ping 消息 if (frame instanceof PingWebSocketFrame)&#123; // 构造 pong 消息返回 ctx.channel().write(new PongWebSocketFrame(frame.content()).retain()); return; &#125; // 本例程仅支持文本信息，故对非文本消息抛出异常 if (!(frame instanceof TextWebSocketFrame))&#123; throw new UnsupportedOperationException(String.format(\"%s frame types not supported\",frame.getClass().getName())); &#125; // 返回应答消息 String request = ((TextWebSocketFrame) frame).text(); if (logger.isLoggable(Level.FINE))&#123; logger.fine(String.format(\"%s received %s\",ctx.channel(),request)); &#125; // 构造新的 TextWebSocketFrame 消息返回给客户端 // 由于握手应答时动态增加了 TextWebSocketframe 的编码类,所以可以直接发送 TextWebSocketFrame对象 ctx.channel().write(new TextWebSocketFrame(request+\"欢迎使用Netty WebSocket服务，现在时刻:\" + new Date().toString())); &#125; private static void sendHttpResponse(ChannelHandlerContext ctx,FullHttpRequest req,FullHttpResponse res)&#123; // 返回应答给客户端 if (res.getStatus().code() != 200)&#123; ByteBuf buf = Unpooled.copiedBuffer(res.getStatus().toString(), CharsetUtil.UTF_8); res.content().writeBytes(buf); // 明确释放ByteBuf的引用计数 buf.release(); setContentLength(res,res.content().readableBytes()); &#125; // 如果是非 Keep-Alive，关闭连接 ChannelFuture f = ctx.channel().writeAndFlush(res); if (!isKeepAlive(req) || res.getStatus().code() != 200)&#123; f.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;WebSoket Demo&lt;/title&gt; &lt;script type=\"text/JavaScript\"&gt; var WebSocket = WebSocket || window.WebSocket || window.MozWebSocket; if (!WebSocket) &#123; alert(\"WebSocket not supported by this browser!\"); &#125; else &#123; var ws = null; function Display() &#123; console.log(\"websocket 测试\"); &#125; var log = function (s) &#123; if (document.readyState !== \"complete\") &#123; log.buffer.push(s); &#125; else &#123; document.getElementById(\"contentId\").value += (s + \"/n\"); &#125; &#125; function CreateConnect () &#123; var msg = document.getElementById(\"wsUrlId\"); console.log(\"CreateConnect(), url: \" + msg.value); if (ws == null) &#123; var wsUrlValue = msg.value; try &#123; ws = new WebSocket(wsUrlValue); ws.onmessage = function (event) &#123; log(\"onmessage(), 接收到服务器消息: \" + event.data); &#125;; ws.onclose = function (event) &#123; log(\"onclose(), Socket 已关闭!\"); ws = null; &#125;; ws.onopen = function (event) &#123; log(\"onopen(), Socket 连接成功!\"); &#125;; ws.onerror = function (event) &#123; &#125;; &#125; catch (e) &#123; ws = null; log(\"连接异常, 重置 websocket\"); &#125; &#125; &#125; function SendMsg() &#123; var msg = document.getElementById(\"messageId\"); console.log(\"SendMsg(), msg: \" + msg.value); if (ws != null) &#123; log(\"发送 Socket 消息: \" + msg.value); ws.send(msg.value); &#125; else &#123; log(\"Socket 还未创建!, msg: \" + msg.value); &#125; &#125; function CloseConnect () &#123; console.log(\"CloseConnect()\"); if (ws != null) &#123; ws.close(); &#125; &#125; &#125; &lt;/script&gt; &lt;/head&gt; &lt;body onload=\"Display()\" &gt; &lt;div id=\"valueLabel\"&gt;&lt;/div&gt; &lt;textarea rows=\"10\" cols=\"40\" id=\"contentId\"&gt;&lt;/textarea&gt; &lt;br/&gt; &lt;input name=\"wsUrl\" id=\"wsUrlId\" value=\"ws://localhost:8080/websocket\"/&gt; &lt;button id=\"createButton\" onClick=\"javascript:CreateConnect()\"&gt;Create&lt;/button&gt; &lt;button id=\"closeButton\" onClick=\"javascript:CloseConnect()\"&gt;Close&lt;/button&gt; &lt;br/&gt; &lt;input name=\"message\" id=\"messageId\" value=\"Hello, Server!\"/&gt; &lt;button id=\"sendButton\" onClick=\"javascript:SendMsg()\"&gt;Send&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; 测试结果控制台打印 浏览器调试","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"},{"name":"WebSocket","slug":"WebSocket","permalink":"http://yoursite.com/tags/WebSocket/"}]},{"title":"2020 读书笔记","slug":"生活杂记/读书笔记","date":"2020-02-16T14:22:10.000Z","updated":"2020-09-05T02:48:43.861Z","comments":true,"path":"2020/02/16/生活杂记/读书笔记/","link":"","permalink":"http://yoursite.com/2020/02/16/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","excerpt":"书籍摘要以及感悟。","text":"书籍摘要以及感悟。 情绪沟通：改变看法与自我认知 底层逻辑 法律 👉 将不规则的生活状态放入到一个规则的容器中 👉 如何处理“溢出”的部分？ 美学 👉 引起共鸣 👉 如何让观者感受到作者所要传达的情绪？ 沟通 👉 说服 👉 没有人喜欢被改变 ❗ 人的观点看法 = 过去的选择 + 过去的偏好 讲道理 / 命令式说教 👉 否定一个的过去 👉 本能抗拒 👎 引起共鸣 👍 给予选择权 👉 选择权语句 / 随时可以反悔 👍 我需要 你为什么要 人的情绪 👉 很大程度决定选择 👉 深挖情绪：这件事对你来说意味着什么？ 👉 根据情绪寻找解决办法 👍 愤怒 👉 强化现有想法 惊讶 👉 动摇现有看法 悲伤 👉 找到重要看法 问题的理解 期待与现实的落差 👉 是否合理 你眼中的问题是别人的解决方案 换角度思考问题 👉 别局限于自身 产品经理四个维度：产品、用户、场景、效率。 产品功能？ 👉 服务特性 ATM 机的显性特性：取钱 ATM 机的隐形特性：分流窗口压力、品牌效应、无利息的现金外存…（运营做大量的市场调查） 产品能否达成战略述求以及能否持续的提供服务 👉 资源的有限性 👉 取舍 用户 👉 分层、用户画像 👉 不同群体有不同的服务需求、用户特性影响决定方式 同程艺龙 VS 美团酒店 场景 👉 时间、空间、情绪 情绪的力量 👉 欲望 👉大甩卖 自我与自制二者不可共存 👉 探寻自我，选择适合自己的一面 十堂极简概率课概率是可以测量的思考：人们如何进行测量？ 在一开始，人们用脚度量长度，这必然存在一定的误差，但若人的数量足够多，那么这个误差将在一定程度上有所减小；后来出现了量尺等测量工具，不同精度的测量工具也存在不同程度的误差。总的来讲，测量是一个不断精进的过程。 概率的测度同样如此。在测度之前，我们先要找到（或者制造）同等可能性的情况，然后计数这些情况发生的次数。于是，事件A的概率，记作P(A)，为 由上述公式可以作出以下推断： 概率永远不会是负值； 如果所有可能发生的情况中均包含事件 A，则 P(A)=1； 如果事件 A 和事件 B 不会同时发生（即相互独立事件），则P(A∪B) = P(A)+P(B)； 在 A 发生条件下 B 的发生概率等于 B 与 A 同时发生的情况数量除以 A 发生的情况数量，即P(A∩B)=P(A)P(B|A)。如果 A 和 B 相互独立，即P(B|A)=P(B)，则P(A∩B)=P(A)P(B)。 此外，某个事件不会发生的概率等于1与该事件发生概率的差：P(!A) = 1–P(A)。！！！这个很重要，有时候正向操作不行就大胆的反向操作一波吧！ 期望值 期望值就是概率的加权平均。期望值可以根据各种结果出现的概率来衡量它们的成本与收益情况，这不仅有助于计算，还是公平性和价值的衡量方法。 骰子问题：一名玩家需要在8次抛掷骰子的赌局中掷出一个6点。此时，投注金额已经确定，这名玩家已经抛掷了3次，但没有一次是6点。如果从赌注中拿出一定比例的钱给这名玩家，让他放弃第4次的抛掷机会（仅放弃这一次），那么给他多少钱才算公平？ 这个问题其实就是一个有关于期望值的问题，也就是说，若玩家不放弃这一次的抛掷，在接下来5此投掷中，每一次的期望收入是多少呢？即多少钱可以很公平的抵消到这一次抛掷的收益。 玩家仍剩余5次机会，假设赌注为S，因为赌注在第4次抛掷骰子时掷出6点的可能性为1/6，所以第4次抛掷的加权平均即为(1/6)*S；剩下的4次投抛掷皆未出现点数6的概率为(5/6)^4=P(A)，所以滴4次抛掷点数不为6并且之后的抛掷出现一次点数为6的概率为(5/6)(1-P(A))=P(B)，即加权平均为P(B)*S。 从结果可以看出，对赌注的加权平均可以得出每种情况下的公平分配，此时给玩家金额(1/6)S就是公平的。 相关性判断就是概率从市场入手，先要假设金钱是价值的量度。有了这个先决条件，我们就可以通过直截了当的方式估算判断概率（judgmental probability），并推断这些具有相关性的判断的数学结构。第二部分则舍弃了这个假设前提，进行了更具一般性的分析。这样一来，概率和效用都是可以测量的伟大思想就完整了。 赌博与判断概率 如果赌局结果为 A 时的收益为1，反之为0，A 的概率就是该赌局的期望值。 如果你为该赌局支付的价格等于 P(A)，这可以被视为一次等价交易。如果价格低于 P(A)，你就会愿意买入；如果价格高于 P(A)，你肯定不愿意买入。因此，根据你的价格平衡点，就可以测算出你对 A 的判断概率。 以上的描述中前者是帕斯卡与费马提出的方法，通过概率来计算期望值；而后者是惠更斯的做法，逆向应用帕斯卡与费马提出的方法，用期望值来计算概率。 也就是说，认真考虑某个人愿意在某个事件上押下的赌注，以此估算该事件的期望值。你的判断概率的加权平均值，就是事件的期望值。 相关性判断 菲尼蒂指出，如果一个人的押注行为是相关的，那么他的判断概率的确具有概率的数学结构。 通过理想化的模型来论证这个观点：假设一个人像赌注经纪人（或者衍生品交易员）那样买卖赌注。如果他的期望值是零，这就是一个公平的赌注；如果他的期望值是正值，这就是一个有利的赌注；如果他的期望值是负值，这就是一个不利的赌注。只要有人找上门来，他便来者不拒，一边买入公平或有利的赌注，一边卖出公平或不利的赌注。在这种情况下，如果若干桩交易以某种方式组合到一起就有可能掉入荷兰赌（Dutch book）的陷阱，也就是说，无论在哪种情况下都会遭受净损失。 荷兰赌：即决策不具有相关性，导致赌局无论如何都不会输。 相关性：人们极易做出根本不相关的草率判断，从根本上来讲，要具备相关性，就必须做到前后一致。相关性表面了判断是具有概率的数学结构。 相关性意味着概率。当且仅当判断概率具有经典概率的数学结构时，这些判断概率才具有相关性。 金融是一种思维方式顺势而为 纵向：处于经济周期的哪个阶段 横向：自己处于人生的那个阶段 套利思维 利用时间 / 空间 / 身份等偏差进行获利 边际 边际效用递减 边际成本上升 效用最大化：边际效用递减 = 边际成本上升 沉没成本 做决策时不要考虑已经付出的成本，应该考虑效用最大化 周期思维 要做风口上的猪 杠杆 借钱 信用 放款 估值 用未来的眼光评价当下的得失 证卷投资都是逆人性的，金融市场有大量的泡沫，要保持理性，要知道自己处于一种什么样的状态。 商业模式 我消灭你，与你无关（降维打击） 羊毛（利润）出在狗（用户）身上，让猪（买单者）付钱","categories":[{"name":"生活杂记","slug":"生活杂记","permalink":"http://yoursite.com/categories/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}]},{"title":"Java 后端开发环境搭建","slug":"环境搭建/Java 后端开发环境搭建","date":"2020-02-15T09:48:23.000Z","updated":"2020-07-10T03:09:35.975Z","comments":true,"path":"2020/02/15/环境搭建/Java 后端开发环境搭建/","link":"","permalink":"http://yoursite.com/2020/02/15/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/Java%20%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","excerpt":"记录开发环境搭建过程以及整合资源以便快速搭建环境。","text":"记录开发环境搭建过程以及整合资源以便快速搭建环境。 JDK 安装 Oracle 官网 下载 JDK 安装包，需要注册账号后才可下载。 云盘资源下载：jdk-8u241-windows-x64.exe 修改安装目录注意事项： ​ 安装在非操作系统所在目录下； ​ 安装的路径中不要有中文及空格。 目录介绍JDK 目录：Java Development Kit 即 Java 开发工具，包含 JRE。 JRE 目录：Java Runtime Environment 即 Java 运行时环境。 JVM：Java Virtual Machine 即 Java 虚拟机 JVM 是整个 Java 实现跨平台的最核心的部分，所有的 Java 程序会首先被编译为 .class 的类文件，这种类文件可以在虚拟机上执行，也就是说 .class 文件并不直接与机器的操作系统相对应，而是经过虚拟机间接与操作系统交互，由虚拟机将程序解释给本地系统执行。 JRE = JVM + lib。光有 JVM 还不能让 .class 文件执行，因为在解释 .class 文件的时候 JVM 需要调用解释所需要的类库 lib。在 JDK 的安装目录里你可以找到 JRE 目录，里面有两个文件夹 bin 和 lib，在这里可以认为 bin 里的就是 jvm，lib 中则是 JVM 工作所需要的类库，所以 JVM 和 lib 和起来就称为 JRE。 JDK 目录 bin：开发工具，包含了开发、执行、调试 Java 程序所使用的工具和实用程序，以及开发工具所需要的类库和支持文件。 include：头文件，它支持使用 Java 本地接口和 Java 虚拟机调试接口的本地代码编程。 jre：运行环境，实现了 Java 运行环境。是运行 Java 程序所必须的环境。JRE 包含了 Java 虚拟机 JavaTM Virtual Machine(JVM)、Java 核心类库和支持文件。如果只运行 Java 程序，则只需要安装 JRE。如需开发 Java 程序，则需安装 JDK。JDK 中包含了 JRE。 lib：包括了 Java 开发环境的 Jar 包，是给 JDK 用的。例如 JDK 下有一些工具，可能要用该目录中的文件。例如，编译器等。 src.zip：构成 Java 核心 API 的所有类的源文件，包括了 java.、javax. 和某些 org.* 包中类的源文件，不包含 com.sun.* 包中类的源文件。 验证安装 配置环境变量新建：JAVA_HOME = [JDK 安装目录]编辑添加：classpath = %JAVA_HOME%/lib/dt.jar;%JAVA_HOME%/lib/tools.jar;.;编辑添加：Path = %JAVA_HOME%/bin; Path 环境变量的值就是一个可执行文件路径的列表，提供给系统寻找和执行应用程序的路径。当执行一个可执行文件时，系统首先在当前路径下寻找，如未找到，则到 Path 中指定的各个路径中去寻找，直到找到为止，如果 Path 路径中也找不到，则报错。Java 的编译器（javac.exe）和解释器（java.exe）都在其安装路径下的 bin 目录中，为了在任何路径下都可以使用它们编译执行 Java 程序，应该将它们所在的目录添加到 Path 变量中;classpath 环境变量指定了 Java 程序编译或运行时所用到的类的搜索列表。Java 虚拟机查找类的过程不同于 Windows 查找可执行命令（.exe、.bat 或 .cmd 以及 .dll 动态链接库）的过程。它不在当前路径下寻找，只到 classpath 指定的路径列表中去寻找，所以在设定环境变量时一定要把当前路径包含进来。变量中的“.”代表当前路径，表示 Java 虚拟机先到当前路径下去查找要使用的类，当前路径指 Java 虚拟机运行时的当前工作目录;3、JAVA_HOME 的值就是 JDK 安装路径的值。当需要使用 JDK 路径时，直接用 %JAVA_HOME% 的方式引用，方便简单。如果 JDK 的安装路径发生了变化或者安装了新版本的 JDK，只需修改 JAVA_HOME 的值即可，其它引用 JAVA_HOME 的地方不需要修改。 测试运行 Java 程序新建 HelloWorld.java 文件。 12345public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println(\"Hello World!\"); &#125;&#125; 编译解释运行。 javac HelloWorld.java的意思是编译 HelloWorld.java 文件，并生成 HelloWorld.class 字节码文件。 java HelloWorld 的意思是运行字节码文件 HelloWorld.class，运行的时候并不需要输入 .class 后缀。 运行成功后输出结果：Hello World! MySQL 安装官网下载：MySQL Installer 8.0.19 云盘资源下载：mysql-installer-community-8.0.19.0.msi 下载 MSI 版本，直接安装，MSI 版本安装的好处是不用手动配置环境变量，和修改配置文件，整个安装过程只需要点击下一步即可。 安装版本分在线安装版本和离线安装版本，这里选择离线安装版本，整个安装过程会比较流畅。而在线安装在网络不佳的情况下可能会特别慢。 运行安装包，选择 Full 模式安装 测试模式下，为了兼容老版本的开发工具以及 jar 包，选择传统授权方法（MySQL 5.x）。 强密码加密方法（MySQL 8.0）采用了新的密码加密方式，也就意味着老版本的数据库访问客户端版本也要更新到支持的版本。 检测是否能连接上 MySQL 服务器。 安装完成。services.msc查看 MySQL 服务是否已经启动，并利用 SQLyog 访问数据库。 Tomcat 安装官网下载：apache-tomcat-8.5.51.exe 运行安装包。 下载完成后，来到安装目录。可以看到 Tomcat 服务已经启动。（可用 startup / showdown.bat 来启动 / 关闭服务） 通过脚本启动服务后发现输出的日志中存在乱码。到安装目录下的 bin 文件夹中编辑 logging.properties 配置文件。 重启 Tomcat 服务器，日志输出正常。成功访问 localhost:8080。 IDEA 安装官网下载：IntelliJ IDEA 云盘资源下载：IntelliJ-2019.1.4.exe IDEA 的安装和配置：点击进入 Maven 的安装和配置：点击进入 IDEA 常用插件介绍（Updating）PS：遇到使用场景后持续更新各插件用法。 Setting → Plugins 下搜索并下载插件 GitToolBox 提供 Git 的一些操作，可以设置时间间隔来比对本地代码和服务器上代码有多少不同，这样可以在提交代码先进行代码的更新。 Findbugs-IDEA 此插件仅支持到 2018.1.8 版本的 IDEA。 PMD 是一个静态代码检测工具，辅助我们检测潜在bug的工具，大大减少了人工审查成本，提高编码效率。 Alibaba Java Coding Guidelines 快捷键Ctrl+Shift+Alt+J，编码规约扫描。 Lombok项目中经常使用 bean，entity 等类，绝大部分数据类类中都需要 get、set、toString、equals 和 hashCode 方法，虽然 eclipse 和 idea 开发环境下都有自动生成的快捷方式，但自动生成这些代码后，如果 bean 中的属性一旦有修改、删除或增加时，需要重新生成或删除 get / set 等方法，给代码维护增加负担。而使用了 lombok 则不一样，使用了 lombok 的注解（@Setter，@Getter，@ToString，@@RequiredArgsConstructor，@EqualsAndHashCode，@Data）之后，就不需要编写或生成 get / set 等方法，很大程度上减少了代码量，而且减少了代码维护的负担。故强烈建议项目中使用 lombok，去掉 bean 中 get、set、toString、equals 和 hashCode 等方法的代码。 pom.xml 中添加 Lombok 依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.10&lt;/version&gt;&lt;/dependency&gt; 示例代码 1234567891011121314151617package com.lombok.demo; import lombok.EqualsAndHashCode;import lombok.Getter;import lombok.Setter;import lombok.ToString;@Setter@Getter@ToString@EqualsAndHashCodepublic class Student &#123; private String name; private int age; private String male;&#125; 测试类 1234567891011121314151617181920212223242526272829package com.lombok.demo; import lombok.extern.java.Log; // 省略了 private static final ogger log = Logger.getLogger(LogExample.class.getName());@Logpublic class LombokTest &#123; public static void main(String[] args) &#123; Student student = new Student(); student.setAge(23); student.setMale(\"man\"); student.setName(\"wingo\"); System.out.println(student.toString()); Student student2 = new Student(); student2.setAge(23); student2.setMale(\"man\"); student2.setName(\"wingo\"); System.out.println(student.equals(student2)); student2.setAge(\"24\"); System.out.println(student.equals(student2)); log.info(\"lombok test\"); &#125;&#125; 测试结果 1234Student(name&#x3D;wingo, age&#x3D;23, male&#x3D;man)truefalselombok test SonarLint 代码分析工具，能够在编码的阶段实时检查代码，并且在代码提交前做检查等，把存在的问题提前暴露，提高代码质量。 .ignore 在项目上右键 → New → .ignore file → .gitignore file(Git)，忽略不需要提交的文件。 Maven Helper 查看 Maven 的冲突与依赖。 Acejump 快速光标跳转方式。 RestfulToolkit 接口测试工具。 CodeGlance 在代码的右侧显示代码小地图。 MyBatis Log Plugin 控制台打印 MyBatis 脚本日志。 String Manipulation 强大的字符串转换工具。 GsonForma Json 实力类快速生成工具。 JUnitGenerator V2.0 帮助生成单元测试工具。 JRebel 热部署插件。完全不用重启服务器，开发后端和前端一样。包括添加类，方法，注解，修改 xml 和 properies 文件。 Translation GenerateAllSetter CodeMaker 代码生成插件。支持增加自定义代码模板（Velocity）；支持选择多个类作为代码模板的上下文。 Grep Console 可以设置不同级别log的字体颜色和背景色。 Key Promoter X 快捷键提示工具。当你在 IDEA 里面使用鼠标的时候，如果这个鼠标操作是能够用快捷键替代的，那么 Key Promoter X 会弹出一个提示框，告知你这个鼠标操作可以用什么快捷键替代。 Swip（Spring Web Initializr） IdeaVim 在 Intellij 中模拟 Vim 的操作方式。 BashSupport IEDA 中的 BashSupport 插件支持在IDEA中编写shell脚本文件，有友好的代码格式，支持自动补全，检查错误，并且配置完之后，还可以在IEDA中直接运行shell脚本 EduTools 使用 EduTools，您可以通过代码练习任务来学习和教导 Kotlin","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Welab","slug":"Welab","permalink":"http://yoursite.com/tags/Welab/"}]},{"title":"Git 命令清单","slug":"开发杂项/Git 命令清单","date":"2020-02-15T07:17:45.000Z","updated":"2020-07-10T03:00:21.734Z","comments":true,"path":"2020/02/15/开发杂项/Git 命令清单/","link":"","permalink":"http://yoursite.com/2020/02/15/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/Git%20%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/","excerpt":"Git 对于程序员可以说使一项必备的技能，故列此清单方便查询。","text":"Git 对于程序员可以说使一项必备的技能，故列此清单方便查询。 转载自：阮一峰的网络日志 → 常用 Git 命令清单 Git 操作流程 Workspace：工作区； Index / Stage：暂存区； Repository：本地仓库； Remote：远程仓库。 新建代码库12345678# 在当前目录新建一个 Git 代码库git init# 新建一个目录，将其初始化为 Git 代码库git init [project-name]# 下载一个项目和它的整个代码历史git clone [url] 配置Git 的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 123456789101112# 显示当前的 Git 配置git config --list# 编辑 Git 配置文件git config -e [--global]# 查看 Git 配置文件git config -l# 设置提交代码时的用户信息git config [--global] user.name \"[name]\"git config [--global] user.email \"[email address]\" 文件操作123456789101112131415161718192021# 添加指定文件到暂存区git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录git add [dir]# 添加当前目录的所有文件到暂存区git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交git add -p# 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区git rm --cached [file]# 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed] 代码提交123456789101112131415161718# 提交暂存区到仓库区git commit -m [message]# 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message]# 提交工作区自上次 commit 之后的变化，直接到仓库区git commit -a# 提交时显示所有 diff 信息git commit -v# 使用一次新的 commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次 commit 的提交信息git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化git commit --amend [file1] [file2] ... 分支分支介绍分支是为了将修改记录的整体流程分叉保存。分叉后的分支不受其他分支的影响，所以在同一个数据库里可以同时进行多个修改。 分叉的分支是可以合并的。 master 分支：在数据库进行最初的提交后, Git 会创建一个名为 master 的分支。因此之后的提交，在切换分支之前都会添加到 master 分支里。 分支的运用Merge 分支：Merge 分支是为了可以随时发布 release 而创建的分支，它还能作为 Topic 分支的源分支使用。通常，大家会将 master 分支当作 Merge 分支使用。 Topic 分支：Topic 分支是为了开发新功能或修复 Bug 等任务而建立的分支。若要同时进行多个的任务，请创建多个的 Topic 分支。Topic 分支是从稳定的 Merge 分支创建的。完成作业后，要把 Topic 分支合并回 Merge 分支。 分支操作实例新建仓库12345mkdir testcd testgit initgit add myfile.txtgit commit -m \"first commit\" 新建分支12345# 创建分支git branch test01# 显示分支列表# 带*的就是现在的分支git branch 切换分支12345# 切换分支git checkout test01Switched to branch 'issue1'git add myfile.txtgit commit -m \"添加add的说明\" 合并分支123git checkout masterSwitched to branch 'master'git merge test01 master 分支指向的提交移动到和 test01 同样的位置。这个是 fast-forward（快进）合并。 常用命令123456789101112131415161718192021222324252627282930313233343536373839404142434445# 列出所有本地分支git branch# 列出所有远程分支git branch -r# 列出所有本地分支和远程分支git branch -a# 新建一个分支，但依然停留在当前分支git branch [branch-name]# 新建一个分支，并切换到该分支git checkout -b [branch]# 新建一个分支，指向指定commitgit branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区git checkout [branch-name]# 切换到上一个分支git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支git merge [branch]# 选择一个commit，合并进当前分支git cherry-pick [commit]# 删除分支git branch -d [branch-name]# 提交远程分支git push origin [branch-name]# 删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 标签1234567891011121314151617181920212223242526# 列出所有taggit tag# 新建一个tag在当前commitgit tag [tag]# 新建一个tag在指定commitgit tag [tag] [commit]# 删除本地taggit tag -d [tag]# 删除远程taggit push origin :refs/tags/[tagName]# 查看tag信息git show [tag]# 提交指定taggit push [remote] [tag]# 提交所有taggit push [remote] --tags# 新建一个分支，指向某个taggit checkout -b [branch] [tag] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件git status# 显示当前分支的版本历史git log# 显示commit历史，以及每次commit发生变更的文件git log --stat# 搜索提交历史，根据关键词git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其\"提交说明\"必须符合搜索条件git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名git log --follow [file]git whatchanged [file]# 显示指定文件相关的每一次diffgit log -p [file]# 显示过去5次提交git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序git shortlog -sn# 显示指定文件是什么人在什么时间修改过git blame [file]# 显示暂存区和工作区的差异git diff# 显示暂存区和上一个commit的差异git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异git diff HEAD# 显示两次提交之间的差异git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码git diff --shortstat \"@&#123;0 day ago&#125;\"# 显示某次提交的元数据和内容变化git show [commit]# 显示某次提交发生变化的文件git show --name-only [commit]# 显示某次提交时，某个文件的内容git show [commit]:[filename]# 显示当前分支的最近几次提交git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动git fetch [remote]# 显示所有远程仓库git remote -v# 显示某个远程仓库的信息git remote show [remote]# 增加一个新的远程仓库，并命名git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch]# 上传本地指定分支到远程仓库git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force# 推送所有分支到远程仓库git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区git checkout [commit] [file]# 恢复暂存区的所有文件到工作区git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file]# 重置暂存区与工作区，与上一次commit保持一致git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支git revert [commit]# 暂时将未提交的变化移除，稍后再移入git stashgit stash pop 实际问题解决123456789## 分支取得更新到 master 文件git checkout mastergit pull mastergit checkout wingogit merge master## 远程修改分支文件后的本地更新git reset --hard # 强制覆盖本地版本git pull origin wingo # 同步远程的分支","categories":[{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"各种乱码问题","slug":"开发杂项/各种乱码问题","date":"2020-02-01T11:38:54.000Z","updated":"2020-07-10T04:14:42.971Z","comments":true,"path":"2020/02/01/开发杂项/各种乱码问题/","link":"","permalink":"http://yoursite.com/2020/02/01/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/%E5%90%84%E7%A7%8D%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98/","excerpt":"在项目开发中，因为各种语言的编码不同常常会遇到各种乱码的问题，令人糟心，故写一篇博客专门用于记录博主在开发过程遇到的各种乱码问题。","text":"在项目开发中，因为各种语言的编码不同常常会遇到各种乱码的问题，令人糟心，故写一篇博客专门用于记录博主在开发过程遇到的各种乱码问题。 Tomcat 控制台乱码到 Tomcat /conf /logging.properties 中修改此项java.util.logging.ConsoleHandler.encoding为 GBK。","categories":[{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"乱码","slug":"乱码","permalink":"http://yoursite.com/tags/%E4%B9%B1%E7%A0%81/"}]},{"title":"Chatroom 简介","slug":"项目开发/聊天室项目","date":"2020-01-31T04:16:27.000Z","updated":"2020-07-10T03:56:15.344Z","comments":true,"path":"2020/01/31/项目开发/聊天室项目/","link":"","permalink":"http://yoursite.com/2020/01/31/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/%E8%81%8A%E5%A4%A9%E5%AE%A4%E9%A1%B9%E7%9B%AE/","excerpt":"一个网页版的线上聊天室","text":"一个网页版的线上聊天室 项目简介项目基本思路此项目以 Tomcat 作为核心服务器用于处理客户登录、个人信息管理等 HTTP 类型的请求，端口号：8080。 在 Tomcat 服务器中另开一个线程启动 Netty WebSocket 服务器用于处理用户消息通信的 WebSocket 类型的请求，端口号：3333。 用户登录后： Tomcat 服务器会返回用户的个人信息，同时更新记录在线用户，根据用户 id 建立一条 WebSocket 连接并保存在后端以便进行实时通信。 浏览器将维持一个 Session 对象来保持登录状态（30 min）。 用户与用户进行通信： 服务器根据收到的消息内容中的对话方的用户 id 找到保存的 WebSocket 连接，通过该连接发送消息。 用户退出： 释放用户的 WebSocket 连接。 清空用户在 Session 对象中的登陆状态。 系统功能模块登录模块：用户登录、用户注销。 聊天管理模块：单条消息发送、群组消息发送。（消息：文本、文件） 服务器pom.xml 123456789101112131415161718192021222324&lt;!-- netty4 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.2.Final&lt;/version&gt;&lt;/dependency&gt;&lt;build&gt; &lt;finalName&gt;WebSocket&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;optimize&gt;true&lt;/optimize&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 自定义 WebSocket 服务器服务器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class WebSocketServer implements Runnable &#123; private final Logger logger = LoggerFactory.getLogger(WebSocketServer.class); @Autowired private EventLoopGroup bossGroup; @Autowired private EventLoopGroup workerGroup; @Autowired private ServerBootstrap serverBootstrap; // port 和 childChannelHandler 用 Xml 的方式配置，需要 getter/setter 方法 private int port; private ChannelHandler childChannelHandler; private ChannelFuture serverChannelFuture; public WebSocketServer() &#123; &#125; @Override public void run() &#123; build(); &#125; // 启动Netty Websocket服务器 public void build() &#123; try &#123; long begin = System.currentTimeMillis(); serverBootstrap.group(bossGroup, workerGroup) // boss 负责客户端的 TCP 连接请求 worker 负责与客户端之前的读写操作 .channel(NioServerSocketChannel.class) // 配置客户端的 channel 类型 .option(ChannelOption.SO_BACKLOG, 1024) // 设置请求队列的长度 .option(ChannelOption.TCP_NODELAY, true) // 降低延迟 .childOption(ChannelOption.SO_KEEPALIVE, true) // 两小时无活动检测心跳 .childOption(ChannelOption.RCVBUF_ALLOCATOR, new FixedRecvByteBufAllocator(592048)) // 配置固定长度接收缓存区分配器 .childHandler(childChannelHandler); // 绑定 I/O 事件的处理类，WebSocketChildChannelHandler 中定义 long end = System.currentTimeMillis(); logger.info(\"Netty Websocket服务器启动完成，耗时 \" + (end - begin) + \" ms，已绑定端口 \" + port + \" 阻塞式等候客户端连接\"); serverChannelFuture = serverBootstrap.bind(port).sync(); &#125; catch (Exception e) &#123; logger.info(e.getMessage()); bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); e.printStackTrace(); &#125; &#125; // 关闭 WebSocket 服务器 public void close()&#123; serverChannelFuture.channel().close(); Future&lt;?&gt; bossGroupFuture = bossGroup.shutdownGracefully(); Future&lt;?&gt; workerGroupFuture = workerGroup.shutdownGracefully(); try &#123; bossGroupFuture.await(); workerGroupFuture.await(); &#125; catch (InterruptedException ignore) &#123; ignore.printStackTrace(); &#125; &#125; public ChannelHandler getChildChannelHandler() &#123; return childChannelHandler; &#125; public void setChildChannelHandler(ChannelHandler childChannelHandler) &#123; this.childChannelHandler = childChannelHandler; &#125; public int getPort() &#123; return port; &#125; public void setPort(int port) &#123; this.port = port; &#125;&#125; 数据处理类 1234567891011121314151617181920@Componentpublic class WebSocketChildChannelHandler extends ChannelInitializer&lt;SocketChannel&gt;&#123; @Resource(name = \"webSocketServerHandler\") private ChannelHandler webSocketServerHandler; @Resource(name = \"httpRequestHandler\") private ChannelHandler httpRequestHandler; // 初始化通道，装载上所需要的处理器 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(\"http-codec\", new HttpServerCodec()); // HTTP编码解码器 ch.pipeline().addLast(\"aggregator\", new HttpObjectAggregator(65536)); // 合并分批次过来的 HTTP 报文，合并后 Handler拿到的将是一个完整的 HTTP 报文 ch.pipeline().addLast(\"http-chunked\", new ChunkedWriteHandler()); // 方便大文件传输，不过实质上都是短的文本数据 ch.pipeline().addLast(\"http-handler\", httpRequestHandler); // 绑定自定义的 Http 请求处理器 ch.pipeline().addLast(\"websocket-handler\",webSocketServerHandler); // 绑定自定义的 WebSocket 请求处理器 &#125;&#125; 自定义 Http 请求处理类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Component@Sharablepublic class HttpRequestHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof FullHttpRequest) &#123; // 处理 Http 请求 handleHttpRequest(ctx, (FullHttpRequest) msg); &#125; else if (msg instanceof WebSocketFrame) &#123; // retain() 增加 ByteBuf 的引用计数，fireChannelRead 通知需要继续调用链表后面的 channelRead ctx.fireChannelRead(((WebSocketFrame) msg).retain()); &#125; &#125; // 处理 Http 请求，主要是完成 HTTP 协议到 Websocket 协议的升级 private void handleHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) &#123; if (!req.decoderResult().isSuccess()) &#123; sendHttpResponse(ctx, req, new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.BAD_REQUEST)); return; &#125; // 构造握手工厂 WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory( \"ws:/\" + ctx.channel() + \"/websocket\", null, false); // 创建握手处理类 WebSocketServerHandshaker handshaker = wsFactory.newHandshaker(req); // 将此连接保存到一个全局 Map 中，用户退出登陆时要移除此处理类 Constant.webSocketHandshakerMap.put(ctx.channel().id().asLongText(), handshaker); if (handshaker == null) &#123; WebSocketServerHandshakerFactory.sendUnsupportedVersionResponse(ctx.channel()); &#125; else &#123; // 构造握手响应消息返回给客户端 // 将 WebSocket 相关的编码和解码类动态添加到 ChannelPipeline 中用于 WebSocket 消息的编解码 handshaker.handshake(ctx.channel(), req); &#125; &#125; private void sendHttpResponse(ChannelHandlerContext ctx, FullHttpRequest req, DefaultFullHttpResponse res) &#123; // 返回应答给客户端 if (res.status().code() != 200) &#123; ByteBuf buf = Unpooled.copiedBuffer(res.status().toString(), CharsetUtil.UTF_8); res.content().writeBytes(buf); buf.release(); &#125; // 如果是非Keep-Alive，关闭连接 boolean keepAlive = HttpUtil.isKeepAlive(req); ChannelFuture f = ctx.channel().writeAndFlush(res); if (!keepAlive) &#123; f.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 自定义 WebSocket 请求处理类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697@Component@Sharablepublic class WebSocketServerHandler extends SimpleChannelInboundHandler&lt;WebSocketFrame&gt; &#123; private static final Logger LOGGER = LoggerFactory.getLogger(WebSocketServerHandler.class); @Autowired private ChatService chatService; // 对 WebSocket 的请求消息进行处理 @Override protected void channelRead0(ChannelHandlerContext ctx, WebSocketFrame msg) throws Exception &#123; handlerWebSocketFrame(ctx, msg); &#125; private void handlerWebSocketFrame(ChannelHandlerContext ctx, WebSocketFrame frame) throws Exception &#123; // 关闭请求 if (frame instanceof CloseWebSocketFrame) &#123; WebSocketServerHandshaker handshaker = Constant.webSocketHandshakerMap.get(ctx.channel().id().asLongText()); if (handshaker == null) &#123; sendErrorMessage(ctx, \"不存在的客户端连接！\"); &#125; else &#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) frame.retain()); &#125; return; &#125; // ping请求 if (frame instanceof PingWebSocketFrame) &#123; ctx.channel().write(new PongWebSocketFrame(frame.content().retain())); return; &#125; // 只支持文本格式，不支持二进制消息 if (!(frame instanceof TextWebSocketFrame)) &#123; sendErrorMessage(ctx, \"仅支持文本(Text)格式，不支持二进制消息\"); &#125; // 客服端发送过来的消息 String request = ((TextWebSocketFrame)frame).text(); LOGGER.info(\"服务端收到新信息：\" + request); JSONObject param = null; try &#123; param = JSONObject.parseObject(request); &#125; catch (Exception e) &#123; sendErrorMessage(ctx, \"JSON字符串转换出错！\"); e.printStackTrace(); &#125; if (param == null) &#123; sendErrorMessage(ctx, \"参数为空！\"); return; &#125; String type = (String) param.get(\"type\"); switch (type) &#123; case \"REGISTER\": chatService.register(param, ctx); break; case \"SINGLE_SENDING\": chatService.singleSend(param, ctx); break; case \"GROUP_SENDING\": chatService.groupSend(param, ctx); break; case \"FILE_MSG_SINGLE_SENDING\": chatService.FileMsgSingleSend(param, ctx); break; case \"FILE_MSG_GROUP_SENDING\": chatService.FileMsgGroupSend(param, ctx); break; default: chatService.typeError(ctx); break; &#125; &#125; // 客户端断开连接 @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; chatService.remove(ctx); &#125; // 出现异常关闭通道 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; private void sendErrorMessage(ChannelHandlerContext ctx, String errorMsg) &#123; String responseJson = new ResponseJson() .error(errorMsg) .toString(); ctx.channel().writeAndFlush(new TextWebSocketFrame(responseJson)); &#125;&#125; 整合 WebSocket 服务器先来看看项目的服务器构成。 Tomcat 是一个免费的开放源代码的 Servlet 容器 回顾一下 Servlet 的生命周期。 要把 Netty WebSocket 服务器整合到 Tomcat 服务器中，那么就需要在 Servlet 初始化之前告诉其所需要进行的操作。写一个@PostConstruct注解方法开启一个独立线程用于启动 Netty WebSocket 服务器；并且写一个@PreDestory注解方法在 Servlet 完全关闭之前关闭 WebSocket 服务器。 1234567891011121314151617181920212223242526@Component@Scope(\"singleton\")public class AppContext &#123; private final Logger logger = LoggerFactory.getLogger(AppContext.class); @Autowired private WebSocketServer webSocketServer; private Thread nettyThread; @PostConstruct public void init() &#123; nettyThread = new Thread(webSocketServer); logger.info(\"开启独立线程，启动 Netty WebSocket 服务器...\"); nettyThread.start(); &#125; @PreDestroy public void close() &#123; logger.info(\"正在释放Netty Websocket相关连接...\"); webSocketServer.close(); logger.info(\"正在关闭Netty Websocket服务器线程...\"); nettyThread.stop(); logger.info(\"系统成功关闭！\"); &#125;&#125; 在执行这个方法之前，Tomcat 已经加载完配置文件，这里的 WebSocket 服务器是通过@AutoWired装载进来的，也就是说 WebSocket 需要在 Spring 容器中注册成为一个 Bean。 ApplicationContext-netty.xml 1234567891011121314&lt;!-- 扫描关于 Netty Websocket 的包 --&gt;&lt;context:component-scan base-package=\"com.wingo.web.websocket\"/&gt;&lt;!-- 把 Netty 的一些类服务器注册到 Spring，方便处理和扩展 --&gt;&lt;!-- 用于处理客户端连接请求 --&gt;&lt;bean id=\"bossGroup\" class=\"io.netty.channel.nio.NioEventLoopGroup\"/&gt;&lt;!-- 用于处理客户端 I/O 操作 --&gt;&lt;bean id=\"workerGroup\" class=\"io.netty.channel.nio.NioEventLoopGroup\"/&gt;&lt;!-- 服务器启动引导类 --&gt;&lt;bean id=\"serverBootstrap\" class=\"io.netty.bootstrap.ServerBootstrap\" scope=\"prototype\"/&gt;&lt;!-- 自定义的Netty Websocket服务器 --&gt;&lt;bean id=\"webSocketServer\" class=\"com.wingo.web.websocket.WebSocketServer\"&gt; &lt;property name=\"port\" value=\"3333\"/&gt; &lt;property name=\"childChannelHandler\" ref=\"webSocketChildChannelHandler\" /&gt;&lt;/bean&gt; Mybatis项目目录 Model： po/User.java 用户实体 Dao UserDao.java 用户数据访问对象 Service UserService.java 用户业务逻辑 UserServiceImpl.java Xml UserMapper.xml 持久层 / 对象关系映射文件 ApplicationContext-dao.xml ApplicationContext-service.xml MybatisConfig.xml pom.xml Properties jdbc.properties 数据库配置文件 项目搭建pom.xml 添加 Mybatis 相关依赖包 12345678910111213141516171819202122232425&lt;!-- MySQL --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- c3p0 连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt; 数据库的建立 123456CREATE TABLE `user` ( `user_id` int(11) NOT NULL AUTO_INCREMENT, `user_name` varchar(64) DEFAULT NULL, `password` varchar(64) DEFAULT NULL, PRIMARY KEY (`user_id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 用户实体类 123456789101112131415161718192021222324252627282930313233343536373839public class User &#123; private Long userId; private String userName; private String password; public Long getUserId() &#123; return userId; &#125; public void setUserId(Long userId) &#123; this.userId = userId; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; @Override public String toString() &#123; return \"User&#123;\" + \"userId='\" + userId + '/'' + \", username='\" + userName + '/'' + \", password='\" + password + '/'' + '&#125;'; &#125;&#125; 数据访问类 1234567public interface UserDao &#123; // 保存用户信息 void saveUser(User user); // 通过 id 取得用户信息 User queryById(long user);&#125; 对象 / 持久层映射文件 1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"wingo.dao.UserDao\"&gt; &lt;insert id=\"saveUser\" parameterType=\"wingo.model.po.User\" useGeneratedKeys=\"true\" keyProperty=\"userId\"&gt; INSERT INTO user (user_name,password) VALUES (#&#123;userName&#125;,#&#123;password&#125;) &lt;/insert&gt; &lt;select id=\"queryById\" parameterType=\"long\" resultType=\"wingo.model.po.User\" &gt; &lt;!-- 具体的sql --&gt; SELECT user_id,user_name,password FROM user WHERE user_id = #&#123;userId&#125; &lt;/select&gt;&lt;/mapper&gt; 业务逻辑类 12345public interface UserService &#123; void saveUser(User user); User queryById(long userId);&#125; 业务逻辑类的实现 123456789101112131415public class UserServiceImpl implements UserService &#123; private Logger logger= LoggerFactory.getLogger(this.getClass()); @Autowired private UserDao userDao; public void saveUser(User user) &#123; userDao.saveUser(user); &#125; public User queryById(long userId) &#123; return userDao.queryById(userId); &#125;&#125; 数据库信息配置 jdbc.properties 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/nettychatroom?useUnicode=true&amp;characterEncoding=utf8jdbc.username=rootjdbc.password= 123456 Mybatis 配置 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!-- 配置全局属性 --&gt; &lt;settings&gt; &lt;!-- 使用jdbc的getGeneratedKeys获取数据库自增主键值 --&gt; &lt;setting name=\"useGeneratedKeys\" value=\"true\" /&gt; &lt;!-- 使用列别名替换列名 默认:true --&gt; &lt;setting name=\"useColumnLabel\" value=\"true\" /&gt; &lt;!-- 开启驼峰命名转换:Table&#123;create_time&#125; -&gt; Entity&#123;createTime&#125; --&gt; &lt;setting name=\"mapUnderscoreToCamelCase\" value=\"true\" /&gt; &lt;!-- 开启控制台打印 SQL 语句 --&gt; &lt;setting name=\"logImpl\" value=\"STDOUT_LOGGING\" /&gt; &lt;/settings&gt;&lt;/configuration&gt; 数据访问层上下文配置 ApplicationContext-dao.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 配置整合mybatis过程 --&gt; &lt;!-- 1.配置数据库相关参数properties的属性：$&#123;url&#125; --&gt; &lt;context:property-placeholder location=\"classpath:jdbc.properties\" /&gt; &lt;!-- 2.数据库连接池 --&gt; &lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;!-- 配置连接池属性 --&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.driver&#125;\" /&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt; &lt;!-- c3p0连接池的私有属性 --&gt; &lt;property name=\"maxPoolSize\" value=\"30\" /&gt; &lt;property name=\"minPoolSize\" value=\"10\" /&gt; &lt;!-- 关闭连接后不自动commit --&gt; &lt;property name=\"autoCommitOnClose\" value=\"false\" /&gt; &lt;!-- 获取连接超时时间 --&gt; &lt;property name=\"checkoutTimeout\" value=\"10000\" /&gt; &lt;!-- 当获取连接失败重试次数 --&gt; &lt;property name=\"acquireRetryAttempts\" value=\"2\" /&gt; &lt;/bean&gt; &lt;!-- 3.配置SqlSessionFactory对象 --&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;!-- 注入数据库连接池 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!-- 配置MyBaties全局配置文件:mybatis-config.xml --&gt; &lt;property name=\"configLocation\" value=\"classpath:MybatisConfig.xml\" /&gt; &lt;!-- 扫描entity包，使用别名 --&gt; &lt;property name=\"typeAliasesPackage\" value=\"wingo.dao\" /&gt; &lt;!-- 扫描sql配置文件:mapper需要的xml文件 --&gt; &lt;property name=\"mapperLocations\" value=\"classpath:mapper/*.xml\" /&gt; &lt;/bean&gt; &lt;!-- 4.配置扫描Dao接口包，动态实现Dao接口，注入到spring容器中 --&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;!-- 注入sqlSessionFactory --&gt; &lt;property name=\"sqlSessionFactoryBeanName\" value=\"sqlSessionFactory\" /&gt; &lt;!-- 给出需要扫描Dao接口包 --&gt; &lt;property name=\"basePackage\" value=\"wingo.dao\" /&gt; &lt;/bean&gt;&lt;/beans&gt; 业务逻辑层上下文配置 12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&gt; &lt;!-- 扫描service包下所有使用注解的类型 --&gt; &lt;context:component-scan base-package=\"wingo.service\"/&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 注入数据库连接池 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;/bean&gt; &lt;!-- 配置基于注解的声明式事务 --&gt; &lt;tx:annotation-driven/&gt;&lt;/beans&gt; Junit 4 测试Junit 4 整合引入依赖 1234567&lt;!-- Junit --&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt;&lt;/dependency&gt; 测试基类 123456789@RunWith(SpringJUnit4ClassRunner.class) //使用junit4进行测试@ContextConfiguration(locations=&#123; \"classpath:spring/ApplicationContext-dao.xml\", \"classpath:spring/ApplicationContext-service.xml\"&#125;) //加载配置文件public class BaseJunit4Test &#123;&#125; 测试类 12345678910111213141516171819202122public class UserTest extends BaseJunit4Test&#123; @Autowired private UserService userService; @Test @Transactional //标明此方法需使用事务 @Rollback(false) //标明使用完此方法后事务不回滚,true时为回滚 public void test()&#123; System.out.println(\"测试Spring整合Junit4进行单元测试\"); // User user = new User(); // user.setUserName(\"test1\"); // user.setPassword(\"111\"); // userService.saveUser(user); User user = userService.queryById(1); System.out.println(user.toString()); &#125;&#125; 测试结果 Spring MVC 项目结构 View： longin.html 用户登录页面 chatroom.html 聊天室页面 Controller UserController.java LoginController.java Xml ApplicationContext-mvc.xml web.xml pom.xml 项目搭建pom.xml 添加 Spring 全家桶依赖。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; web.xml 中将请求交给 SpringMVC 处理 12345678910111213141516&lt;!-- 前端控制器 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/ApplicationContext-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 对 SpringMVC 进行配置 12345678910111213141516171819202122232425262728293031323334&lt;!-- MVC 负责扫描 Controller --&gt;&lt;context:component-scan base-package=\"wingo.web.controller\"/&gt;&lt;!-- 启动 Spring MVC 的注解功能，完成请求和注解POJO的映射,解决 @ResponseBody 乱码问题 --&gt;&lt;!-- 需要在 annotation-driven 之前,否则乱码问题同样无法解决 --&gt;&lt;bean class=\"org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter\"&gt; &lt;property name=\"messageConverters\"&gt; &lt;list&gt; &lt;bean class=\"org.springframework.http.converter.StringHttpMessageConverter\"&gt; &lt;property name=\"supportedMediaTypes\"&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 开启 MVC 的注解式配置 --&gt;&lt;mvc:annotation-driven/&gt;&lt;!-- 视图解释类 --&gt;&lt;bean name=\"viewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/views/\" /&gt; &lt;property name=\"suffix\" value=\".html\" /&gt; &lt;property name=\"order\" value=\"0\" /&gt;&lt;/bean&gt;&lt;!-- 设置不拦截静态文件 --&gt;&lt;mvc:resources location=\"/WEB-INF/views/\" mapping=\"/WEB-INF/views/**\" /&gt;&lt;mvc:resources location=\"/static/\" mapping=\"/static/**/**\" /&gt;&lt;mvc:resources location=\"/UploadFile/\" mapping=\"/UploadFile/**\" /&gt; 编写控制器方法 12345678@Controllerpublic class UserController &#123; // 跳转到登陆页面 @RequestMapping(value = &#123;\"login\", \"/\"&#125;, method = RequestMethod.GET) public String toLogin() &#123; return \"login\"; &#125;&#125; 1234567public class ChatroomController &#123; // 登录成功后的跳转页面 @RequestMapping(method = RequestMethod.GET) public String toChatroom() &#123; return \"chatroom\"; &#125;&#125; 项目测试控制台输出 @RequestMapping(value = {&quot;login&quot;, &quot;/&quot;}, method = RequestMethod.GET) 整合 SwaggerSwagger 是一款 RESTFUL 接口的文档在线自动生成 + 功能测试功能软件。 @Api ：修饰整个类，描述Controller的作用；@ApiOperation ：描述一个类的一个方法，或者说一个接口；@ApiParam ：单个参数描述；用在方法参数上@ApiModel ：用对象来接收参数；@ApiProperty ：用对象接收参数时，描述对象的一个字段；@ApiResponse ：HTTP响应其中1个描述；@ApiResponses ：HTTP响应整体描述；@ApiIgnore ：使用该注解忽略这个API；@ApiError ：发生错误返回的信息；@ApiImplicitParam ：一个请求参数：用在方法上 。@ApiImplicitParams ：多个请求参数：用在方法上。 引入依赖包 123456789101112131415161718192021222324252627282930313233&lt;!-- Swagger 可帮助开发人员设计，构建，记录和使用 RESTful Web 服务 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Jackson 包，配合 Spring @ResponseBody 注解，以及 Json 工具包 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt;&lt;/dependency&gt; 配置 Swagger 1234567891011121314151617181920212223@EnableSwagger2 //使 Swagger2 生效@ComponentScan(basePackages = &#123;\"wingo.web.controller\"&#125;) //需要扫描的controller包路径@Configurable //配置注解，自动在本类上下文加载一些环境变量信息public class SwaggerConfig extends WebMvcConfigurationSupport &#123; //RestApiConfig @Bean public Docket buildDocket()&#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(buildApiInf()) .select() .apis(RequestHandlerSelectors.basePackage(\"wingo.web.controller\"))//controller路径 .paths(PathSelectors.any()) .build(); &#125; private ApiInfo buildApiInf()&#123; return new ApiInfoBuilder() .title(\"Wingo's Swagger\") .description(\"Web Restful 测试\") .build(); &#125;&#125; Spring 加载 SwaggerConfig 123456&lt;!-- 加载 SwaggerConfig --&gt;&lt;bean class=\"springfox.documentation.swagger2.configuration.Swagger2DocumentationConfiguration\" id=\"swagger2Config\"/&gt;&lt;!-- 不拦截所需静态文件 --&gt;&lt;mvc:resources mapping=\"swagger-ui.html\" location=\"classpath:/META-INF/resources/\"/&gt;&lt;mvc:resources mapping=\"/webjars/**\" location=\"classpath:/META-INF/resources/webjars/\"/&gt; 在需要标注的控制器类方法上添加注解 123456789101112131415161718192021@Controller@RequestMapping(value=\"/swagger\")public class TestSwaggerController &#123; @ApiOperation(value = \"测试专用\", notes = \"返回一个 JSON 字符串\") @RequestMapping(value=\"/test\",method= RequestMethod.GET) @ResponseBody public String test()&#123; ObjectMapper mapper = new ObjectMapper(); User user = new User(); user.setUserId((long) 1); user.setUserName(\"中文测试\"); user.setPassword(\"111\"); String str = null; try &#123; str = mapper.writeValueAsString(user); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; return str; &#125;&#125; 访问：[项目根目录]/swagger-ui.html 点击 Try it out 进行请求的发送。 成功请求并的到响应。 功能模块数据库结构 基于 Spring 注解式开发 前后端分离，返回 Json 的视图对象类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class ResponseJson extends HashMap&lt;String, Object&gt; &#123; private static final long serialVersionUID = 1L; private static final Integer SUCCESS_STATUS = 200; private static final Integer ERROR_STATUS = -1; private static final String SUCCESS_MSG = \"一切正常\"; public ResponseJson() &#123; super(); &#125; public ResponseJson(int code) &#123; super(); setStatus(code); &#125; public ResponseJson(HttpStatus status) &#123; super(); setStatus(status.value()); setMsg(status.getReasonPhrase()); &#125; public ResponseJson success() &#123; put(\"msg\", SUCCESS_MSG); put(\"status\", SUCCESS_STATUS); return this; &#125; public ResponseJson success(String msg) &#123; put(\"msg\", msg); put(\"status\", SUCCESS_STATUS); return this; &#125; public ResponseJson error(String msg) &#123; put(\"msg\", msg); put(\"status\", ERROR_STATUS); return this; &#125; public ResponseJson setData(String key, Object obj) &#123; @SuppressWarnings(\"unchecked\") HashMap&lt;String, Object&gt; data = (HashMap&lt;String, Object&gt;) get(\"data\"); if (data == null) &#123; data = new HashMap&lt;String, Object&gt;(); put(\"data\", data); &#125; data.put(key, obj); return this; &#125; public ResponseJson setStatus(int status) &#123; put(\"status\", status); return this; &#125; public ResponseJson setMsg(String msg) &#123; put(\"msg\", msg); return this; &#125; public ResponseJson setValue(String key, Object val) &#123; put(key, val); return this; &#125; /** * 返回JSON字符串 */ @Override public String toString() &#123; return JSONObject.toJSONString(this); &#125;&#125; 用来存放 Netty WebSocket 连接信息的常量类 123456789101112public class Constant &#123; // 一个 userId 对应一个 Session public static final String USER_TOKEN = \"userId\"; // 用于保存 Http 升级 WebSocket 后的握手实例 public static Map&lt;String, WebSocketServerHandshaker&gt; webSocketHandshakerMap = new ConcurrentHashMap&lt;String, WebSocketServerHandshaker&gt;(); // 用于保存已登录的用户的通道信息 public static Map&lt;String, ChannelHandlerContext&gt; onlineUserMap = new ConcurrentHashMap&lt;String, ChannelHandlerContext&gt;();&#125; 用户登录 User 👉 实体类，与数据库表 user 向对应 123456789public class User &#123; private Long userId; private String username; private String password; private String avatarUrl; // 省略了 Getter Setter toString 方法&#125; UserDao、UserMapper👉 通过用户输入的用户名到数据库中找到此用户的用户信息。 1User getByUsername(String username); 123456789&lt;select id=\"getByUsername\" resultType=\"com.wingo.model.po.User\" &gt; &lt;!-- 具体的sql --&gt; SELECT user_id, username, password FROM user WHERE username = #&#123;username&#125;&lt;/select&gt; SecurityService、SecurityServiceImpl 👉 实现登录的业务逻辑 123456789101112public ResponseJson login(String username, String password, HttpSession session) &#123; User user = userDao.getByUsername(username); if (user == null) &#123; return new ResponseJson().error(\"不存在该用户名\"); &#125; if (!user.getPassword().equals(password)) &#123; return new ResponseJson().error(\"密码不正确\"); &#125; // 将已登录的用户的 userId 保存在 Session 中 session.setAttribute(Constant.USER_TOKEN, user.getUserId()); return new ResponseJson().success();&#125; SecurityController 👉 URL映射以及前后端数据交互 12345@RequestMapping(value = \"login\", method = RequestMethod.POST)@ResponseBodypublic ResponseJson login(HttpSession session, @RequestParam String username, @RequestParam String password) &#123; return securityService.login(username, password, session);&#125; 用户点击登录按钮后，发送 Ajax 请求进行用户验证，验证通过后跳转到 chatroom 页面。 123456789101112131415161718192021&lt;script type=\"text/javascript\"&gt; function login() &#123; $.ajax(&#123; type : 'POST', url : 'login', dataType: 'json', data : &#123; username: $(\"#username\").val(), password: $(\"#password\").val() &#125;, async : false, success: function(data) &#123; if (data.status == 200) &#123; window.location.href=\"chatroom\"; &#125; else &#123; alert(data.msg); &#125; &#125; &#125;);&#125;&lt;/script&gt; 初始化用户信息 用户拦截认证UserAuthInteceptor 👉 检测用户是否登录并设置资源访问权限 12345678910111213141516171819202122232425262728public class UserAuthInteceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HttpSession session = request.getSession(); Object userToken = session.getAttribute(Constant.USER_TOKEN); if (userToken == null) &#123; response.sendRedirect(\"login\"); return false; &#125; return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; // 允许所有资源都可以访问资源 response.setHeader(\"Access-Control-Allow-Origin\", \"*\"); // 表示是否可以将对请求的响应暴露给页面 response.setHeader(\"Access-Control-Allow-Credentials\",\"true\"); &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; &#125;&#125; 配置 Spring MVC 对 chatoom 页面的访问请求进行拦截认证 1234567&lt;!-- 用户认证拦截器 --&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=\"/chatroom/**\" /&gt; &lt;bean class=\"com.wingo.web.interceptor.UserAuthInteceptor\" /&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; PO &amp; VO在返回给用户的信息中，除了用户的个人信息，还需要用户的好友列表信息以及群聊列表信息。创建一个 VO 类用于保存前端所需要的所有信息。 UserInfo 类 123456789101112public class UserInfo &#123; private Long userId; private String username; private String password; private String avatarUrl; private List&lt;User&gt; friendList; // 好友列表信息 private List&lt;Chatgroup&gt; groupList; // 群聊信息 // 构造器方法便于实例化 // ...&#125; Chatgroup 类 123456789// group 为 MySQL 的关键字，不可使用public class Chatgroup &#123; private Long groupId; private String groupName; private String groupAvatarUrl; // ...&#125; GroupInfo 类 12345678910public class GroupInfo &#123; private Long groupId; private String groupName; private String groupAvatarUrl; private List&lt;User&gt; members; // 构造器方法便于实例化 // ...&#125; ChatroomController 👉 通过 userId 获取用户的信息 1234567@RequestMapping(value = \"/get_userinfo\", method = RequestMethod.POST) @ResponseBodypublic ResponseJson getUserInfo(HttpSession session) &#123; Object userId = session.getAttribute(Constant.USER_TOKEN); System.out.println(\"Session 中的 userId = \" + userId); return userService.getByUserId(String.valueOf(userId));&#125; UserServiceImpl 123456789101112131415161718192021222324252627282930313233343536373839@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserDao userDao; @Autowired private UserChatRelationDao userChatRelationDao; @Autowired private GroupDao groupDao; @Override public ResponseJson getByUserId(String userId) &#123; User user = userDao.getByUserId(userId); UserInfo userInfo = new UserInfo( user.getUserId(), user.getUsername(), user.getPassword(), user.getAvatarUrl() ); // friendList 构造 List&lt;String&gt; friendsId = userChatRelationDao.getUserFriendsIdByUserId(userId); List&lt;User&gt; friends = new ArrayList&lt;User&gt;(); for(String friendId : friendsId)&#123; User friend = userDao.getByUserId(friendId); friends.add(friend); &#125; // groupList 构造 List&lt;String&gt; groupsId = userChatRelationDao.getGroupsIdByUserId(userId); List&lt;Chatgroup&gt; groups = new ArrayList&lt;Chatgroup&gt;(); for(String groupId : groupsId)&#123; Chatgroup group = groupDao.getByGroupId(groupId); groups.add(group); &#125; userInfo.setGroupList(groups); userInfo.setFriendList(friends); return new ResponseJson().success() .setData(\"userInfo\", userInfo); &#125;&#125; 通过 Swagger 模拟请求取得返回的 Json 数据。 123456789101112131415161718192021222324252627282930313233343536373839&#123; \"msg\": \"一切正常\", \"data\": &#123; \"userInfo\": &#123; \"userId\": 1, \"username\": \"test1\", \"password\": null, \"avatarUrl\": \"static/img/avatar/Member001.jpg\", \"friendList\": [ &#123; \"userId\": 2, \"username\": \"test2\", \"password\": null, \"avatarUrl\": \"static/img/avatar/Member001.jpg\" &#125;, &#123; \"userId\": 4, \"username\": \"test3\", \"password\": null, \"avatarUrl\": \"static/img/avatar/Member001.jpg\" &#125;, &#123; \"userId\": 5, \"username\": \"test4\", \"password\": null, \"avatarUrl\": \"static/img/avatar/Member001.jpg\" &#125; ], \"groupList\": [ &#123; \"groupId\": 1000, \"groupName\": \"Group1\", \"groupAvatarUrl\": \"static/img/avatar/Group01.jpg\" &#125; ] &#125; &#125;, \"status\": 200&#125; 用户信息初始化完成。 页面布局粗略版 用户单聊前端页面在取得后台所返回的 Json 信息之后，根据信息初始化用户的聊天页面。 此程序不将聊天信息保存到数据库中，而是保存在前端的聊天页面用 Javascript 所自定义的一个数据结构里，一个SentMessageMap(key, new Array())。用户进行用户信息的初始化时要将此用户信息中的 groupId 以及 userId 作为键初始化这个聊天信息的 Map，以便后面进行聊天信息的保存。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263function setUserInfo() &#123; $.ajax(&#123; type : 'POST', url : 'chatroom/get_userinfo', dataType: 'json', async : true, success: function(data) &#123; console.log(\"获取用户信息...\"); if (data.status == 200) &#123; // 自定义的数据结构，用于保存聊天信息进行回显 sentMessageMap = new SentMessageMap(); // 获取用户信息 var userInfo = data.data.userInfo; userId = userInfo.userId; $(\"#username\").html(userInfo.username); $(\"#avatarUrl\").attr(\"src\", userInfo.avatarUrl); var groupListHTML = \"\"; // 获取用户的群组信息 var groupList = userInfo.groupList; // 初始化群聊框列表 for (var i = 0; i &lt; groupList.length; i++) &#123; // 添加 Key sentMessageMap.put(groupList[i].groupId, new Array()); groupListHTML += '&lt;li&gt;' + '&lt;div class=\"liLeft\"&gt;&lt;img src=\"' + groupList[i].groupAvatarUrl + '\"&gt;&lt;/div&gt;' + '&lt;div class=\"liRight\"&gt;' + '&lt;span class=\"hidden-groupId\"&gt;' + groupList[i].groupId + '&lt;/span&gt;' + '&lt;span class=\"intername\"&gt;' + groupList[i].groupName + '&lt;/span&gt;' + '&lt;span class=\"infor\"&gt;&lt;/span&gt;' + '&lt;/div&gt;' + '&lt;/li&gt;'; &#125; // 添加群聊框列表 $('.conLeft ul').append(groupListHTML); var friendListHTML = \"\"; // 获取用户好友信息 var friendList = userInfo.friendList; // 初始化好友框列表 for (var i = 0; i &lt; friendList.length; i++) &#123; // 添加 Key sentMessageMap.put(friendList[i].userId, new Array()); friendListHTML += '&lt;li&gt;' + '&lt;div class=\"liLeft\"&gt;&lt;img src=\"' + friendList[i].avatarUrl + '\"&gt;&lt;/div&gt;' + '&lt;div class=\"liRight\"&gt;' + '&lt;span class=\"hidden-userId\"&gt;' + friendList[i].userId + '&lt;/span&gt;' + '&lt;span class=\"intername\"&gt;' + friendList[i].username + '&lt;/span&gt;' + '&lt;span class=\"infor\"&gt;&lt;/span&gt;' + '&lt;/div&gt;' + '&lt;/li&gt;'; &#125; // 添加好友列表 $('.conLeft ul').append(friendListHTML); // 绑定好友框点击事件：显示右侧消息框 $('.conLeft ul li').on('click', friendLiClickEvent); &#125; else &#123; alert(data.msg); &#125; &#125; &#125;);&#125; 单聊流程 监听绑定1234567891011121314151617181920212223242526272829303132333435363738if(!window.WebSocket)&#123; window.WebSocket = window.MozWebSocket; &#125; if(window.WebSocket)&#123; socket = new WebSocket(\"ws://localhost:3333\"); socket.onmessage = function(event)&#123; var json = JSON.parse(event.data); if (json.status == 200) &#123; var type = json.data.type; console.log(\"收到一条新信息，类型为：\" + type); switch(type) &#123; case \"REGISTER\": ws.registerReceive(); break; case \"SINGLE_SENDING\": ws.singleReceive(json.data); break; default: console.log(\"不正确的类型！\"); &#125; &#125; else &#123; alert(json.msg); console.log(json.msg); &#125; &#125;; // 连接成功1秒后，将用户信息注册到服务器在线用户表 socket.onopen = setTimeout(function(event)&#123; console.log(\"WebSocket已成功连接！\"); ws.register(); &#125;, 1000) socket.onclose = function(event)&#123; console.log(\"WebSocket已关闭...\"); &#125;; &#125; else &#123; alert(\"您的浏览器不支持WebSocket！\"); &#125; WebSocket 处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162var ws = &#123; register: function() &#123; if (!window.WebSocket) &#123; return; &#125; if (socket.readyState == WebSocket.OPEN) &#123; var data = &#123; \"userId\" : userId, \"type\" : \"REGISTER\" &#125;; socket.send(JSON.stringify(data)); &#125; else &#123; alert(\"Websocket连接没有开启！\"); &#125; &#125;, singleSend: function(fromUserId, toUserId, content) &#123; if (!window.WebSocket) &#123; return; &#125; if (socket.readyState == WebSocket.OPEN) &#123; var data = &#123; \"fromUserId\" : fromUserId, \"toUserId\" : toUserId, \"content\" : content, \"type\" : \"SINGLE_SENDING\" &#125;; // 将信息发送给客户端的 WebSocketServerhandler 进行处理 socket.send(JSON.stringify(data)); &#125; else &#123; alert(\"Websocket连接没有开启！\"); &#125; &#125;, registerReceive: function() &#123; console.log(\"userId为 \" + userId + \" 的用户登记到在线用户表成功！\"); &#125;, singleReceive: function(data) &#123; // 获取、构造参数 console.log(data); var fromUserId = data.fromUserId; var content = data.content; var fromAvatarUrl; var $receiveLi; // 为设置消息提醒以及获取头像取得元素 $('.conLeft').find('span.hidden-userId').each(function()&#123; if (this.innerHTML == fromUserId) &#123; fromAvatarUrl = $(this).parent(\".liRight\") .siblings(\".liLeft\").children('img').attr(\"src\"); $receiveLi = $(this).parent(\".liRight\").parent(\"li\"); &#125; &#125;) var answer=''; answer += '&lt;li&gt;' + '&lt;div class=\"answers\"&gt;'+ content +'&lt;/div&gt;' + '&lt;div class=\"answerHead\"&gt;&lt;img src=\"' + fromAvatarUrl + '\"/&gt;&lt;/div&gt;' + '&lt;/li&gt;'; // 消息框处理 放入暂存区，若用户正处于与新发消息用户的聊天窗口则回显（利用暂存区计算高度） processMsgBox.receiveSingleMsg(answer, fromUserId); // 好友列表处理 1.设置红色提醒标志 2.设置部分新消息提醒 3.置顶新消息 processFriendList.receiving(content, $receiveLi); &#125;,&#125; 发送按钮1234567891011121314151617181920212223242526272829303132333435$('.sendBtn').on('click',function()&#123; var fromUserId = userId; var toUserId = $('#toUserId').val(); var toGroupId = $('#toGroupId').val(); var news = $('#dope').val(); if (toUserId == '' &amp;&amp; toGroupId == '') &#123; alert(\"请选择对话方\"); return; &#125; if(news == '')&#123; alert('消息不能为空'); return; &#125; else &#123; if (toUserId.length != 0) &#123; ws.singleSend(fromUserId, toUserId, news); &#125; else &#123; // 群发 &#125; $('#dope').val(''); var avatarUrl = $('#avatarUrl').attr(\"src\"); var msg = ''; msg += '&lt;li&gt;'+ '&lt;div class=\"news\"&gt;' + news + '&lt;/div&gt;' + '&lt;div class=\"nesHead\"&gt;&lt;img src=\"' + avatarUrl + '\"/&gt;&lt;/div&gt;' + '&lt;/li&gt;'; // 消息框处理： processMsgBox.sendMsg(msg, toUserId, toGroupId); // 好友列表处理： var $sendLi = $('.conLeft').find('li.bg'); processFriendList.sending(news, $sendLi); &#125;&#125;) 消息框处理123456789101112131415161718192021222324252627282930sendMsg: function(msg, toUserId, toGroupId) &#123; // 把内容添加到消息框 $('.newsList').append(msg); // 2. 手动计算、调整回显消息的宽度 var $newsDiv = $('.newsList li').last().children(\"div\").first(); var fixWidth = 300; // 自定义的消息框本身的最长宽度 var maxWidth = 493; // 消息框所在行 div 的满宽度（不包含头像框的宽度部分） var minMarginLeftWidth = 224; // 按理说应该是 maxwidth - fixWidth，这里出现了点问题 var marginLeftWidth; // 要计算消息框的margin-left宽度 if ($newsDiv.actual('width') &lt; fixWidth) &#123; marginLeftWidth = maxWidth - $newsDiv.actual('width'); $newsDiv.css(\"margin-left\", marginLeftWidth + \"px\"); &#125; else &#123; $newsDiv.css(\"width\", fixWidth + \"px\") .css(\"margin-left\", minMarginLeftWidth + \"px\"); &#125; // 3. 把 调整后的消息html标签字符串 添加到已发送用户消息表 if (toUserId.length != 0) &#123; // 得到数组添加新消息 以 outerHTML 的形式 // last() 得到匹配元素的最后一个 sentMessageMap.get(toUserId).push($('.newsList li').last().prop(\"outerHTML\")); &#125; else &#123; sentMessageMap.get(toGroupId).push($('.newsList li').last().prop(\"outerHTML\")); &#125; // 4. 滚动条往底部移 $('.RightCont').scrollTop($('.RightCont')[0].scrollHeight );&#125; 好友列表处理12345678910111213141516sending: function(content, $sendLi) &#123; // 设置部分新消息提醒 if (content.length &gt; 8) &#123; content = content.substring(0, 8) + \"...\"; &#125; $('.conLeft').find('li.bg').children('.liRight').children('.infor').text(content); // 如果存在新消息提醒徽章，则去除徽章 if ($sendLi.find('.layui-badge').length &gt; 0) &#123; $sendLi.find('.layui-badge').remove(); &#125; //$('.conLeft ul').prepend('&lt;li class=\"bg\"&gt;' + $sendLi.html() + '&lt;/li&gt;'); // 好友框新消息置顶 $('.conLeft ul').prepend($sendLi.prop(\"outerHTML\")); $sendLi.remove(); $('.conLeft ul li').first().on('click', friendLiClickEvent)&#125; 服务器处理12345678910111213141516171819202122@Overridepublic void singleSend(JSONObject param, ChannelHandlerContext ctx) &#123; String fromUserId = String.valueOf(param.get(\"fromUserId\")); String toUserId = (String)param.get(\"toUserId\"); String content = (String)param.get(\"content\"); // 取得接收者的通道 ChannelHandlerContext toUserCtx = Constant.onlineUserMap.get(toUserId); if (toUserCtx == null) &#123; String responseJson = new ResponseJson() .error(MessageFormat.format(\"userId为 &#123;0&#125; 的用户没有登录！\", toUserId)) .toString(); sendMessage(ctx, responseJson); &#125; else &#123; String responseJson = new ResponseJson().success() .setData(\"fromUserId\", fromUserId) .setData(\"content\", content) .setData(\"type\", ChatType.SINGLE_SENDING) .toString(); // 发送消息给接收者的通道，浏览器 socket 监听处理 sendMessage(toUserCtx, responseJson); &#125;&#125; 测试 通过控制台调试可以取得 sentMessageMap 中保存了与 userId=2 的用户的聊天信息数组。 通过控制台调试可以取得 sentMessageMap 中保存了与 userId=1 的用户的聊天信息数组。 群组聊天群组聊天的流程与用户单聊流程基本相似，只是在给接收者的通道发送消息时需要通过群组信息取得群组里的成员的信息，并获取成员的所有通道进行消息的遍发送。 1234567891011121314151617181920212223242526272829303132333435363738@Overridepublic void groupSend(JSONObject param, ChannelHandlerContext ctx) &#123; String fromUserId = String.valueOf(param.get(\"fromUserId\")); String toGroupId = (String)param.get(\"toGroupId\"); String content = (String)param.get(\"content\"); Chatgroup chatgroup = groupDao.getByGroupId(toGroupId); if (chatgroup == null) &#123; String responseJson = new ResponseJson().error(\"该群id不存在\").toString(); sendMessage(ctx, responseJson); &#125; else &#123; List&lt;String&gt; groupUsersId = userChatRelationDao.getUsersIdByGroupId(toGroupId); List&lt;User&gt; groupUser = new ArrayList&lt;User&gt;(); for(String groupUserId : groupUsersId)&#123; groupUser.add(userDao.getByUserId(groupUserId)); &#125; GroupInfo groupInfo = new GroupInfo( chatgroup.getGroupId(), chatgroup.getGroupName(), chatgroup.getGroupAvatarUrl(), groupUser ); String responseJson = new ResponseJson().success() .setData(\"fromUserId\", fromUserId) .setData(\"content\", content) .setData(\"toGroupId\", toGroupId) .setData(\"type\", ChatType.GROUP_SENDING) .toString(); groupInfo.getMembers() .forEach(member -&gt; &#123; ChannelHandlerContext toCtx = Constant.onlineUserMap.get(String.valueOf(member.getUserId())); if (toCtx != null &amp;&amp; !String.valueOf(member.getUserId()).equals(fromUserId)) &#123; sendMessage(toCtx, responseJson); &#125; &#125;); &#125;&#125; 接收消息者则根据 groupId 来显示接收到的消息。 1234567891011121314151617181920212223242526272829303132groupReceive: function(data) &#123; // 获取、构造参数 console.log(data); var fromUserId = data.fromUserId; var content = data.content; var toGroupId = data.toGroupId; var fromAvatarUrl; var $receiveLi; $('.conLeft').find('span.hidden-userId').each(function()&#123; if (this.innerHTML == fromUserId) &#123; fromAvatarUrl = $(this).parent(\".liRight\") .siblings(\".liLeft\").children('img').attr(\"src\"); /* $receiveLi = $(this).parent(\".liRight\").parent(\"li\"); */ &#125; &#125;) $('.conLeft').find('span.hidden-groupId').each(function()&#123; // 群组聊天框 if (this.innerHTML == toGroupId) &#123; $receiveLi = $(this).parent(\".liRight\").parent(\"li\"); &#125; &#125;) var answer=''; answer += '&lt;li&gt;' + '&lt;div class=\"answers\"&gt;'+ content +'&lt;/div&gt;' + '&lt;div class=\"answerHead\"&gt;&lt;img src=\"' + fromAvatarUrl + '\"/&gt;&lt;/div&gt;' + '&lt;/li&gt;'; // 消息框处理 processMsgBox.receiveGroupMsg(answer, toGroupId); // 好友列表处理 processFriendList.receiving(content, $receiveLi);&#125;, 发送表情表情的发送的底层原理和发送文字是一样的，只是发送表情的话，发送内容是一串静态资源的 URI。 表情模块绑定的监听事件： 1234567$('.ExP').on('mouseenter',function()&#123; $('.emjon').show();&#125;)$('.emjon').on('mouseleave',function()&#123; $('.emjon').hide();&#125;) 发送文件文件的上传使用的是 commons-fileupload 工具类。 123456&lt;!-- 文件上传 --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.3&lt;/version&gt;&lt;/dependency&gt; 文件上传的模态框表格： 123456789101112&lt;div class=\"modal-body\"&gt; &lt;form class=\"form-horizontal\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"col-sm-3 control-label\"&gt;选择文件&lt;/label&gt; &lt;div class=\"col-sm-9\"&gt; &lt;input type=\"file\" name=\"file\" class=\"col-sm-9 myfile\" /&gt; &lt;p class=\"help-block\"&gt;注意：文件大小不超过30M，有效期为7天&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt;&lt;/div&gt; Bootstrap 的文件上传控件 bootstrap-fileinput 的使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778$(\".myfile\").fileinput(&#123; uploadUrl:\"chatroom/upload\", uploadAsync : true, // 默认异步上传 showUpload : true, // 是否显示上传按钮 showRemove : false, // 显示移除按钮 showCaption : false, // 是否显示标题 showPreview : true, // 是否显示预览，默认为 true dropZoneTitle: \"请通过拖拽图片文件放到这里\", dropZoneEnabled : false, // 是否显示拖拽区域，默认不写为 true，但是会占用很大区域 maxFileSize: 30720, // 单位为 kb，如果为 0 表示不限制文件大小 maxFileCount : 1, // 表示允许同时上传的最大文件个数 enctype : 'multipart/form-data', validateInitialCount : true, previewFileIcon : \"&lt;i class='glyphicon glyphicon-file'&gt;&lt;/i&gt;\", msgFilesTooMany : \"选择上传的文件数量(&#123;n&#125;) 超过允许的最大数值&#123;m&#125;！\", language : 'zh'&#125;)// 异步上传返回文件上传错误结果处理$('.myfile').on('fileerror', function(event, data, msg) &#123; console.log(\"fileerror\"); console.log(data);&#125;);// 异步上传返回结果处理$(\".myfile\").on(\"fileuploaded\", function(event, data, previewId, index) &#123; // 1. 上传成功1.5秒后自动关闭上传模态框 console.log(\"fileuploaded\"); setTimeout(function() &#123; $('#upload-cancel').trigger('click'); $('.fileinput-remove').trigger('click'); &#125;, 1500); // 2. 获取、设置参数 var returnData = data.response.data; var originalFilename = returnData.originalFilename; var fileSize = returnData.fileSize; var fileUrl = returnData.fileUrl; var content = \"[文件]\"; var fromUserId = userId; var avatarUrl = $('#avatarUrl').attr(\"src\"); var $sendLi = $('.conLeft').find('li.bg'); var toUserId = $('#toUserId').val(); var toGroupId = $('#toGroupId').val(); // 拼接发送者对话框的消息 var fileHtml = '&lt;li&gt;'+ '&lt;div class=\"send-file-shown\"&gt;' + '&lt;div class=\"media\"&gt;' + '&lt;a href=\"' + fileUrl + '\" class=\"media-left\"&gt;' + '&lt;i class=\"glyphicon glyphicon-file\" style=\"font-size:28pt;\"&gt;&lt;/i&gt;' + '&lt;/a&gt;' + '&lt;div class=\"media-body\"&gt; ' + '&lt;h5 class=\"media-heading\"&gt;' + originalFilename + '&lt;/h5&gt;' + '&lt;span&gt;'+ fileSize + '&lt;/span&gt;' + '&lt;/div&gt;' + '&lt;/div&gt;'+ '&lt;/div&gt;' + '&lt;div class=\"nesHead\"&gt;&lt;img src=\"' + avatarUrl + '\"/&gt;&lt;/div&gt;' + '&lt;/li&gt;'; // 3. 发送信息到服务器 if (toUserId.length != 0) &#123; ws.fileMsgSingleSend(fromUserId, toUserId, originalFilename, fileUrl, fileSize); &#125; else &#123; ws.fileMsgGroupSend(fromUserId, toGroupId, originalFilename, fileUrl, fileSize); &#125; // 4. 消息框处理： processMsgBox.sendFileMsg(fileHtml, toUserId, toGroupId); // 5. 好友列表处理 processFriendList.sending(content, $sendLi);&#125;);//上传前$('.myfile').on('filepreupload', function(event, data, previewId, index) &#123; console.log(\"filepreupload\");&#125;); 文件上传的控制器： 123456@RequestMapping(value = \"/upload\", method = POST)@ResponseBody public ResponseJson upload( @RequestParam(value = \"file\", required = true) file, HttpServletRequest request) &#123; return fileUploadService.upload(file, request);&#125; 文件上传的业务逻辑处理： 123456789101112131415161718192021222324252627282930313233343536373839public class FileUploadServiceImpl implements FileUploadService &#123; // 项目的打包目录，本项目打包在根目录下 private final static String SERVER_URL_PREFIX = \"http://localhost:8080/\"; // 保存上传文件的文件夹名称 private final static String FILE_STORE_PATH = \"UploadFile\"; @Override public ResponseJson upload(MultipartFile file, HttpServletRequest request) &#123; // 重命名文件，防止重名 String filename = getRandomUUID(); String suffix = \"\"; String originalFilename = file.getOriginalFilename(); String fileSize = FileUtils.getFormatSize(file.getSize()); // 截取文件的后缀名 if (originalFilename.contains(\".\")) &#123; suffix = originalFilename.substring(originalFilename.lastIndexOf(\".\")); &#125; filename = filename + suffix; // getRealPath(“/”) 获取实际路径,“/”指代项目根目录,所以代码返回的是项目在容器中的实际发布运行的根路径 String prefix = request.getSession().getServletContext().getRealPath(\"/\") + FILE_STORE_PATH; System.out.println(\"存储路径为:\" + prefix + \"\\\\\" + filename); Path filePath = Paths.get(prefix, filename); try &#123; Files.copy(file.getInputStream(), filePath); &#125; catch (IOException e) &#123; e.printStackTrace(); return new ResponseJson().error(\"文件上传发生错误！\"); &#125; return new ResponseJson().success() .setData(\"originalFilename\", originalFilename) .setData(\"fileSize\", fileSize) .setData(\"fileUrl\", SERVER_URL_PREFIX + FILE_STORE_PATH + \"\\\\\" + filename); &#125; private String getRandomUUID() &#123; return UUID.randomUUID().toString().replace(\"-\", \"\"); &#125;&#125; WebSoclet 处理文件发送方法： 123456789101112131415161718192021222324public void FileMsgSingleSend(JSONObject param, ChannelHandlerContext ctx) &#123; String fromUserId = String.valueOf(param.get(\"fromUserId\")); String toUserId = (String)param.get(\"toUserId\"); String originalFilename = (String)param.get(\"originalFilename\"); String fileSize = (String)param.get(\"fileSize\"); String fileUrl = (String)param.get(\"fileUrl\"); ChannelHandlerContext toUserCtx = Constant.onlineUserMap.get(toUserId); if (toUserCtx == null) &#123; String responseJson = new ResponseJson() .error(MessageFormat.format(\"userId为 &#123;0&#125; 的用户没有登录！\", toUserId)) .toString(); sendMessage(ctx, responseJson); &#125; else &#123; String responseJson = new ResponseJson().success() .setData(\"fromUserId\", fromUserId) .setData(\"originalFilename\", originalFilename) .setData(\"fileSize\", fileSize) .setData(\"fileUrl\", fileUrl) .setData(\"type\", ChatType.FILE_MSG_SINGLE_SENDING) .toString(); sendMessage(toUserCtx, responseJson); &#125;&#125;// 群发文件则与群组聊天类似，取群组中的用户的通道遍历发送文件信息 接收者对话框接收文件消息的拼接： 1234567891011121314151617181920212223242526272829303132333435363738fileMsgSingleRecieve: function(data) &#123; // 获取、构造参数 console.log(data); var fromUserId = data.fromUserId; var originalFilename = data.originalFilename; var fileSize = data.fileSize; var fileUrl = data.fileUrl; var content = \"[文件]\"; var fromAvatarUrl; var $receiveLi; $('.conLeft').find('span.hidden-userId').each(function()&#123; if (this.innerHTML == fromUserId) &#123; fromAvatarUrl = $(this).parent(\".liRight\") .siblings(\".liLeft\").children('img').attr(\"src\"); $receiveLi = $(this).parent(\".liRight\").parent(\"li\"); &#125; &#125;) var fileHtml = '&lt;li&gt;'+ '&lt;div class=\"receive-file-shown\"&gt;' + '&lt;div class=\"media\"&gt;' + '&lt;div class=\"media-body\"&gt; ' + '&lt;h5 class=\"media-heading\"&gt;' + originalFilename + '&lt;/h5&gt;' + '&lt;span&gt;'+ fileSize + '&lt;/span&gt;' + '&lt;/div&gt;' + '&lt;a href=\"' + fileUrl + '\" class=\"media-right\"&gt;' + '&lt;i class=\"glyphicon glyphicon-file\" style=\"font-size:28pt;\"&gt;&lt;/i&gt;' + '&lt;/a&gt;' + '&lt;/div&gt;'+ '&lt;/div&gt;' + '&lt;div class=\"answerHead\"&gt;&lt;img src=\"' + fromAvatarUrl + '\"/&gt;&lt;/div&gt;' + '&lt;/li&gt;'; // 消息框处理 processMsgBox.receiveSingleMsg(fileHtml, fromUserId); // 好友列表处理 processFriendList.receiving(content, $receiveLi);&#125;,","categories":[{"name":"项目开发","slug":"项目开发","permalink":"http://yoursite.com/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"聊天室","slug":"聊天室","permalink":"http://yoursite.com/tags/%E8%81%8A%E5%A4%A9%E5%AE%A4/"}]},{"title":"Maven 安装和配置","slug":"环境搭建/Maven 的安装和配置","date":"2020-01-30T06:36:59.000Z","updated":"2020-07-10T03:07:09.078Z","comments":true,"path":"2020/01/30/环境搭建/Maven 的安装和配置/","link":"","permalink":"http://yoursite.com/2020/01/30/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/Maven%20%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/","excerpt":"Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。","text":"Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。 Maven 的安装Maven 下载地址：Maven 下载 3..6.x 版本会有莫名其妙的报错，建议使用 3.5.x 版本 解压后进行环境变量的配置：M2_HOME、%M2_HOME%\\bin。 在 cmd 窗口下输入mvn -v显示 Maven 版本即配置成功。 修改配置文件目录：…apache-maven-3.6.3\\conf\\settings.xml 本地仓库位置的修改在&lt;localRepository&gt;标签内添加自己的本地位置路径。 修改默认的 JDK 版本123456789101112&lt;profile&gt; &lt;id&gt;JDK-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt; 添加国内镜像源1234567891011121314151617181920212223&lt;!-- 阿里云仓库 --&gt;&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt;&lt;!-- 中央仓库1 --&gt;&lt;mirror&gt; &lt;id&gt;repo1&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://repo1.maven.org/maven2/&lt;/url&gt;&lt;/mirror&gt;&lt;!-- 中央仓库2 --&gt;&lt;mirror&gt; &lt;id&gt;repo2&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://repo2.maven.org/maven2/&lt;/url&gt;&lt;/mirror&gt; IDEA 中配置 Maven","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/tags/Maven/"}]},{"title":"IDEA 安装与配置","slug":"环境搭建/IDEA 安装与配置","date":"2020-01-29T08:25:01.000Z","updated":"2020-04-19T06:41:32.058Z","comments":true,"path":"2020/01/29/环境搭建/IDEA 安装与配置/","link":"","permalink":"http://yoursite.com/2020/01/29/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/IDEA%20%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"IDEA 全称 IntelliJ IDEA，是java语言开发的集成环境，IntelliJ在业界被公认为最好的java开发工具之一。让我们来了解一下这款编辑器，并且进行一定的配置。","text":"IDEA 全称 IntelliJ IDEA，是java语言开发的集成环境，IntelliJ在业界被公认为最好的java开发工具之一。让我们来了解一下这款编辑器，并且进行一定的配置。 IDEA 安装与破解下载 IDEA，链接：https://share.weiyun.com/50wrg7G 下载破解补丁，链接：https://share.weiyun.com/5kRW75Z 安装软件 （全部默认就行，小编安装在 D 盘的根目录），安装好后将破解补丁复制到 \\IntelliJ IDEA 2019.1.4\\bin\\ 目录下，在 bin 目录下分别找到 idea.exe.vmoptions 和 idea64.exe.vmoptions，在末尾添加-javaagent:安装目录下的\\IntelliJ IDEA 2019.1.3\\bin\\jetbrains-agent.jar后保存。 打开 IDEA，选择使用（Evaluate for free），选择 Configure - Edit Custom VM Options，在弹出的窗口确定末尾是刚刚修改的参数 -javaagent:D:\\IntelliJ IDEA 2019.1.4\\bin\\jetbrains-agent.jar （如果提示创建，选择是，在末尾再添加上面的参数）。 选择 Manage License， License Server 填写http://jetbrains-license-server，点击 Activate。重启你的 IDEA ,查看是否已经破解成功。 IDEA 模块在 Eclipse 中我们有 Workspace（工作空间）和 Project（工程）的概念，在 IDEA 中只有 Project（工程）和 Module（模块）的概念。 Eclipse IDEA Workspace Project Project Module IntelliJ IDEA 是无法在同一个窗口管理 n 个项目。 IntelliJ IDEA 提供的解决方案是打开多个项目实例，即打开多个项目窗口。 即： 一个 Project 打开一个 Window 窗口。 在 IntelliJ IDEA 中 Project 是最顶级的级别，次级别是 Module。一个 Project 可以有多个 Module。目前主流的大型项目都是分布式部署的， 结构都是类似这种多 Module 结构。 IDEA 配置View：Toolbar [√] 设置主题下载喜欢的 IDEA 的主题，下载地址：themesmap File：Import setting 选中下载的主题导入后提示重启，等待重启完成 Editor General Mouse：Change font size(Zoom) with Ctrl+Mouse Whee Other：Show quick documentation on mouse move Auto Import Java： appearance：Show method separators Code Completion：Match case [×] Editor Tabs：Show tabs in one row [×] Font：Size File and Code Templates 1234/**@author Wingo@create $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;TIME&#125;*/ File Encoding： Properties Files：UTF-8 [√] 快捷键设置KeyMap：Eclipse Eclipse 常用快捷键 执行 (run) alt+r 提示补全 (Class Name Completion) alt+/ 单行注释 ctrl + / 多行注释 ctrl + shift + / 向下复制一行 (Duplicate Lines) ctrl+alt+down 删除一行或选中行 (delete line) ctrl+d 向下移动行 (move statement down) alt+down 向上移动行 (move statement up) alt+up 向下开始新的一行 (start new line) shift+enter 向上开始新的一行 (Start New Line before current) ctrl+shift+enter 如何查看源码 (class) ctrl + 选中指定的结构 / ctrl + shift + t 万能解错 / 生成返回值变量 alt + enter 退回到前一个编辑的页面 (back) alt + left 进入到下一个编辑的页面 ( 针对于上条 ) (forward) alt + right 查看继承关系 (type hierarchy) F4 格式化代码 (reformat code) ctrl+shift+F 提示方法参数类型 (Parameter Info) ctrl+alt+/ 复制代码 ctrl + c 撤销 ctrl + z 反撤销 ctrl + y 查看类的结构：类似于 eclipse 的 outline ctrl+o 重构： 修改变量名与方法名 (rename) alt+shift+r 大写转小写 / 小写转大写 (toggle case) ctrl+shift+y 生成构造器/get/set/toString alt +shift + s 查看文档说明(quick documentation) F2 生成 try-catch 等(surround with) alt+shift+z 局部变量抽取为成员变量(introduce field) alt+shift+f 查找/替换(当前) ctrl+f 查找(全局) ctrl+h 查找文件 double Shift 查看类的继承结构图(Show UML Diagram) ctrl + shift + u 查看方法的多层重写结构(method hierarchy) ctrl+alt+h 抽取方法(Extract Method) alt+shift+m 关闭当前打开的代码栏(close) ctrl + w 关闭打开的所有代码栏(close all) ctrl + shift + w 快速搜索类中的错误(next highlighted error) ctrl + shift + q 查找方法在哪里被调用(Call Hierarchy) ctrl+shift+h 关于模板Editor &gt; General &gt; Postfix Completion Editor &gt; Live Templates（可自定义） 修改当行注解风格打开 idea 的设置，依次点击「Editor」——「Code Style」——「Java」——「Code Generation」，然后取消勾选 Line comment at first column，同时勾选 Add a space at comment start","categories":[{"name":"Software","slug":"Software","permalink":"http://yoursite.com/categories/Software/"}],"tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"}]},{"title":"Hexo 安装与配置","slug":"环境搭建/Hexo 安装与配置","date":"2020-01-29T04:59:01.000Z","updated":"2020-07-10T02:59:31.918Z","comments":true,"path":"2020/01/29/环境搭建/Hexo 安装与配置/","link":"","permalink":"http://yoursite.com/2020/01/29/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/Hexo%20%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"Hexo 介绍Hexo 是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。","text":"Hexo 介绍Hexo 是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。 官网： http://hexo.io GitHub 搭建工作仓库搭建Hexo 是基于 GitHub 的，首先你需要有一个 GitHub 的账号，并且创建一个用于存放博客的静态文件的仓库。这个仓库的名称必须是[username].github.io，之后你便可以用http://[username].github.io访问你搭建的博客。 域名绑定在云平台购买自己喜欢的域名，并且在平台的控制台进行域名的绑定。 打开控制台ping http://[username].github.io，得到 IP 地址。 然后到你的域名 DNS 设置页，将 A 记录指向你 ping 出来的 IP，将 CNAME 指向[username].github.io，这样可以保证无论是否添加 www 都可以访问。 CNAME 文件：创建一个 CNAME ，在其中输入你的域名，保存后放置在你的 theme 目录下的 resource 目录下 配置 SSH Key到 Github 官网下载客户端，右键选择 git bash here。输入 cd ~/.ssh 检查本机是否已经存在 SSH 密钥。 若显示 No such file or directory 则说明本机还未创建 SSH 密钥。 输入ssh-keygen -t rsa -C &quot;[email]&quot;，并连续回车直到提示密钥生成成功。 输入cd ~/.ssh，然后输入ls即可看到此目录下生成的公钥（pub）和私钥。利用cat命令取得公钥的内容，并且复制公钥内容，粘贴到 GitHub 的 Settings 中的 SSH Keys 中（名称自定义即可）。 输入ssh -T git@github.com，输入 yes 后若提示连接成功则 SSH Key 配置成功。 此外，你还需要输入git config --global user.name &quot;[username]&quot;和git config --global user.email &quot;[email]&quot;进行配置。（局部配置时不加 --global） 多个 SSH Key为了区别个人 / 公司的代码仓库，公司的代码仓库的邮件生成一个新的 SSH Key，为避免之前的创建的 id_rsa 被覆盖，此次创建需要显示的重命名文件 id_rsa_welab。 得到公钥并且复制到公司用户后，进行项目克隆时报错，排查后发现此 id_rsa_welab 非默认命名，故需要手动添加到 ssh-agent 中（需手动开启）。 12# 开启 ssh-agent !!! 注意 是 ~ 下面的 ``，不是 ''eval `ssh-agent` 输入 ssh-add [path]命令。 添加后即可成功克隆项目。 Hexo 安装首先要安装 Node.js，然后利用 npm 安装 Hexo。 右键选择 git bash here。输入npm install -g Hexo，等待下载完成。 输入 cd [blog catalogue]，到你放置 Blog 文件的目录下，输入hexo init初始化 Hexo 文件。 输入 hexo generate回车，hexo server回车，浏览器输入localhost:4000即可看到博客页面 常用命令1234hexo cleanhexo generatehexo serverhexo deploy 主题更换见：[hexo-theme-indigo]","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}]},{"title":"Netty 引导","slug":"后台技术/Netty/Netty 引导","date":"2020-01-25T16:25:24.000Z","updated":"2020-07-10T04:04:28.236Z","comments":true,"path":"2020/01/26/后台技术/Netty/Netty 引导/","link":"","permalink":"http://yoursite.com/2020/01/26/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%BC%95%E5%AF%BC/","excerpt":"引导一个应用程序是指对它进行配置，并使它运行起来的过程。Netty处理引导的方式使你的应用程序和网络层相隔离，无论它是客户端还是服务器。所有的框架组件都将会在后台结合在 一起并且启用。","text":"引导一个应用程序是指对它进行配置，并使它运行起来的过程。Netty处理引导的方式使你的应用程序和网络层相隔离，无论它是客户端还是服务器。所有的框架组件都将会在后台结合在 一起并且启用。 Bootstrap 类引导类的层次结构包括一个抽象的父类和两个具体的引导子类。 相对于将具体的引导类分别看作用于服务器和客户端的引导来说，记住它们的本意是用来支 撑不同的应用程序的功能。也就是说，服务器致力于使用一个父 Channel 来接受来自客户端的连接，并创建子 Channel 以用于它们之间的通信；而客户端将最可能只需要一个单独的、没有父 Channel 的 Channel 来用于所有的网络交互。 为什么引导类是 Cloneable 的： 你有时可能会需要创建多个具有类似配置或者完全相同配置的Channel。为了支持这种模式而又不需要为每个 Channel 都创建并配置一个新的引导类实例， AbstractBootstrap 被标记为了 Cloneable。 注意，这种方式只会创建引导类实例的 EventLoopGroup 的一个浅拷贝，所以，后者将在所有克隆的 Channel 实例之间共享。这是可以接受的，因为通常这些克隆的 Channel 的生命周期都很短暂，一个典型的场景是：创建一个 Channel 以进行一次HTTP请求。 引导客户端Bootstrap 类负责为客户端和使用无连接协议的应用程序创建 Channel。 12345678910111213141516171819202122232425262728293031323334// 引导一个客户端EventLoopGroup group = new NioEventLoopGroup();// 创建一个 Bootstrap 类的实例用来创建和连接新的客户端 ChannelBootstrap bootstrap = new Bootstrap();// 设置 EventLoopGroup 提供用于处理 Channel 事件的 EventLoopbootstrap.group(group) // 指定要使用的 Channel 实现 .channel(NioSocketChannel.class) .handler( // 设置用于 Channel 事件和数据的 ChannelInboundHandler new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channeRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; System.out.println(\"Received data\"); &#125; &#125; );ChannelFuture future = bootstrap.connect( // 连接到远程主机 new InetSocketAddress(\"www.manning.com\", 80));future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(\"Connection established\"); &#125; else &#123; System.err.println(\"Connection attempt failed\"); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 引导服务器负责引导 ServerChannel 的 ServerBootstrap 提供了负责创建子 Channel 的方法 childXxx()，这些子 Channel 代表了已被接收的连接。 1234567891011121314151617181920212223242526272829// 引导服务器NioEventLoopGroup group = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(group) .channel(NioServerSocketChannel.class) // 子线程 .childHandler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf byteBuf) throws Exception &#123; System.out.println(\"Received data\"); &#125; &#125; );// 监听端口ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(\"Server bound\"); &#125; else &#123; System.err.println(\"Bound attempt failed\"); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 从 Channel 引导客户端假设你的服务器正在处理一个客户端的请求，这个请求需要它充当第三方系统的客户端。 1234567891011121314151617181920212223242526272829303132333435ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(new NioEventLoopGroup(), new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; ChannelFuture connectFuture; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class).handler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; System.out.println(\"Received data\"); &#125; &#125; ); // 使用与分配给已被接受的子 Channel 相同的 EventLoop bootstrap.group(ctx.channel().eventLoop()); connectFuture = bootstrap.connect( new InetSocketAddress(\"www.manning.com\", 80)); &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; if (connectFuture.isDone()) &#123; // do something with the data &#125; &#125; &#125; );ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture)throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(\"Server bound\"); &#125; else &#123; System.err.println(\"Bind attempt failed\"); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 尽可能地重用 EventLoop，以减少线程创建所带来的开销。 在引导过程中添加多个 ChannelHandler在前面的几个例子中，在引导的过程中调用了 handler() 或者 childHandler() 方法来添加单个的 ChannelHandler。这对于简单的应用程序来说可能已经足够了，但是它不能满足更加复杂的需求。 protected abstract void initChannel(C ch) throws Exception; 这个方法提供了一种将多个 ChannelHandler 添加到一个 ChannelPipeline 中的简便方法 12345678910111213141516// 引导和使用 ChannelInitializerServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(new NioEventLoopGroup(), new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializerImpl());ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.sync();// ChannelInitializer 的实现final class ChannelInitializerImpl extends ChannelInitializer&lt;Channel&gt; &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new HttpClientCodec()); pipeline.addLast(new HttpObjectAggregator(Integer.MAX_VALUE)); &#125; &#125; 如果你的应用程序使用了多个 ChannelHandler，请定义你自己的 ChannelInitializer 实现来将它们安装到 ChannelPipeline 中。 ChannelOption 和属性你可以使用 option() 方法来将 ChannelOption 应用到引导。你所提供的值将会被自动应用到引导所创建的所有 Channel。可用的 ChannelOption 包括了底层连接的详细信息，如 keep-alive 或者超时属性以及缓冲区设置。 Netty 提供了 AttributeMap 抽象（一个由 Channel 和引导类提供的集合）以及 AttributeKey（一 个用于插入和获取属性值的泛型类）。使用这些工具，便可以安全地将任何类型的数据项与客户端和服务器 Channel（包含 ServerChannel 的子 Channel）相关联了。 123456789101112131415161718192021222324252627// 使用属性值// 创建一个 AttributeKey 以标识该属性final AttributeKey&lt;Integer&gt; id = new AttributeKey&lt;Integer&gt;(\"ID\");Bootstrap bootstrap = new Bootstrap();bootstrap.group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // 使用 AttributeKey 检索属性以及它的值 Integer idValue = ctx.channel().attr(id).get(); // do something with the idValue &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; System.out.println(\"Received data\"); &#125; &#125; );// 引导的每一个 Channel 都将具有这些属性bootstrap.option(ChannelOption.SO_KEEPALIVE,true) .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000);// 存储该 id 属性bootstrap.attr(id, 123456);ChannelFuture future = bootstrap.connect(new InetSocketAddress(\"www.manning.com\", 80));future.syncUninterruptibly(); 引导 DatagramChannel除了基于 TCP 协议的 SocketChannel，Bootstrap 类也可以被用于无连接协议。为此。Netty 提供了各种 DatagramChannel 的实现。唯一区别就是，不再调用 connect() 方法，而是只调用 bind() 方法。 1234567891011121314151617181920212223242526// 使用 Bootstrap 和 DatagramChannelBootstrap bootstrap = new Bootstrap();bootstrap.group(new OioEventLoopGroup()) .channel(OioDatagramChannel.class) .handler( new SimpleChannelInboundHandler&lt;DatagramPacket&gt;()&#123; @Override public void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; // Do something with the packet &#125; &#125; );// 调用 bind() 方法，因为该协议是无连接的ChannelFuture future = bootstrap.bind(new InetSocketAddress(0));future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(\"Channel bound\"); &#125; else &#123; System.err.println(\"Bind attempt failed\"); channelFuture.cause().printStackTrace(); &#125; &#125;&#125;); 关闭引导使你的应用程序启动并且运行起来，但是迟早你都需要优雅地将它关闭。 123456789// 优雅的关闭EventLoopGroup group = new NioEventLoopGroup();Bootstrap bootstrap = new Bootstrap();bootstrap.group(group) .channel(NioSocketChannel.class);...;Future&lt;?&gt; future = group.shutdownGracefully();// block until the group has shutdownfuture.syncUninterruptibly(); 注意，shutdownGracefully() 方法也是一个异步的操作，所以你需要阻塞等待直到它完成，或者向所返回的 Future 注册一个监听器以在关闭完成时获得通知。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Netty EventLoop 和线程模型","slug":"后台技术/Netty/Netty EventLoop 和线程模型","date":"2020-01-25T08:09:20.000Z","updated":"2020-07-10T04:04:11.749Z","comments":true,"path":"2020/01/25/后台技术/Netty/Netty EventLoop 和线程模型/","link":"","permalink":"http://yoursite.com/2020/01/25/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20EventLoop%20%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"线程模型指定了操作系统、编程语言、框架或者应用程序的上下文中的线程管理的关键方面，如何以及何时创建线程将对应用程序代码的执行产生显著的影响。","text":"线程模型指定了操作系统、编程语言、框架或者应用程序的上下文中的线程管理的关键方面，如何以及何时创建线程将对应用程序代码的执行产生显著的影响。 线程模型概述多核心或多个 CPU 的计算机现在已经司空见惯，大多数的现代应用程序都利用了 复杂的多线程处理技术以有效地利用系统资源。 在早期的 Java 语言中，我们使用多线程处理的主要方式无非是按需创建和启动新的 Thread 来执行并发的任务单元，这是一种在高负载下工作得很差的原始方式。Java 5 随后引入了 Executor API，其线程池通过缓存和重用 Thread 极大地提高了性能。 虽然池化和重用线程相对于简单地为每个任务都创建和销毁线程是一种进步，但是它并不能 消除由上下文切换所带来的开销，其将随着线程数量的增加很快变得明显，并且在高负载下愈演愈烈。Netty 框架帮助简化了这一处理。 EventLoop 接口运行任务来处理在连接的生命周期内发生的事件是任何网络框架的基本功能。与之相应的编程上的构造通常被称为事件循环：Netty 使用了 interface io.netty.channel.EventLoop 来适配的术语。 1234567// 在事件循环中执行任务while (!terminated) &#123; List&lt;Runnable&gt; readyEvents = blockUntilEventsReady(); for (Runnable ev: readyEvents) &#123; ev.run(); &#125; &#125; Netty 的 EventLoop 是协同设计的一部分，它采用了两个基本的 API：并发和网络编程。首先，io.netty.util.concurrent 包构建在 JDK 的 java.util.concurrent 包上，用来提供线程执行器。其次，io.netty.channel 包中的类，为了与 Channel 的事件进行交互，扩展了这些接口/类。 在这个模型中，一个 EventLoop 将由一个永远都不会改变的 Thread 驱动，同时任务 （Runnable 或者 Callable）可以直接提交给 EventLoop 实现，以立即执行或者调度执行。根据配置和可用核心的不同，可能会创建多个 EventLoop 实例用以优化资源的使用，并且单个 EventLoop 可能会被指派用于服务多个 Channel。 事件/任务的执行顺序 ： 事件和任务是以先进先出（FIFO）的顺序执行的。这样可以通过保证字节内容总是按正确的顺序被处理，消除潜在的数据损坏的可能性。 任务调度使用核心的 Java API 和 Netty 的 EventLoop 来调度任务 JDK 的任务调度 API123456789101112// 使用 ScheduledExecutorService 来在 60 秒的延迟之后执行一个任务ScheduledExecutorService executor = Executors.newScheduledThreadPool(10);ScheduledFuture&lt;?&gt; future = executor.schedule( new Runnable() &#123; @Override public void run() &#123; System.out.println(\"60 seconds later\"); &#125; &#125;, 60, TimeUnit.SECONDS); // 调度任务在从现在开始的 60 秒之后执行...;executor.shutdown(); 使用 EventLoop 调度任务12345678910// 使用 EventLoop 来在 60 秒的延迟之后执行一个任务Channel ch = ...;ScheduledFuture&lt;?&gt; future = ch.eventLoop().schedule( new Runnable() &#123; @Override public void run() &#123; System.out.println(\"60 seconds later\"); &#125; &#125;, 60, TimeUnit.SECONDS); 123456789101112// 使用 EventLoop 调度周期性的任务Channel ch = ...;ScheduledFuture&lt;?&gt; future = ch.eventLoop().scheduleAtFixedRate( new Runnable() &#123; @Override public void run() &#123; System.out.println(\"Run every 60 seconds\"); &#125; &#125;, 60, 60, TimeUnit.Seconds);// 利用每个异步操作所返回的 ScheduledFuture 取消任务future.cancel(false); 实现细节更加详细地探讨 Netty 的线程模型和任务调度实现的主要内容。 线程管理Netty 线程模型的卓越性能取决于对于当前执行的 Thread 的身份的确定。 永远不要将一个长时间运行的任务放入到执行队列中，因为它将阻塞需要在同一线程上执行的任何其他任务。如果必须要进行阻塞调用或者执行长时间运行的任务，我们建议使用一个专门的 EventExecutor。 EventLoop / 线程的分配服务于 Channel 的 I/O 和事件的 EventLoop 包含在 EventLoopGroup 中。根据不同的传输实现，EventLoop 的创建和分配方式也不同。 异步传输异步传输实现只使用了少量的 EventLoop（以及和它们相关联的 Thread），而且在当前的线程模型中，它们可能会被多个 Channel 所共享。这使得可以通过尽可能少量的 Thread 来支撑大量的 Channel，而不是每个 Channel 分配一个 Thread。 一旦一个 Channel 被分配给一个 EventLoop，它将在它的整个生命周期中都使用这个 EventLoop（以及相关联的 Thread）。 阻塞传输这里每一个 Channel 都将被分配给一个 EventLoop（以及它的 Thread）。 每个 Channel 的 I/O 事件都将只会被一个 Thread （用于支撑该 Channel 的 EventLoop 的那个 Thread）处理（Netty 的一致性体现）。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Netty ChannelHandler 和 ChannelPipeline","slug":"后台技术/Netty/Netty ChannelHandler 和 ChannelPipeline","date":"2020-01-24T14:09:35.000Z","updated":"2020-07-10T04:04:14.733Z","comments":true,"path":"2020/01/24/后台技术/Netty/Netty ChannelHandler 和 ChannelPipeline/","link":"","permalink":"http://yoursite.com/2020/01/24/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20ChannelHandler%20%E5%92%8C%20ChannelPipeline/","excerpt":"ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑，还有一个重要的关系 ChannelHandlerContext。","text":"ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑，还有一个重要的关系 ChannelHandlerContext。 ChannelHandler 家族Channel 的生命周期当 Channel 的状态发生改变时，将会生成对应的事件。这些事件将会被转发给 ChannelPipeline 中的 ChannelHandler，其可以随后对它们做出响应。 ChannelHandler 的生命周期 类型 描述 handlerAdded 当把 ChannelHandler 添加到 ChannelPipeline 中时被调用 handlerRemoved 当从 ChannelPipeline 中移除 ChannelHandler 时被调用 exceptionCaught 当处理过程中在 ChannelPipeline 中有错误产生时被调用 Netty 定义了两个重要的 Channelhandler 子接口： ChannelInboundHandler——处理入站数据以及各种状态变化； ChannelOutboundHandler——处理出站数据并且允许拦截所有的操作。 ChannelInboundHandler 接口12345678910// 释放消息资源@Sharable// 扩展了 ChannelInboundHandlerAdapterpublic class DiscardHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; // 丢弃已接收的消息 ReferenceCountUtil.release(msg); &#125; &#125; 123456789// 使用 SimpleChannelInboundHandler 会自动释放资源@Sharablepublic class SimpleDiscardHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; @Override public void channelRead0(ChannelHandlerContext ctx, Object msg) &#123; // 不需要任何显式的资源释放 // No need to do anything special &#125; &#125; ChannelOutboundHandler 接口ChannelOutboundHandler 的一个强大的功能是可以按需推迟操作或者事件，这使得可 以通过一些复杂的方法来处理请求。 ChannelPromise 与 ChannelFuture：ChannelOutboundHandler 中的大部分方法都需要一个 ChannelPromise 参数，以便在操作完成时得到通知。ChannelPromise 是 ChannelFuture 的一个子类，其定义了一些可写的方法，如 setSuccess() 和 setFailure()，从而使 ChannelFuture 不可变。 ChannelHandler 适配器适配器对 ChannelHandler 进行了简单的实现，只需要简单地扩展它们，并且重写那些你想要自定义的方法。 ChannelInboundHandlerAdapter 和 ChannelOutboundHandlerAdapter 中所提供的方法体调用了其相关联的 ChannelHandlerContext 上的等效方法，从而将事件转发到了 ChannelPipeline 中的下一个 ChannelHandler 中。 资源管理每当通过调用 ChannelInboundHandler.channelRead() 或者 ChannelOutboundHandler.write() 方法来处理数据时，你都需要确保没有任何的资源泄漏。 Netty 使用引用计数来处理池化的 ByteBuf。所以在完全使用完某个 ByteBuf 后，调整其引用计数是很重要的。 123456789101112// 丢弃并释放出战消息@Sharablepublic class DiscardOutboundHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) &#123; // 释放资源 ReferenceCountUtil.release(msg); // 通知 ChannelPromise 数据已经被处理了 promise.setSuccess(); &#125;&#125; 重要的是，不仅要释放资源，还要通知 ChannelPromise。否则可能会出现 ChannelFutureListener 收不到某个消息已经被处理了的通知的情况。 如果一个消息被消费或者丢弃了，并且没有传递给 ChannelPipeline 中的下一个 ChannelOutboundHandler，那么用户就有责任调用 ReferenceCountUtil.release()。 ChannelPipeline 接口每一个新创建的 Channel 都将会被分配一个新的 ChannelPipeline。这项关联是永久性的；Channel 既不能附加另外一个 ChannelPipeline，也不能分离其当前的。 根据事件的起源，事件将会被 ChannelInboundHandler 或者 ChannelOutboundHandler 处理。随后，通过调用 ChannelHandlerContext 实现，它将被转发给同一超类型的下一个 ChannelHandler。 修改 ChannelPipelineChannelHandler 可以通过添加 addXxx()、删除 remove() 或者替换 replace() 其他的 ChannelHandler 来实时地修改 ChannelPipeline 的布局。 ChannelHandler 的执行和阻塞 通常 ChannelPipeline 中的每一个 ChannelHandler 都是通过它的 EventLoop（I/O 线程）来处理传递给它的事件的。所以至关重要的是不要阻塞这个线程，因为这会对整体的 I/O 处理产生负面的影响。 但有时可能需要与那些使用阻塞 API 的遗留代码进行交互。对于这种情况，ChannelPipeline 有一些接受一个 EventExecutorGroup 的 add()方法。如果一个事件被传递给一个自定义的 EventExecutorGroup，它将被包含在这个 EventExecutorGroup 中的某个 EventExecutor 所处理，从而被从该 Channel 本身的 EventLoop 中移除。对于这种用例，Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。 getXxx()：通过类型或者名称来访问 ChannelHandler 的方法。 context()：返回和 ChannelHandler 绑定的 ChannelHandlerContext。 names()：返回 ChannelPipeline 中所有 ChannelHandler 的名称。 触发事件ChannelPipeline 的 API 公开了用于调用入站和出站操作的附加方法。 小结 ChannelPipeline 保存了与 Channel 相关联的 ChannelHandler； ChannelPipeline 可以根据需要，通过添加或者删除 ChannelHandler 来动态地修改； ChannelPipeline 有着丰富的 API 用以被调用，以响应入站和出站事件。 ChannelHandlerContext 接口ChannelHandlerContext 代表了 ChannelHandler 和 ChannelPipeline 之间的关联，每当有 ChannelHandler 添加到 ChannelPipeline 中时，都会创建 ChannelHandlerContext。 ChannelHandlerContext 的主要功能是管理它所关联的 ChannelHandler 和在同一个 ChannelPipeline 中的其他 ChannelHandler 之间的交互。 ChannelHandlerContext 和 ChannelHandler 之间的关联（绑定）是永远不会改变的，所以缓存对它的引用是安全的； 使用 ChannelHandlerContextChannel、ChannelPipeline、ChannelHandler 以及 ChannelHandlerContext 之间的关系 1234// 从 ChannelHandlerContext 访问 ChannelChannelHandlerContext ctx = ..;Channel channel = ctx.channel();channel.write(Unpooled.copiedBuffer(\"Netty in Action\", CharsetUtil.UTF_8)); 1234// 通过 ChannelHandlerContext 访问 ChannelPipelineChannelHandlerContext ctx = ..;ChannelPipeline pipeline = ctx.pipeline();pipeline.write(Unpooled.copiedBuffer(\"Netty in Action\", CharsetUtil.UTF_8)); 重要的是要注意到，虽然被调用的 Channel 或 ChannelPipeline 上的 write()方法将一直传播事件通 过整个 ChannelPipeline，但是在 ChannelHandler 的级别上，事件从一个 ChannelHandler 到下一个 ChannelHandler 的移动是由 ChannelHandlerContext 上的调用完成的. 1234// 调用 ChannelHandlerContext 的 write()方法ChannelHandlerContext ctx = ..;// write()方法将把缓冲区数据发送到下一个 ChannelHandlerctx.write(Unpooled.copiedBuffer(\"Netty in Action\", CharsetUtil.UTF_8)); 高级用法缓存 ChannelHandlerContext 的引用以供稍后使用，这可能会发生在任何的 ChannelHandler 方法之外，甚至来自于不同的线程。 12345678910111213// 缓存到 ChannelHandlerContext 的引用public class WriteHandler extends ChannelHandlerAdapter &#123; private ChannelHandlerContext ctx; @Override public void handlerAdded(ChannelHandlerContext ctx) &#123; //存储到 ChannelHandlerContext 的引用以供稍后使用 this.ctx = ctx; &#125; public void send(String msg) &#123; // 使用之前存储的到 ChannelHandlerContext 的引用来发送消息 ctx.writeAndFlush(msg); &#125; &#125; 因为一个 ChannelHandler 可以从属于多个 ChannelPipeline，所以它也可以绑定到多个 ChannelHandlerContext 实例。对于这种用法指在多个 ChannelPipeline 中共享同一 个 ChannelHandler，对应的 ChannelHandler 必须要使用 @Sharable 注解标注；否则， 试图将它添加到多个 ChannelPipeline 时将会触发异常。显而易见，为了安全地被用于多个 并发的 Channel（即连接），这样的 ChannelHandler 必须是线程安全的。 123456789// 可共享的 ChannelHandler@Sharable public class SharableHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; System.out.println(\"Channel read message: \" + msg); ctx.fireChannelRead(msg); &#125; &#125; 异常处理Netty 提供了几种方式用于处理入站或者出站处理过程中所抛出的异常。 处理入站异常12345678// 基本的入站异常处理public class InboundExceptionHandler extends ChannelInboundHandlerAdapter &#123; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; &#125; 处理出站异常出站的异常处理基于一下通知机制： 每个出站操作都将返回一个 ChannelFuture。注册到 ChannelFuture 的 ChannelFutureListener 将在操作完成时被通知该操作是成功了还是出错了。 几乎所有的 ChannelOutboundHandler 上的方法都会传入一个 ChannelPromise 的实例。作为 ChannelFuture 的子类，ChannelPromise 也可以被分配用于异步通知的监听器。但是，ChannelPromise 还具有提供立即通知的可写方法。 ChannelPromise 的可写方法 通过调用 ChannelPromise 上的 setSuccess() 和 setFailure() 方法，可以使一个操作的状态在 ChannelHandler 的方法返回给其调用者时便即刻被感知到。 12345678910// 添加 ChannelFutureListener 到 ChannelFutureChannelFuture future = channel.write(someMessage);future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture f) &#123; if (!f.isSuccess()) &#123; f.cause().printStackTrace(); f.channel().close(); &#125; &#125;&#125;); 123456789101112131415// 添加 ChannelFutureListener 到 ChannelPromisepublic class OutboundExceptionHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) &#123; promise.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture f) &#123; if (!f.isSuccess()) &#123; f.cause().printStackTrace(); f.channel().close(); &#125; &#125; &#125;); &#125; &#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Netty ByteBuf","slug":"后台技术/Netty/Netty ByteBuf","date":"2020-01-24T03:29:09.000Z","updated":"2020-07-10T04:04:17.155Z","comments":true,"path":"2020/01/24/后台技术/Netty/Netty ByteBuf/","link":"","permalink":"http://yoursite.com/2020/01/24/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20ByteBuf/","excerpt":"总所周知，网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。因此，Netty 提供了一个替代品 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。","text":"总所周知，网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。因此，Netty 提供了一个替代品 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。 ByteBuf 的 APINetty 的数据处理 API 通过两个组件暴露：abstract class ByteBuf 和 interface ByteBufHolder。 ByteBuf 类ByteBuf 类是 Netty 的数据容器，通过使用不同的索引来简化对它所包含的数据的访问。 HowByteBuf 维护了两个不同的索引：一个用于读取，一个用于写入。读取时，readerIndex 将会被递增已经被读取的字节数；写入时，writerIndex 也会被递增。readerIndex == writerIndex 时则到达可读取数据的末尾。 名称以 read 或者 write 开头的 ByteBuf 方法，将会推进其对应的索引，而名称以 set 或者 get 开头的操作则不会。后面的这些方法将在作为一个参数传入的一个相对索引上执行操作。 Mode在使用 Netty 时，有几种常见的围绕 ByteBuf 而构建的使用模式。 堆缓冲区最常用的 ByteBuf 模式是将数据存储在 JVM 的堆空间中。这种模式被称为支撑数组。 12345678910111213// 支撑数组ByteBuf heapBuf = ...;// 检查 ByteBuf 是否有一个支撑数组if (heapBuf.hasArray()) &#123; // 获取对该数组的引用 byte[] array = heapBuf.array(); // 计算第一个字节的偏移量 int offset = heapBuf.arrayOffset() + heapBuf.readerIndex(); // 获取可读字节数 int length = heapBuf.readableBytes(); // 使用获取的信息作为参数调用自定义方法 handleArray(array, offset, length);&#125; 直接缓冲区（Updating）复合缓冲器（Updating）字节级操作ByteBuf 提供了许多超出基本读、写操作的方法用于修改它的数据。 随机访问索引123456// 访问数据ByteBuf buffer = ...;for (int i = 0; i &lt; buffer.capacity(); i++) &#123; byte b = buffer.getByte(i); System.out.println((char)b);&#125; 顺序访问索引虽然 ByteBuf 同时具有读索引和写索引，但是 JDK 的 ByteBuffer 却只有一个索引，这也就是为什么必须调用 flip() 方法来在读模式和写模式之间进行切换的原因 ByteBuf 的内部分段 可丢弃字节可丢弃字节的分段包含了已经被读过的字节。通过调用 discardReadBytes() 方法，可以丢弃它们并回收空间。这个分段的初始大小为 0，存储在 readerIndex 中，会随着 read 操作的执行而增加（get*操作不会移动 readerIndex）。 可读字节ByteBuf 的可读字节分段存储了实际数据。 12345// 读取所有数据ByteBuf buffer = ...;while (buffer.isReadable()) &#123; System.out.println(buffer.readByte());&#125; 可写字节可写字节分段是指一个拥有未定义内容的、写入就绪的内存区域。 12345// 写数据ByteBuf buffer = ...;while (buffer.writableBytes() &gt;= 4) &#123; buffer.writeInt(random.nextInt());&#125; 索引管理可以通过调用 markReaderIndex()、markWriterIndex()、resetWriterIndex() 和 resetReaderIndex() 来标记和重置 ByteBuf 的 readerIndex 和 writerIndex。 也可以通过调用 readerIndex(int) 或者 writerIndex(int) 来将索引移动到指定位置。试图将任何一个索引设置到一个无效的位置都将导致一个 IndexOutOfBoundsException。 可以通过调用 clear()方法来将 readerIndex 和 writerIndex 都设置为 0（不会清除内存中的内容）。 查找操作（Updating）123// 使用 ByteBufProcessor 来寻找\\rByteBuf buffer = ...;int index = buffer.forEachByte(ByteBufProcessor.FIND_CR); 派生缓冲区派生缓冲区为 ByteBuf 提供了以专门的方式来呈现其内容的视图。修改了其中一个，源实例也会被修改（共享）。 12345678910// 对 ByteBuf 进行切片Charset utf8 = Charset.forName(\"UTF-8\");ByteBuf buf = Unpooled.copiedBuffer(\"Netty in Action rocks!\", utf8);ByteBuf sliced = buf.slice(0, 15);// 打印 Netty in ActionSystem.out.println(sliced.toString(utf8));// 更改索引 0 处的字节buf.setByte(0, (byte)'J');// True 因为数据是共享关系assert buf.getByte(0) == sliced.getByte(0); ByteBuf 复制 如果需要一个现有缓冲区的真实副本，请使用 copy() 或者 copy(int, int) 方法。不同于派生缓冲区，由这个调用所返回的 ByteBuf 拥有独立的数据副本 12345678// 复制一个 ByteBufCharset utf8 = Charset.forName(\"UTF-8\");ByteBuf buf = Unpooled.copiedBuffer(\"Netty in Action rocks!\", utf8);ByteBuf copy = buf.copy(0, 15);System.out.println(copy.toString(utf8));buf.setByte(0, (byte) 'J');// True 因为数据是非共享关系assert buf.getByte(0) != copy.getByte(0); 除了修改原始 ByteBuf 的切片或者副本的效果以外，这两种场景是相同的。只要有可能， 使用 slice() 方法来避免复制内存的开销。 读写操作getXxx() 和 setXxx() 操作，从给定的索引开始，并且保持索引不变。 12345678910111213// getXxx() And setXxx()Charset utf8 = Charset.forName(\"UTF-8\");ByteBuf buf = Unpooled.copiedBuffer(\"Netty in Action rocks!\", utf8);// 打印第一个字符 NSystem.out.println((char)buf.getByte(0));int readerIndex = buf.readerIndex();int writerIndex = buf.writerIndex();// 更改索引 0 处的字节buf.setByte(0, (byte)'B');System.out.println((char)buf.getByte(0));// True 不会影响相应的索引assert readerIndex == buf.readerIndex();assert writerIndex == buf.writerIndex(); readXxx() 和 writeXxx() 操作，从给定的索引开始，并且会根据已经访问过的字节数对索 引进行调整。 1234567891011// readXxx() And writeXxx()Charset utf8 = Charset.forName(\"UTF-8\");ByteBuf buf = Unpooled.copiedBuffer(\"Netty in Action rocks!\", utf8);System.out.println((char)buf.readByte());int readerIndex = buf.readerIndex();int writerIndex = buf.writerIndex();// 将字符 ？ 追加到缓存区，writerIndex 改变buf.writeByte((byte)'?');// Trueassert readerIndex == buf.readerIndex();assert writerIndex != buf.writerIndex(); ByteBufHolder 接口如果想要实现一个将其有效负载存储在 ByteBuf 中的消息对象，那么 ByteBufHolder 将是个不错的选择。 ByteBuf 分配常见的几种管理 ByteBuf 实例的不同方式。 按需分配：ByteBufAllocator 接口为了降低分配和释放内存的开销，Netty 通过 interface ByteBufAllocator 实现了（ByteBuf 的）池化，它可以用来分配我们所描述过的任意类型的 ByteBuf 实例。 可以通过 Channel（每个都可以有一个不同的 ByteBufAllocator 实例）或者绑定到 ChannelHandler 的 ChannelHandlerContext 获取一个到 ByteBufAllocator 的引用。 1234567// 获取一个到 ByteBufAllocator 的引用Channel channel = ...;ByteBufAllocator allocator = channel.alloc();....ChannelHandlerContext ctx = ...;ByteBufAllocator allocator2 = ctx.alloc();... Netty提供了两种ByteBufAllocator的实现：PooledByteBufAllocator 和 UnpooledByteBufAllocator。 Unpooled 缓冲池可能某些情况下，你不需要一个 ByteBufAllocator 的引用。对于这种情况，Netty 提供了一个简单的称为 Unpooled 的工具类，它提供了静态的辅助方法来创建未池化的 ByteBuf 实例。 ByteBufUtil 类ByteBufUtil 提供了用于操作 ByteBuf 的静态的辅助方法。 引用计数引用计数是一种通过在某个对象所持有的资源不再被其他对象引用时释放该对象所持有的资源来优化内存使用和性能的技术。Netty 在第 4 版中为 ByteBuf 和 ByteBufHolder 引入了引用计数技术，它们都实现了 interface ReferenceCounted。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Netty 传输","slug":"后台技术/Netty/Netty 传输","date":"2020-01-22T11:03:21.000Z","updated":"2020-07-10T04:04:47.561Z","comments":true,"path":"2020/01/22/后台技术/Netty/Netty 传输/","link":"","permalink":"http://yoursite.com/2020/01/22/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E4%BC%A0%E8%BE%93/","excerpt":"流经网络的数据总是具有相同的类型：字节。这些字节是如何流动的主要取决于我们所说的网络传输（一个帮助我们抽象底层数据传输机制的概念）","text":"流经网络的数据总是具有相同的类型：字节。这些字节是如何流动的主要取决于我们所说的网络传输（一个帮助我们抽象底层数据传输机制的概念） 案例研究：JDK VS. Netty原生 JDK123456789101112131415161718192021222324252627282930313233343536// JDK 的阻塞网络编程public class PlainOioServer &#123; public void serve(int port) throws IOException &#123; final ServerSocket socket = new ServerSocket(port); try &#123; for (;;) &#123; final Socket clientSocket = socket.accept(); System.out.println(\"Accepted connection from \" + clientSocket); new Thread( // 创建一个新的线程来处理该连接 new Runnable() &#123; @Override public void run() &#123; OutputStream out; try &#123; out = clientSocket.getOutputStream(); // 将消息写给已连接的客户端 out.write(\"Hi!\\r\\n\".getBytes(Charset.forName(\"UTF-8\"))); out.flush(); clientSocket.close(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; clientSocket.close(); &#125;catch (IOException ex) &#123; // ignore on close &#125; &#125; &#125; &#125;).start(); // 启动线程 &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 随着应用程序的用户越来越多，阻塞 I/O 并不能很好地伸缩到支撑成千上万地并发连入连接。此时异步 I/O 可以解决这个问题，但异步 I/O 的 API 是完全不同的，因此不得不重写此应用程序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// JDK 非阻塞网络处理public class PlainNioServer &#123; public void serve(int port) throws IOException &#123; ServerSocketChannel serverChannel = ServerSocketChannel.open(); serverChannel.configureBlocking(false); ServerSocket ssocket = serverChannel.socket(); InetSocketAddress address = new InetSocketAddress(port); ssocket.bind(address); // 打开 Selector 来处理 Channel Selector selector = Selector.open(); // 将 ServerChannel 注册到 Selector 以便接受连接（报个名） serverChannel.register(selector, SelectionKey.OP_ACCEPT); final ByteBuffer msg = ByteBuffer.wrap(\"Hi!\\r\\n\".getBytes()); for (;;) &#123; try &#123; // 等待 select() 需要处理的新事件，阻塞将一直持续到下一个传入事件 selector.select(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); // handle exception break; &#125; // 获取所有接收事件的标识 SelectionKey() Set&lt;SelectionKey&gt; readyKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; // 取出标识，以便标识所对应的事件进行业务操作 SelectionKey key = iterator.next(); // 此事件已经取出处理，将其标识符移除 iterator.remove(); try &#123; // 检查事件类型 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel)key.channel(); // 接受客户端 SocketChannel client = server.accept(); client.configureBlocking(false); // 将客户端注册到选择器 client.register(selector, SelectionKey.OP_WRITE | SelectionKey.OP_READ, msg.duplicate()); System.out.println(\"Accepted connection from \" + client); &#125; // 检查事件类型 if (key.isWritable()) &#123; SocketChannel client = (SocketChannel)key.channel(); ByteBuffer buffer =(ByteBuffer)key.attachment(); while (buffer.hasRemaining()) &#123; // 将数据写到已连接的客户端 if (client.write(buffer) == 0) &#123; break; &#125; &#125; client.close(); &#125; &#125; catch (IOException ex) &#123; key.cancel(); try &#123; key.channel().close(); &#125; catch (IOException cex) &#123; // ignore on close &#125; &#125; &#125; // while &#125; // for &#125; &#125; Netty 框架因为 Netty 为每种传输的实现都暴露了相同的 API，阻塞与非阻塞网络传输的实现都依赖于 interface Channel、 ChannelPipeline 和 ChannelHandler。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 使用 Netty 的网络处理public class NettyOioServer &#123; public void server(int port) throws Exception &#123; final ByteBuf buf = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer(\"Hi!\\r\\n\", Charset.forName(\"UTF-8\")) ); // 阻塞模式 EventLoopGroup group = new OioEventLoopGroup(); // 非阻塞模式 // EventLoopGroup group = new OioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(group) // 非阻塞模式 // .channel(NioServerSocketChannel.class) .channel(OioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) // 每个连接的通道都需要调用此初始化方法 .childHandler( new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast( // 拦截和处理事件 new ChannelInboundHandlerAdapter() &#123; @Override public void channelActive(ChannelHandlerContext ctx)throws Exception &#123; ctx.writeAndFlush(buf.duplicate()) // 监听，以便在消息写完后关闭连接 .addListener(ChannelFutureListener.CLOSE); &#125; &#125; ); &#125; &#125; ); ChannelFuture f = b.bind().sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; &#125; 传输 API传输 API 的核心是 interface Channel，它被用于所有的 I/O 操作 每个 Channel 都将会被分配一个 ChannelPipeline 和 ChannelConfig。ChannelConfig 包含了该 Channel 的所有配置设置，并且支持热更新。 由于 Channel 是独一无二的，为了保证顺序将 Channel 声明为 java.lang. Comparable 的一个子接口。 ChannelPipeline 持有所有将应用于入站和出站数据以及事件的 ChannelHandler 实例，这些 ChannelHandler 实现了应用程序用于处理状态变化以及数据处理的逻辑。你也可以根据需要通过添加或者移除ChannelHandler实例来修改 ChannelPipeline。通过利用Netty的这项能力可以构建出高度灵活的应用程序。 123456789101112131415161718192021// 写出到 ChannelChannel channel = ...;// 创建持有要写数据的 ByteBufByteBuf buf = Unpooled.copiedBuffer(\"your data\", CharsetUtil.UTF_8);// 写数据并且冲刷它ChannelFuture cf = channel.writeAndFlush(buf);// 添加监听器以便在操作完成后接收到通知cf.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; // 操作完成并且没有发生错误 if (future.isSuccess()) &#123; System.out.println(\"Write successful\"); &#125; else &#123; System.err.println(\"Write error\"); future.cause().printStackTrace(); &#125; &#125; &#125;); Netty 的 Channel 实现是线程安全的，因此你可以存储一个到 Channel 的引用，并且每当你需要向远程节点写数据时，都可以使用它，即使当时许多线程都在使用它。 1234567891011121314151617// 从多个线程使用同一个 Channelfinal Channel channel = ...;final ByteBuf buf = Unpooled.copiedBuffer(\"your data\",CharsetUtil.UTF_8).retain();// 创建将数据写到 Channel 的 RunnableRunnable writer = new Runnable() &#123; @Override public void run() &#123; channel.writeAndFlush(buf.duplicate()); &#125;&#125;;// 获取到线程池 Executor 的引用Executor executor = Executors.newCachedThreadPool();// write in one threadexecutor.execute(writer);// write in another threadexecutor.execute(writer);... 内置的传输Netty 内置了一些可开箱即用的传输。因为并不是它们所有的传输都支持每一种协议，所以你必须选择一个和你的应用程序所使用的协议相容的传输。 NIONIO 提供了一个所有 I/O 操作的全异步的实现，利用了 JDK1.4 时便可用的基于选择器的 API。 选择器背后的基本概念是充当一个注册表，在那里你将可以请求在 Channel 的状态发生变化时得到通知。 class java.nio.channels.SelectionKey 定义了一组应用程序正在请求通知的状态变化集。 选择器选择并处理状态的变化 零拷贝（zero-copy）：它使你可以快速高效地将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间，其在像 FTP 或者 HTTP 这样的协议中可以显著地提升性能。 Epoll用于 Linux 的本地非阻塞传输，Linux JDK NIO API 使用了这些 epoll 调用。 OIONetty 的 OIO 传输实现代表了一种折中：它可以通过常规的传输 API 使用，但是由于它是建立在 java.net 包的阻塞实现之上的，所以它不是异步的。但是，它仍然非常适合于某些用途。 Local 传输用于在同一个 JVM 中运行的客户端和服务器程序之间的异步通信。 Embedded 传输Netty 提供了一种额外的传输，使得你可以将一组 ChannelHandler 作为帮助器类嵌入到其他的 ChannelHandler 内部。通过这种方式，你将可以扩展一个 ChannelHandler 的功能，而又不需要修改其内部代码。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Netty 组件和设计","slug":"后台技术/Netty/Netty 组件和设计","date":"2020-01-22T06:09:03.000Z","updated":"2020-07-10T04:04:23.276Z","comments":true,"path":"2020/01/22/后台技术/Netty/Netty 组件和设计/","link":"","permalink":"http://yoursite.com/2020/01/22/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E7%BB%84%E4%BB%B6%E5%92%8C%E8%AE%BE%E8%AE%A1/","excerpt":"Channel、EventLoop 和 ChannelFutureChannel、EventLoop 和 ChannelFuture 类合在一起可以被认为是 Netty 的网络抽象代表","text":"Channel、EventLoop 和 ChannelFutureChannel、EventLoop 和 ChannelFuture 类合在一起可以被认为是 Netty 的网络抽象代表 Channel：Socket EventLoop：控制流、多线程处理、并发 ChannelFuture：异步通知 Channel 接口基本的 I/O 操作（bind()、connect()、read()和 write()）依赖于底层网络传输所提供的原语。在基于 Java 的网络编程中，其基本的构造是 class Socket。Netty 的 Channel 接口所提供的 API，大大地降低了直接使用 Socket 类的复杂性。此外，Channel 也是拥有许多预定义的、专门化实现的广泛类层次结构的根。 EvenLoop 接口 一个 EventLoopGroup 包含一个或者多个 EventLoop； 一个 EventLoop 在它的生命周期内只和一个 Thread 绑定； 所有由 EventLoop 处理的 I/O 事件都将在它专有的 Thread 上被处理； 一个 Channel 在它的生命周期内只注册于一个 EventLoop； 一个 EventLoop 可能会被分配给一个或多个 Channel。 在这种设计中，一个给定 Channel 的 I/O 操作都是由相同的 Thread 执行的，实际上消除了对于同步的需要。 channelFuture 接口Netty 中所有的 I/O 操作都是异步的，因为一个操作可能不会立即返回，所以我们需要一种用于在之后的某个时间点确定其结果的方法。为此，Netty 提供了 ChannelFuture 接口，其 addListener() 方法注册了一个 ChannelFutureListener，以便在某个操作完成时（无论是否成功）得到通知。 ChannelHandler 和 ChannelPipeline接下来，让我们更加细致的看一看哪那些管理数据流以及执行应用程序处理逻辑的组件。 ChannelHandler 接口从应用程序开发人员的角度来看，Netty 的主要组件是 ChannelHandler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。这是可行的，因为 ChannelHandler 的方法是由网络事件（其中术语“事件”的使用非常广泛）触发的。事实上，ChannelHandler 可专门用于几乎任何类型的动作，例如将数据从一种格式转换为另外一种格式，或者处理转换过程中所抛出的异常。 你的应用程序的业务逻辑通常驻留在一个或者多个 ChannelInboundHandler 中。 ChannelPipeline 接口ChannelPipeline 提供了 ChannelHandler 链的容器，并定义了用于在该链上传播入站和出站事件流的 API。当 Channel 被创建时，它会被自动地分配到它专属的 ChannelPipeline。 ChannelHandler 安装到 ChannelPipeline 中的过程： 一个ChannelInitializer的实现被注册到了ServerBootstrap中； 当 ChannelInitializer.initChannel() 方法被调用时，ChannelInitializer 将在 ChannelPipeline 中安装一组自定义的 ChannelHandler； ChannelInitializer 将它自己从 ChannelPipeline 中移除。 ChannelHandler 是专为支持广泛的用途而设计的，可以将它看作是处理往来 ChannelPipeline 事件（包括数据）的任何代码的通用容器。使得事件流经 ChannelPipeline 是 ChannelHandler 的工作，它们是在应用程序的初始化或者引导阶段被安装的。这些对象接收事件、执行它们所实现的处理逻辑，并将数据传递给链中的下一个 ChannelHandler。它们的执行顺序是由它们被添加的顺序所决定的。实际上，被我们称为 ChannelPipeline 的是这些 ChannelHandler 的编排顺序。 如同上图所示，入站和出站 ChannelHandler 可以被安装到同一个 ChannelPipeline 中。虽然 ChannelInboundHandle 和 ChannelOutboundHandle 都扩展自 ChannelHandler，但是 Netty 能区分 ChannelInboundHandler 实现和 ChannelOutboundHandler 实现，并确保数据只会在具有相同定向类型的两个 ChannelHandler 之间传递。 当 ChannelHandler 被添加到 ChannelPipeline 时，它将会被分配一个 ChannelHandlerContext，其代表了 ChannelHandler 和 ChannelPipeline 之间的绑定。虽然这个对象可以被用于获取底层的 Channel，但是它主要还是被用于写出站数据。 在 Netty 中，有两种发送消息的方式。你可以直接写到 Channel 中，也可以写到和 ChannelHandler 相关联的 ChannelHandlerContext 对象中。前一种方式将会导致消息从 ChannelPipeline 的尾端开始流动，而后者将导致消息从 ChannelPipeline 中的下一个 ChannelHandler 开始流动。 编码器和解码器正如有用来简化 ChannelHandler 的创建的适配器类一样，所有由 Netty 提供的编码器/解码器适配器类都实现 了 ChannelOutboundHandler 或者 ChannelInboundHandler 接口。 通常来说，这些基类的名称将类似于 ByteToMessageDecoder 或 MessageToByteEncoder。对于特殊的类型，你可能会发现类似于 ProtobufEncoder 和 ProtobufDecoder 这样的名称——预置的用来支持 Google 的 Protocol Buffers。 抽象类 SimpleChannelInboundHandler最常见的情况是，你的应用程序会利用一个 ChannelHandler 来接收解码消息，并对该数据应用业务逻辑。要创建一个这样的 ChannelHandler，你只需要扩展基类 SimpleChannelInboundHandler&lt;T&gt;，其中 T 是你要处理的消息的 Java 类型 。在这个 ChannelHandler 中，你将需要重写基类的一个或者多个方法，并且获取一个到 ChannelHandlerContext 的引用， 这个引用将作为输入参数传递给 ChannelHandler 的所有方法。 引导Netty 的引导类为应用程序的网络层配置提供了容器，这涉及将一个进程绑定到某个指定的端口，或者将一个进程连接到另一个运行在某个指定主机的指定端口上的进程。 “服务器”和“客户端”实际上表示了不同的网络行为；换句话说，是监听传入的连接还是建立到一个或者多个进程的连接。 因此，有两种类型的引导：一种用于客户端（简单地称为 Bootstrap），而另一种（ServerBootstrap）用于服务器。无论你的应用程序使用哪种协议或者处理哪种类型的数据，唯一决定它使用哪种引导类的是它是作为一个客户端还是作为一个服务器。 Bootstrap 将连接远程主机和端口，数量通常为一个。 ServerBootstrap 将绑定到一个端口，因为服务器必须要监听连接，数量通常为两个。 第一组将只包含一个 ServerChannel，代表服务器自身的已绑定到某个本地端口的正在监听的套接字。 第二组将包含所有已创建的用来处理传入客户端连接（对于每个服务器已经接受的连接都有一个）的 Channel。 与 ServerChannel 相关联的 EventLoopGroup 将分配一个负责为传入连接请求创建 Channel 的 EventLoop。一旦连接被接受，第二个 EventLoopGroup 就会给它的 Channel 分配一个 EventLoop。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"一个简单的 Netty 应用程序","slug":"后台技术/Netty/Netty 简单应用程序","date":"2020-01-21T14:46:53.000Z","updated":"2020-07-10T04:04:35.183Z","comments":true,"path":"2020/01/21/后台技术/Netty/Netty 简单应用程序/","link":"","permalink":"http://yoursite.com/2020/01/21/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"应用程序 EchoEcho 客户端和服务器之间的交互是非常简单的；在客户端建立一个连接之后，它会向服务器发送一个或多个消息，反过来，服务器又会将每个消息回送给客户端。这一简单的功能充分体现了客户端/服务器系统中典型的请求-响应交互模式。","text":"应用程序 EchoEcho 客户端和服务器之间的交互是非常简单的；在客户端建立一个连接之后，它会向服务器发送一个或多个消息，反过来，服务器又会将每个消息回送给客户端。这一简单的功能充分体现了客户端/服务器系统中典型的请求-响应交互模式。 编写 Echo 服务器所有的 Netty 服务器都需要以下两部分 至少一个 ChannelHandler：该组件实现了服务器对从客户端接收的数据的处理，即它的业务逻辑。 引导：这是配置服务器的启动代码。至少，它会将服务器绑定到它要监听连接请求的端口上。 ChannelHandler 和业务逻辑因为你的 Echo 服务器会响应传入的消息，所以它需要实现 ChannelInboundHandler 接口，用来定义响应入站事件的方法。这个简单的应用程序只需要用到少量的这些方法，所以继承 ChannelInboundHandlerAdapter 类也就足够了，它提供了 ChannelInboundHandler 的默认实现。 123456789101112131415161718192021222324// EchoServerHandler 类@Sharable // 标示一个 ChannelHandler 可以被多个 Channel 安全地共享public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; // 对于每个传入的消息都要调用 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf in = (ByteBuf) msg; System.out.println(\"Server received: \" + in.toString(CharsetUtil.UTF_8)); // 将接收到的数据写给发送者 ctx.write(in); &#125; // 通知 ChannelInboundHandler 最后一次对 channelRead() 的调用是当前批量读取中的最后一条消息 @Override public void channelReadComplete(ChannelHandlerContext ctx) &#123; // 将消息冲刷到远程节点并且关闭该 Channel ctx.writeAndFlush(Unpooled.EMPTY_BUFFER).addListener(ChannelFutureListener.CLOSE); &#125; // 在读取操作期间，有异常抛出时会调用 @Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; ChannelHandler 的一些关键点： 针对不同类型的事件来调用 ChannelHandler； 应用程序通过实现或者扩展 ChannelHandler 来挂钩到事件的生命周期，并且提供自定义的应用程序逻辑； 在架构上，ChannelHandler 有助于保持业务逻辑与网络处理代码的分离。这简化了开发过程，因为代码必须不断地演化以响应不断变化的需求。 引导服务器绑定到服务器将在其上监听并接受传入连接请求的端口； 配置 Channel，以将有关的入站消息通知给 EchoServerHandler 实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// EchoServer 类public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.println(\"Usage: \" + EchoServer.class.getSimpleName() + \" &lt;port&gt;\"); &#125; int port = Integer.parseInt(args[0]); new EchoServer(port).start(); &#125; public void start() throws Exception &#123; final EchoServerHandler serverHandler = new EchoServerHandler(); // 创建一个循环组 EventLoopGroup EventLoopGroup group = new NioEventLoopGroup(); try &#123; // 创建一个 ServerBootstrap 的实例以引导和绑定服务器 ServerBootstrap b = new ServerBootstrap(); b.group(group) .channel(NioServerSocketChannel.class) // 指定服务器绑定的本地的 InetSocketAddress .localAddress(new InetSocketAddress(port)) .childHandler( // 接受一个新连接则一个新的子 Channel 将会被创建 new ChannelInitializer&lt;SocketChannel&gt;()&#123; // 使用一个 EchoServerHandler 的实例初始化每一个新的 Channel @Override public void initChannel(SocketChannel ch)throws Exception &#123; ch.pipeline().addLast(serverHandler); &#125; &#125; ); // 异步地绑定服务器，sync() 方法的调用将导致当前 Thread 阻塞，一直到绑定操作完成为止 ChannelFuture f = b.bind().sync(); // 异步地关闭服务器 f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; &#125; 编写 Echo 客户端 Echo 客户端将会： 连接到服务器； 发送一个或者多个消息； 对于每个消息，等待并接受从服务器发送回的相同消息 关闭连接。 通过 ChannelHandler 实现客户端逻辑如同服务器，客户端将拥有一个用来处理数据的 ChannelInboundHandler。 123456789101112131415161718192021// 客户端的 ChannelHandler// ByteBuf 为 Nutty 的字节容器@Sharablepublic class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 在到服务器的连接已经建立之后将被调用 @Override public void channelActive(ChannelHandlerContext ctx) &#123; ctx.writeAndFlush(Unpooled.copiedBuffer(\"Netty rocks!\",CharsetUtil.UTF_8)); &#125; // 当从服务器接收到一条消息时被调用 @Override public void channelRead0(ChannelHandlerContext ctx, ByteBuf in) &#123; System.out.println(\"Client received: \" + in.toString(CharsetUtil.UTF_8)); &#125; // 在处理过程中引发异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; SimpleChannelInboundHandler 与 ChannelInboundHandler 你可能会想：为什么我们在客户端使用的是 SimpleChannelInboundHandler，而不是在 EchoServerHandler 中所使用的 ChannelInboundHandlerAdapter 呢？这和两个因素的相互作用有关：业务逻辑如何处理消息以及 Netty 如何管理资源。 在客户端，当 channelRead0() 方法完成时，你已经有了传入消息，并且已经处理完它了。当该方法返回时，SimpleChannelInboundHandler 负责释放指向保存该消息的 ByteBuf 的内存引用。 在 EchoServerHandler 中，你仍然需要将传入消息回送给发送者，而 write()操作是异步的，直到 channelRead() 方法返回后可能仍然没有完成。为此，EchoServerHandler 扩展了 ChannelInboundHandlerAdapter，其在这个时间点上不会释放消息。 消息在 EchoServerHandler 的 channelReadComplete() 方法中，当 writeAndFlush() 方法被调用时被释放 引导客户端引导客户端类似于引导服务器，不同的是，客户端是使用主机和端口参数来连接远程地址，也就是这里的 Echo 服务器的地址，而不是绑定到一个一直被监听的端口。 12345678910111213141516171819202122232425262728293031323334353637383940// 客户端的主类public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .remoteAddress(new InetSocketAddress(host, port)) .handler( new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler()); &#125; &#125; ); ChannelFuture f = b.connect().sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println(\"Usage: \" + EchoClient.class.getSimpleName() + \" &lt;host&gt; &lt;port&gt;\"); return; &#125; String host = args[0]; int port = Integer.parseInt(args[1]); new EchoClient(host, port).start(); &#125; &#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"环境变量","slug":"环境搭建/环境变量","date":"2020-01-21T14:35:41.000Z","updated":"2020-07-10T02:50:01.154Z","comments":true,"path":"2020/01/21/环境搭建/环境变量/","link":"","permalink":"http://yoursite.com/2020/01/21/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","excerpt":"环境变量配置汇总","text":"环境变量配置汇总 Java 环境设置JAVA_HOME：[ JDK 安装目录 ] CLASSPATH：…;%JAVA_HOME%\\bin Maven 环境设置M2_HOME：[ 安装目录 ]/apache-maven-x.x.x CLASSPATH：…;%M2_HOME%\\bin","categories":[{"name":"技术杂项","slug":"技术杂项","permalink":"http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"环境变量","slug":"环境变量","permalink":"http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"}]},{"title":"Netty 入门简介","slug":"后台技术/Netty/Netty 入门简介","date":"2020-01-21T01:37:11.000Z","updated":"2020-07-10T04:04:31.425Z","comments":true,"path":"2020/01/21/后台技术/Netty/Netty 入门简介/","link":"","permalink":"http://yoursite.com/2020/01/21/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B/","excerpt":"Java IO无论是 C 语言，还是 Java，在进行网络编程的开发时都较为不友好。早期的 Java API（ java.net ）只支持由本地系统套接字库提供所谓的阻塞函数。","text":"Java IO无论是 C 语言，还是 Java，在进行网络编程的开发时都较为不友好。早期的 Java API（ java.net ）只支持由本地系统套接字库提供所谓的阻塞函数。 12345678910111213141516171819202122// 阻塞 I/O 示例I/O// 创建一个 ServerSocket 用以监听端口上的连接请求ServerSocket serverSocket = new ServerSocket(portNumber);// accept() 方法调用将被阻塞，直到一个连接建立Socket clientSocket = serverSocket.accept();BufferReader in = new BufferReader( new InputStreamReader( clientSocket.getInputStream() ));// PrintWriter(OutputStream out, boolean autoFlush) ???PrintWriter out = new PrintWriter(clientSocket.getOutputStream(), true);String request, response;while ((request = in.readLine()) != null) &#123; if (\"Done\".equals(request)) &#123; break; // 客户端发送了 Done 则退出处理循环 &#125; // You can use ProcessRequest to handle your request response = processRequest(request); out.println(response);&#125; 这段代码片段将只能同时处理一个连接，要管理多个并发客户端，需要为每个新的客户端 Socket 创建一个新的 Thread。 使用阻塞 I/O处理多个连接： Java NIOclass java.nio.channels.Selector 是 Java 的非阻塞 I/O 实现的关键。它使用了事件通知 API 以确定在一组非阻塞套接字中有哪些已经就绪能够进 行 I/O 相关的操作。因为可以在任何的时间检查任意 的读操作或者写操作的完成状态。 使用 Selector 的非阻塞 I/O Netty 简介在网络编程领域，Netty 是 Java 的卓越框架。它驾驭了 Java 高 API 的能力，并将其隐藏在一个易于使用的 API 之后。Netty 使你可以专注于自己真正感兴趣的：你的应用程序的独一无二的价值。 一个既是异步的又是事件驱动的系统会表现出一种特殊的、对我们来说极具价值的行为：它可以以任意的顺序响应在任意的时间点产生的事件。 完全异步的 I/O：非阻塞网络调用使得我们可以不必等待一个操作的完成。异步方法会立即返回，并且在它完成时，会直接或者在稍后的某个时间点通知用户。 选择器使得我们能够通过较少的线程便可监视许多连接上的事件。 Netty 的核心组件ChannelChannel 时 Java NIO 的一个基本构造。 它代表一个到实体（如一个硬件设备、一个文件、一个网络套接字或者一个能够执 行一个或者多个不同的I/O操作的程序组件）的开放连接，如读操作和写操作。 在一定程度上可以把 Channel 看作是传入（入站）或者传出（出站）数据的载体。因此，它可以被打开或者被关闭，连接或者断开连接。 回调Netty 在内部使用了回调来处理事件；当一个回调被触发时，相关的事件可以被一个 interfaceChannelHandler 的实现处理 12345678910// 被回调触发的 ChannelHandlerpublic class ConnectHandler extends ChannelInboundHandlerAdapter &#123; @Override // 当一个新的连接被建立完成时，此方法将会被调用 public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println( \"Client \" + ctx.channel().remoteAddress() + \" connected\" ); &#125;&#125; FutureFuture 提供了另一种在操作完成时通知应用程序的方式。这个对象可以看作是一个异步操作的结果的占位符；它将在未来的某个时刻完成，并提供对其结果的访问。 JDK 预置了 interface java.util.concurrent.Future，但是其所提供的实现，只允许手动检查对应的操作是否已经完成，或者一直阻塞直到它完成。这是非常繁琐的，所以 Netty 提供了它自己的实现：ChannelFuture，用于在执行异步操作的时候使用。 ChannelFuture提供了几种额外的方法，这些方法使得我们能够注册一个或者多个 ChannelFutureListener 实例。由 ChannelFutureListener 提供的通知机制消除了手动检查对应的操作是否完成的必要。 监听器的回调方法 operationComplete()：将会在对应的操作完成时被调用。 监听器可以判断操作是成功地完成了还是出错了（ 检索产生的 Throwable ）。 每个 Netty 的出站 I/O 操作都将返回一个 ChannelFuture；也就是说，它们都不会阻塞。正如我们前面所提到过的一样，Netty 完全是异步和事件驱动的。 123456789101112131415161718192021222324252627// 异步的建立连接Channel channel = ...;// Does not blockChannelFuture future = channel.connect( // 异步地连接到远程节点 new InetSocketAddress(\"192.168.0.1\", 25));// 注册一个 ChannelFutureListener 以便在操作完成时获得通知future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; // 检查操作的状态 if (future.isSuccess())&#123; ByteBuf buffer = Unpooled.copiedBuffer( \"Hello\",Charset.defaultCharset() ); // 将数据异步发送到远程节点 ChannelFuture wf = future.channel().writeAndFlush(buffer); .... &#125; else &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; &#125; &#125;); 回调和 Future 是相互补充的机制；它们相互结合，构成了 Netty 本身的关键构件块之一。 事件和 ChannelHandlerNetty 是一个网络编程框架，所以事件是按照它们与入站或出站数据流的相关性进行分类的。 可能由入站数据或者相关的状态更改而触发的事件： 连接已被激活或者连接失活； 数据读取； 用户事件； 错误事件。 出站事件是未来将会触发的某个动作的操作结果，这些动作包括： 打开或者关闭到远程节点的连接； 将数据写到或者冲刷到套接字。 每个事件都可以被分发给 ChannelHandler 类中的某个用户实现的方法，后面会对此类进行更进一步的说明，目前你可以每个 ChannelHandler 的实例都类似于一种为了响应特定事件而被执行的回调。 流经 ChannelHandler 链的入站事件和出站事件 各组件的整合 Future、回调和 ChannelHandler Netty 的异步编程模型是建立在 Future 和回调的概念之上的，而将事件派发到 ChannelHandler 的方法则发生在更深的层次上。结合在一起，这些元素就提供了一个处理环境，使你的应用程序逻 辑可以独立于任何网络操作相关的顾虑而独立地演变。这也是 Netty 的设计方式的一个关键目标。 拦截操作以及高速地转换入站数据和出站数据，都只需要你提供回调或者利用操作所返回的 Future。这使得链接操作变得既简单又高效，并且促进了可重用的通用代码的编写。 选择器、事件和 EventLoop Netty 通过触发事件将 Selector 从应用程序中抽象出来，消除了所有本来将需要手动编写 的派发代码。在内部，将会为每个 Channel 分配一个 EventLoop，用以处理所有事件。 EventLoop 本身只由一个线程驱动，其处理了一个 Channel 的所有 I/O 事件，并且在该 EventLoop 的整个生命周期内都不会改变（ 无需顾虑同步 ）。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]}],"categories":[{"name":"联邦学习","slug":"联邦学习","permalink":"http://yoursite.com/categories/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学推导","slug":"数学推导","permalink":"http://yoursite.com/categories/%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"},{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"},{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"技术原理","slug":"技术原理","permalink":"http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/"},{"name":"开发杂项","slug":"开发杂项","permalink":"http://yoursite.com/categories/%E5%BC%80%E5%8F%91%E6%9D%82%E9%A1%B9/"},{"name":"项目开发","slug":"项目开发","permalink":"http://yoursite.com/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"},{"name":"环境搭建","slug":"环境搭建","permalink":"http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"name":"基础知识","slug":"基础知识","permalink":"http://yoursite.com/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"生活杂记","slug":"生活杂记","permalink":"http://yoursite.com/categories/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/"},{"name":"Software","slug":"Software","permalink":"http://yoursite.com/categories/Software/"},{"name":"技术杂项","slug":"技术杂项","permalink":"http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"FATE","slug":"FATE","permalink":"http://yoursite.com/tags/FATE/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"ML 理论","slug":"ML-理论","permalink":"http://yoursite.com/tags/ML-%E7%90%86%E8%AE%BA/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"基础","slug":"基础","permalink":"http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"},{"name":"Flume","slug":"Flume","permalink":"http://yoursite.com/tags/Flume/"},{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"},{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"算法整理","slug":"算法整理","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"Hive","slug":"Hive","permalink":"http://yoursite.com/tags/Hive/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://yoursite.com/tags/RabbitMQ/"},{"name":"Dubbo","slug":"Dubbo","permalink":"http://yoursite.com/tags/Dubbo/"},{"name":"点赞","slug":"点赞","permalink":"http://yoursite.com/tags/%E7%82%B9%E8%B5%9E/"},{"name":"H2","slug":"H2","permalink":"http://yoursite.com/tags/H2/"},{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"},{"name":"Aysnc","slug":"Aysnc","permalink":"http://yoursite.com/tags/Aysnc/"},{"name":"Scheduled","slug":"Scheduled","permalink":"http://yoursite.com/tags/Scheduled/"},{"name":"Email","slug":"Email","permalink":"http://yoursite.com/tags/Email/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"},{"name":"Security","slug":"Security","permalink":"http://yoursite.com/tags/Security/"},{"name":"Shiro","slug":"Shiro","permalink":"http://yoursite.com/tags/Shiro/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"},{"name":"MQ","slug":"MQ","permalink":"http://yoursite.com/tags/MQ/"},{"name":"Springboot","slug":"Springboot","permalink":"http://yoursite.com/tags/Springboot/"},{"name":"Cache","slug":"Cache","permalink":"http://yoursite.com/tags/Cache/"},{"name":"MyBatis","slug":"MyBatis","permalink":"http://yoursite.com/tags/MyBatis/"},{"name":"高并发编程","slug":"高并发编程","permalink":"http://yoursite.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"},{"name":"开发工具","slug":"开发工具","permalink":"http://yoursite.com/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"用户管理系统","slug":"用户管理系统","permalink":"http://yoursite.com/tags/%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"},{"name":"JPA","slug":"JPA","permalink":"http://yoursite.com/tags/JPA/"},{"name":"Druid","slug":"Druid","permalink":"http://yoursite.com/tags/Druid/"},{"name":"JDBC","slug":"JDBC","permalink":"http://yoursite.com/tags/JDBC/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"Web","slug":"Web","permalink":"http://yoursite.com/tags/Web/"},{"name":"Log","slug":"Log","permalink":"http://yoursite.com/tags/Log/"},{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"WebSocket","slug":"WebSocket","permalink":"http://yoursite.com/tags/WebSocket/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"Welab","slug":"Welab","permalink":"http://yoursite.com/tags/Welab/"},{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"},{"name":"乱码","slug":"乱码","permalink":"http://yoursite.com/tags/%E4%B9%B1%E7%A0%81/"},{"name":"聊天室","slug":"聊天室","permalink":"http://yoursite.com/tags/%E8%81%8A%E5%A4%A9%E5%AE%A4/"},{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/tags/Maven/"},{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"},{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"环境变量","slug":"环境变量","permalink":"http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"}]}